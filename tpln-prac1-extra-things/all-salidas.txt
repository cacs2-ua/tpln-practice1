---- SECTION 2 ----

 

[Cache] Using existing file: emnlp2016-2018.json

[Provenance]
 - Source URL: https://sbert.net/datasets/emnlp2016-2018.json
 - Local path: emnlp2016-2018.json
 - File size : 1.05 MB
 - SHA-256   : 9e6020503e5f0dd0e91dbb970d47f43c7621f67321249daed5990057e159961c

[Sanity checks]
 - N papers loaded: 974
 - Key presence: OK (all required keys found in all records)
 - Empty titles   : 0
 - Empty abstracts: 0
 - Non-string titles   : 0
 - Non-string abstracts: 0
 - Year distribution (top): [(2018, 549), (2017, 230), (2016, 195)]
 - Years outside 2016–2018: 0
 - Venue distribution (top): [('EMNLP', 974)]
 - Duplicate URL keys: 0 (unique non-empty URLs: 974)

[Access check — first record]
 - title   : Rule Extraction for Tree-to-Tree Transducers by Cost Minimization
 - abstract: Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set...
 - url     : http://aclweb.org/anthology/D16-1002
 - venue   : EMNLP
 - year    : 2016
  
   
    

 
  











----- SECTION 3 -----


[Section 3] Preprocessing complete.
 - Document unit          : 1 paper = 1 document
 - N documents (paper_texts): 974
 - DataFrame created      : True

[Section 3] Example document (first record):
Rule Extraction for Tree-to-Tree Transducers by Cost Minimization Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the ...


title	abstract	url	venue	year	text
0	Rule Extraction for Tree-to-Tree Transducers b...	Finite-state transducers give efficient repres...	http://aclweb.org/anthology/D16-1002	EMNLP	2016	Rule Extraction for Tree-to-Tree Transducers b...
1	A Neural Network for Coordination Boundary Pre...	We propose a neural-network based model for co...	http://aclweb.org/anthology/D16-1003	EMNLP	2016	A Neural Network for Coordination Boundary Pre...
2	Distinguishing Past, On-going, and Future Even...	The tremendous amount of user generated data t...	http://aclweb.org/anthology/D16-1005	EMNLP	2016	Distinguishing Past, On-going, and Future Even...


 
  








----- SECTION 4 -----



[Section 4] BoW experiments complete.
Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).
Each column is a token; each cell is the token count in that document.

--- bow_lc_on_sw_off ---
Config: lowercase=True, stop_words=None
Matrix type : csr_matrix (sparse)
Shape       : (974, 8404)  (#docs x #vocab)
nnz         : 90157
Density     : 0.011014
Avg nnz/doc : 92.56
Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']
Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']

--- bow_lc_on_sw_on ---
Config: lowercase=True, stop_words=english
Matrix type : csr_matrix (sparse)
Shape       : (974, 8153)  (#docs x #vocab)
nnz         : 64523
Density     : 0.008125
Avg nnz/doc : 66.25
Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']
Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']

--- bow_lc_off_sw_off ---
...
Stopword casing nuance: 155 vocab terms look like stopwords after lowercasing (e.g., 'The')
Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']
Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']

Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...



run	lowercase	stop_words	N_docs	V_vocab	nnz	density	avg_nnz_per_doc	case_sensitive_stopword_survivors
0	bow_lc_on_sw_off	True	None	974	8404	90157	0.011014	92.563655	0
1	bow_lc_on_sw_on	True	english	974	8153	64523	0.008125	66.245380	0
2	bow_lc_off_sw_off	False	None	974	10342	96257	0.009556	98.826489	0
3	bow_lc_off_sw_on	False	english	974	10094	72143	0.007338	74.068789	155




[Section 4] Default BoW run set for later use: bow_lc_on_sw_off






---- SECTION 5 ----

[Section 5] TF-IDF experiments complete.
Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).
Each column is a token; each cell is a TF-IDF weight (NOT a raw count).
Note: sklearn TF-IDF uses L2-normalization by default, so ||doc_vector||_2 ≈ 1.

--- tfidf_lc_on_sw_off ---
Config: lowercase=True, stop_words=None
Matrix type : csr_matrix (sparse)
Dtype       : float64 (TF-IDF weights are floating-point)
Shape       : (974, 8404)  (#docs x #vocab)
nnz         : 90157
Density     : 0.011014
Avg nnz/doc : 92.56
L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000
Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']
Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']
Top TF-IDF terms (doc 0):
 - transducers           0.3709
 - minimization          0.3430
 - tree                  0.2335
 - complex               0.2053
 - restrictions          0.1854
 - encountered           0.1715
 - representations       0.1600
 - compact               0.1525
 - involving             0.1499
 - finite                0.1475

--- tfidf_lc_on_sw_on ---
Config: lowercase=True, stop_words=english
Matrix type : csr_matrix (sparse)
Dtype       : float64 (TF-IDF weights are floating-point)
Shape       : (974, 8153)  (#docs x #vocab)
nnz         : 64523
Density     : 0.008125
Avg nnz/doc : 66.25
L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000
Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']
Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']
Top TF-IDF terms (doc 0):
 - transducers           0.4070
 - minimization          0.3764
 - tree                  0.2563
 - complex               0.2253
 - restrictions          0.2035
 - encountered           0.1882
 - representations       0.1756
 - compact               0.1674
 - involving             0.1645
 - finite                0.1619

--- tfidf_lc_off_sw_off ---
Config: lowercase=False, stop_words=None
Matrix type : csr_matrix (sparse)
Dtype       : float64 (TF-IDF weights are floating-point)
Shape       : (974, 10342)  (#docs x #vocab)
nnz         : 96257
Density     : 0.009556
Avg nnz/doc : 98.83
L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000
Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']
Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']
Top TF-IDF terms (doc 0):
 - Tree                  0.2795
 - complex               0.2054
 - Cost                  0.1966
 - Minimization          0.1966
 - Transducers           0.1966
 - Rule                  0.1855
 - restrictions          0.1855
 - transducers           0.1855
 - Finite                0.1855
 - minimization          0.1716

--- tfidf_lc_off_sw_on ---
Config: lowercase=False, stop_words=english
Matrix type : csr_matrix (sparse)
Dtype       : float64 (TF-IDF weights are floating-point)
Shape       : (974, 10094)  (#docs x #vocab)
nnz         : 72143
Density     : 0.007338
Avg nnz/doc : 74.07
L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000
Stopword casing nuance: 155 vocab terms look like stopwords after lowercasing (e.g., 'The')
Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']
Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']
Top TF-IDF terms (doc 0):
 - Tree                  0.3069
 - complex               0.2256
 - Minimization          0.2159
 - Cost                  0.2159
 - Transducers           0.2159
 - restrictions          0.2037
 - Rule                  0.2037
 - Finite                0.2037
 - transducers           0.2037
 - minimization          0.1884



run	lowercase	stop_words	N_docs	V_vocab	nnz	density	avg_nnz_per_doc	l2_norm_mean(sample)	case_sensitive_stopword_survivors
0	tfidf_lc_on_sw_off	True	None	974	8404	90157	0.011014	92.563655	1.0	0
1	tfidf_lc_on_sw_on	True	english	974	8153	64523	0.008125	66.245380	1.0	0
2	tfidf_lc_off_sw_off	False	None	974	10342	96257	0.009556	98.826489	1.0	0
3	tfidf_lc_off_sw_on	False	english	974	10094	72143	0.007338	74.068789	1.0	155




[Section 5] Default TF-IDF run set for later use: tfidf_lc_on_sw_off

[Section 5] BoW vs TF-IDF (doc 0 example)
Top BoW terms by COUNT (doc 0):
 - of                    4
 - to                    4
 - tree                  2
 - for                   2
 - minimization          2
 - representations       2
 - complex               2
 - the                   2
 - transducers           2
 - operation             1

Top TF-IDF terms by WEIGHT (doc 0):
 - transducers           0.3709
 - minimization          0.3430
 - tree                  0.2335
 - complex               0.2053
 - restrictions          0.1854
 - encountered           0.1715
 - representations       0.1600
 - compact               0.1525
 - involving             0.1499
 - finite                0.1475











----- SECTION 6 -----


[Section 6] Queries ready.
 - N queries     : 3
 - Order/labels  : ['Query 1', 'Query 2', 'Query 3']

[Section 6] Preview (first 300 chars each):

Query 1: QUALES: Machine translation quality estimation via supervised and unsupervised machine learning.
QUALES: Machine translation quality estimation via supervised and unsupervised machine learning. The automatic quality estimation (QE) of machine translation consists in measuring the quality of translations without access to human references, usually via machine learning approaches. A good QE syste...

Query 2: Learning to Ask Unanswerable Questions for Machine Reading Comprehension
Learning to Ask Unanswerable Questions for Machine Reading Comprehension Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable ques...

Query 3: Unsupervised Neural Text Simplification
Unsupervised Neural Text Simplification The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabelled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based lo...









----- SECTION 7 -----

repr,run,lowercase,stop_words,query,rank,doc_id,score,title,venue,year,url,abstract_snippet
BoW,bow_lc_off_sw_off,False,None,Query 1,1,412,0.5862225581108623,Neural Machine Translation with Source Dependency Representation,EMNLP,2017,http://aclweb.org/anthology/D17-1304,"We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple..."
BoW,bow_lc_off_sw_off,False,None,Query 1,2,100,0.5486717282283898,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016,http://aclweb.org/anthology/D16-1134,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
BoW,bow_lc_off_sw_off,False,None,Query 1,3,738,0.5378212329705866,The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1313,"The advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. In this paper, the efficiencie..."
BoW,bow_lc_off_sw_off,False,None,Query 2,1,881,0.6435172056104048,A Nil-Aware Answer Extraction Framework for Question Answering,EMNLP,2018,http://aclweb.org/anthology/D18-1456,"In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defin..."
BoW,bow_lc_off_sw_off,False,None,Query 2,2,522,0.6038822648935608,Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models,EMNLP,2018,http://aclweb.org/anthology/D18-1097,"Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established models in this task. However, in languages other than English, th..."
BoW,bow_lc_off_sw_off,False,None,Query 2,3,756,0.6007622973936925,Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1331,"Most of the Neural Machine Translation (NMT) models are based on the sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped with the attention mechanism. However, the conventional attention mecha..."
BoW,bow_lc_off_sw_off,False,None,Query 3,1,426,0.4768006683498644,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
BoW,bow_lc_off_sw_off,False,None,Query 3,2,488,0.4735378834010061,Learning Unsupervised Word Translations Without Adversaries,EMNLP,2018,http://aclweb.org/anthology/D18-1063,Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embed...
BoW,bow_lc_off_sw_off,False,None,Query 3,3,286,0.46190771067662606,A Joint Sequential and Relational Model for Frame-Semantic Parsing,EMNLP,2017,http://aclweb.org/anthology/D17-1128,We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (obj...
BoW,bow_lc_off_sw_on,False,english,Query 1,1,762,0.4770506650595697,Prediction Improves Simultaneous Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1337,"We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine..."
BoW,bow_lc_off_sw_on,False,english,Query 1,2,100,0.46747513669930907,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016,http://aclweb.org/anthology/D16-1134,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
BoW,bow_lc_off_sw_on,False,english,Query 1,3,936,0.4557234328332087,Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,EMNLP,2018,http://aclweb.org/anthology/D18-1512,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
BoW,bow_lc_off_sw_on,False,english,Query 2,1,258,0.3711482237813678,Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension,EMNLP,2017,http://aclweb.org/anthology/D17-1087,"We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about..."
BoW,bow_lc_off_sw_on,False,english,Query 2,2,852,0.3471517180222783,Answer-focused and Position-aware Neural Question Generation,EMNLP,2018,http://aclweb.org/anthology/D18-1427,Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions inc...
BoW,bow_lc_off_sw_on,False,english,Query 2,3,559,0.3242821157509936,A dataset and baselines for sequential open-domain question answering,EMNLP,2018,http://aclweb.org/anthology/D18-1134,We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generati...
BoW,bow_lc_off_sw_on,False,english,Query 3,1,426,0.3820843983081298,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
BoW,bow_lc_off_sw_on,False,english,Query 3,2,853,0.35955554711063287,Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation,EMNLP,2018,http://aclweb.org/anthology/D18-1428,"Existing text generation methods tend to produce repeated and ""boring"" expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The..."
BoW,bow_lc_off_sw_on,False,english,Query 3,3,488,0.33996978180681053,Learning Unsupervised Word Translations Without Adversaries,EMNLP,2018,http://aclweb.org/anthology/D18-1063,Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embed...
BoW,bow_lc_on_sw_off,True,None,Query 1,1,412,0.6090979092037714,Neural Machine Translation with Source Dependency Representation,EMNLP,2017,http://aclweb.org/anthology/D17-1304,"We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple..."
BoW,bow_lc_on_sw_off,True,None,Query 1,2,100,0.5913358208661111,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016,http://aclweb.org/anthology/D16-1134,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
BoW,bow_lc_on_sw_off,True,None,Query 1,3,936,0.5677869274924967,Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,EMNLP,2018,http://aclweb.org/anthology/D18-1512,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
BoW,bow_lc_on_sw_off,True,None,Query 2,1,881,0.6452192031293627,A Nil-Aware Answer Extraction Framework for Question Answering,EMNLP,2018,http://aclweb.org/anthology/D18-1456,"In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defin..."
BoW,bow_lc_on_sw_off,True,None,Query 2,2,255,0.6210231899546157,RACE: Large-scale ReAding Comprehension Dataset From Examinations,EMNLP,2017,http://aclweb.org/anthology/D17-1082,"We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, in..."
BoW,bow_lc_on_sw_off,True,None,Query 2,3,522,0.6130833220037393,Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models,EMNLP,2018,http://aclweb.org/anthology/D18-1097,"Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established models in this task. However, in languages other than English, th..."
BoW,bow_lc_on_sw_off,True,None,Query 3,1,426,0.6132382338418328,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
BoW,bow_lc_on_sw_off,True,None,Query 3,2,488,0.5507662467772694,Learning Unsupervised Word Translations Without Adversaries,EMNLP,2018,http://aclweb.org/anthology/D18-1063,Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embed...
BoW,bow_lc_on_sw_off,True,None,Query 3,3,152,0.5205554108665139,Learning Robust Representations of Text,EMNLP,2016,http://aclweb.org/anthology/D16-1207,"Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch),..."
BoW,bow_lc_on_sw_on,True,english,Query 1,1,300,0.5236525269065232,Memory-augmented Neural Machine Translation,EMNLP,2017,http://aclweb.org/anthology/D17-1146,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality..."
BoW,bow_lc_on_sw_on,True,english,Query 1,2,469,0.5236525269065232,Semi-Autoregressive Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1044,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality..."
BoW,bow_lc_on_sw_on,True,english,Query 1,3,762,0.5145791488562981,Prediction Improves Simultaneous Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1337,"We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine..."
BoW,bow_lc_on_sw_on,True,english,Query 2,1,258,0.3747010270857358,Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension,EMNLP,2017,http://aclweb.org/anthology/D17-1087,"We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about..."
BoW,bow_lc_on_sw_on,True,english,Query 2,2,660,0.37121293493384755,A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension,EMNLP,2018,http://aclweb.org/anthology/D18-1235,"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading compreh..."
BoW,bow_lc_on_sw_on,True,english,Query 2,3,852,0.3527244133780797,Answer-focused and Position-aware Neural Question Generation,EMNLP,2018,http://aclweb.org/anthology/D18-1427,Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions inc...
BoW,bow_lc_on_sw_on,True,english,Query 3,1,426,0.47379252522615356,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
BoW,bow_lc_on_sw_on,True,english,Query 3,2,152,0.36876442508001295,Learning Robust Representations of Text,EMNLP,2016,http://aclweb.org/anthology/D16-1207,"Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch),..."
BoW,bow_lc_on_sw_on,True,english,Query 3,3,488,0.3638527494714232,Learning Unsupervised Word Translations Without Adversaries,EMNLP,2018,http://aclweb.org/anthology/D18-1063,Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embed...
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 1,1,100,0.2907773189529845,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016,http://aclweb.org/anthology/D16-1134,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 1,2,762,0.29058009293344916,Prediction Improves Simultaneous Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1337,"We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine..."
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 1,3,936,0.2803094722629892,Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,EMNLP,2018,http://aclweb.org/anthology/D18-1512,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 2,1,660,0.29790442187541233,A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension,EMNLP,2018,http://aclweb.org/anthology/D18-1235,"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading compreh..."
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 2,2,255,0.2635249637133646,RACE: Large-scale ReAding Comprehension Dataset From Examinations,EMNLP,2017,http://aclweb.org/anthology/D17-1082,"We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, in..."
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 2,3,258,0.2565078480679707,Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension,EMNLP,2017,http://aclweb.org/anthology/D17-1087,"We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about..."
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 3,1,426,0.2525961505957454,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 3,2,780,0.23063617401927092,Integrating Transformer and Paraphrase Rules for Sentence Simplification,EMNLP,2018,http://aclweb.org/anthology/D18-1355,Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned...
TF-IDF,tfidf_lc_off_sw_off,False,None,Query 3,3,488,0.1961440037871336,Learning Unsupervised Word Translations Without Adversaries,EMNLP,2018,http://aclweb.org/anthology/D18-1063,Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embed...
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 1,1,762,0.2972915393327178,Prediction Improves Simultaneous Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1337,"We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine..."
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 1,2,100,0.2839022878178559,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016,http://aclweb.org/anthology/D16-1134,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 1,3,936,0.2748576879387061,Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,EMNLP,2018,http://aclweb.org/anthology/D18-1512,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 2,1,660,0.2906084860749794,A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension,EMNLP,2018,http://aclweb.org/anthology/D18-1235,"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading compreh..."
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 2,2,258,0.23774151349540107,Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension,EMNLP,2017,http://aclweb.org/anthology/D17-1087,"We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about..."
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 2,3,255,0.22387519719366147,RACE: Large-scale ReAding Comprehension Dataset From Examinations,EMNLP,2017,http://aclweb.org/anthology/D17-1082,"We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, in..."
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 3,1,426,0.24368725668825086,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 3,2,780,0.22114948453614292,Integrating Transformer and Paraphrase Rules for Sentence Simplification,EMNLP,2018,http://aclweb.org/anthology/D18-1355,Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned...
TF-IDF,tfidf_lc_off_sw_on,False,english,Query 3,3,488,0.17832160907948208,Learning Unsupervised Word Translations Without Adversaries,EMNLP,2018,http://aclweb.org/anthology/D18-1063,Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embed...
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 1,1,300,0.322673215362745,Memory-augmented Neural Machine Translation,EMNLP,2017,http://aclweb.org/anthology/D17-1146,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality..."
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 1,2,100,0.3213772766096021,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016,http://aclweb.org/anthology/D16-1134,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 1,3,469,0.3185318915932452,Semi-Autoregressive Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1044,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality..."
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 2,1,660,0.3713779342595086,A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension,EMNLP,2018,http://aclweb.org/anthology/D18-1235,"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading compreh..."
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 2,2,255,0.31461679580601487,RACE: Large-scale ReAding Comprehension Dataset From Examinations,EMNLP,2017,http://aclweb.org/anthology/D17-1082,"We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, in..."
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 2,3,194,0.2975039361103109,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",EMNLP,2016,http://aclweb.org/anthology/D16-1264,"We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine compreh..."
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 3,1,426,0.31895527371803856,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 3,2,780,0.2951421251739889,Integrating Transformer and Paraphrase Rules for Sentence Simplification,EMNLP,2018,http://aclweb.org/anthology/D18-1355,Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned...
TF-IDF,tfidf_lc_on_sw_off,True,None,Query 3,3,239,0.2335934703661715,Sentence Simplification with Deep Reinforcement Learning,EMNLP,2017,http://aclweb.org/anthology/D17-1062,Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simp...
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 1,1,300,0.34434948472037963,Memory-augmented Neural Machine Translation,EMNLP,2017,http://aclweb.org/anthology/D17-1146,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality..."
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 1,2,469,0.33926956537190667,Semi-Autoregressive Neural Machine Translation,EMNLP,2018,http://aclweb.org/anthology/D18-1044,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality..."
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 1,3,100,0.31493421482214257,HUME: Human UCCA-Based Evaluation of Machine Translation,EMNLP,2016,http://aclweb.org/anthology/D16-1134,Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation p...
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 2,1,660,0.3661475279488657,A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension,EMNLP,2018,http://aclweb.org/anthology/D18-1235,"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading compreh..."
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 2,2,255,0.27699150159494723,RACE: Large-scale ReAding Comprehension Dataset From Examinations,EMNLP,2017,http://aclweb.org/anthology/D17-1082,"We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, in..."
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 2,3,658,0.2639197819849893,Interpretation of Natural Language Rules in Conversational Machine Reading,EMNLP,2018,http://aclweb.org/anthology/D18-1233,"Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because..."
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 3,1,426,0.3025056669447462,Privacy-preserving Neural Representations of Text,EMNLP,2018,http://aclweb.org/anthology/D18-1001,"Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summ..."
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 3,2,780,0.28236511632470324,Integrating Transformer and Paraphrase Rules for Sentence Simplification,EMNLP,2018,http://aclweb.org/anthology/D18-1355,Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned...
TF-IDF,tfidf_lc_on_sw_on,True,english,Query 3,3,239,0.22469677620045392,Sentence Simplification with Deep Reinforcement Learning,EMNLP,2017,http://aclweb.org/anthology/D17-1062,Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simp...
