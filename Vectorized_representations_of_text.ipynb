{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBki8x4YoCi"
      },
      "source": [
        "# Using vectorized representations of text for semantic text similarity\n",
        "\n",
        "This notebook has been created to allow students of the TNLP 25/26 course to complete their assignment on vectorized representations of text. This notebook is provided with the minimal information to start working on the assignment. Students will have to follow the [instructions](https://mespla.github.io/tpln2526/assignment-searchinvectorialspace/) of the assignment reflecting in this notebook the work done.\n",
        "\n",
        "The starting point will be to install both the `scikit-learn` and the `sentence-embeddings` python libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3_yWE7DX_eD",
        "outputId": "6d65e29e-3902-49bb-fa9c-adb47e567c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (1.7.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.6.0+cpu)\n",
            "Requirement already satisfied: scipy in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.34.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tgcVmtaYmkN"
      },
      "source": [
        "Once this is done, students will have to download and read the file containing the dataset consisting of a list of scientific paper's title and abstract.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWRIDdL4YknO",
        "outputId": "f1e5a49b-e8e3-4a77-d5c3-021740a6963a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Cache] Using existing file: emnlp2016-2018.json\n",
            "\n",
            "[Provenance]\n",
            " - Source URL: https://sbert.net/datasets/emnlp2016-2018.json\n",
            " - Local path: emnlp2016-2018.json\n",
            " - File size : 1.05 MB\n",
            " - SHA-256   : 9e6020503e5f0dd0e91dbb970d47f43c7621f67321249daed5990057e159961c\n",
            "\n",
            "[Sanity checks]\n",
            " - N papers loaded: 974\n",
            " - Key presence: OK (all required keys found in all records)\n",
            " - Empty titles   : 0\n",
            " - Empty abstracts: 0\n",
            " - Non-string titles   : 0\n",
            " - Non-string abstracts: 0\n",
            " - Year distribution (top): [(2018, 549), (2017, 230), (2016, 195)]\n",
            " - Years outside 2016–2018: 0\n",
            " - Venue distribution (top): [('EMNLP', 974)]\n",
            " - Duplicate URL keys: 0 (unique non-empty URLs: 974)\n",
            "\n",
            "[Access check — first record]\n",
            " - title   : Rule Extraction for Tree-to-Tree Transducers by Cost Minimization\n",
            " - abstract: Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set...\n",
            " - url     : http://aclweb.org/anthology/D16-1002\n",
            " - venue   : EMNLP\n",
            " - year    : 2016\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 2 — Dataset Acquisition & Loading (EMNLP 2016–2018 JSON)\n",
        "# =========================\n",
        "\n",
        "import os, json, hashlib\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "DATA_URL = \"https://sbert.net/datasets/emnlp2016-2018.json\"\n",
        "DATA_PATH = \"emnlp2016-2018.json\"   # Feel free to rename, but keep it consistent across the notebook\n",
        "FORCE_DOWNLOAD = False             # Set True if you want to re-download even if the file exists\n",
        "\n",
        "def sha256_of_file(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def download_json(url: str, out_path: str, timeout: int = 120) -> None:\n",
        "    r = requests.get(url, stream=True, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "# --- (1) Acquire the dataset file (download if missing) ---\n",
        "if FORCE_DOWNLOAD or (not os.path.exists(DATA_PATH)):\n",
        "    print(f\"[Download] Fetching dataset from: {DATA_URL}\")\n",
        "    download_json(DATA_URL, DATA_PATH)\n",
        "    print(f\"[Download] Saved to: {DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"[Cache] Using existing file: {DATA_PATH}\")\n",
        "\n",
        "# --- (2) Load JSON into memory ---\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    papers = json.load(f)\n",
        "\n",
        "# Alias for compatibility with the starter template cells\n",
        "data = papers\n",
        "\n",
        "# --- (3) Validate expected JSON structure: root must be a list of dicts ---\n",
        "if not isinstance(papers, list):\n",
        "    raise TypeError(f\"Expected JSON root to be a list, got: {type(papers)}\")\n",
        "\n",
        "if len(papers) == 0:\n",
        "    raise ValueError(\"Dataset loaded but it is empty (N=0). File may be corrupted or not the expected dataset.\")\n",
        "\n",
        "if not isinstance(papers[0], dict):\n",
        "    raise TypeError(f\"Expected each item to be a dict, got first item type: {type(papers[0])}\")\n",
        "\n",
        "# --- (4) Provenance info (practical traceability) ---\n",
        "file_size_mb = os.path.getsize(DATA_PATH) / (1024 * 1024)\n",
        "print(f\"\\n[Provenance]\")\n",
        "print(f\" - Source URL: {DATA_URL}\")\n",
        "print(f\" - Local path: {DATA_PATH}\")\n",
        "print(f\" - File size : {file_size_mb:.2f} MB\")\n",
        "print(f\" - SHA-256   : {sha256_of_file(DATA_PATH)}\")\n",
        "\n",
        "# --- (5) Mandatory sanity checks ---\n",
        "N = len(papers)\n",
        "print(f\"\\n[Sanity checks]\")\n",
        "print(f\" - N papers loaded: {N}\")\n",
        "\n",
        "required_keys = {\"title\", \"abstract\", \"url\", \"venue\", \"year\"}\n",
        "missing_key_counts = Counter()\n",
        "\n",
        "empty_title = 0\n",
        "empty_abstract = 0\n",
        "non_string_title = 0\n",
        "non_string_abstract = 0\n",
        "\n",
        "years = []\n",
        "venues = []\n",
        "urls = []\n",
        "\n",
        "for p in papers:\n",
        "    # Key presence\n",
        "    for k in required_keys:\n",
        "        if k not in p:\n",
        "            missing_key_counts[k] += 1\n",
        "\n",
        "    # Title / abstract existence + type + emptiness\n",
        "    t = p.get(\"title\", None)\n",
        "    a = p.get(\"abstract\", None)\n",
        "\n",
        "    if not isinstance(t, str):\n",
        "        non_string_title += 1\n",
        "        t = \"\" if t is None else str(t)\n",
        "    if not isinstance(a, str):\n",
        "        non_string_abstract += 1\n",
        "        a = \"\" if a is None else str(a)\n",
        "\n",
        "    if len(t.strip()) == 0:\n",
        "        empty_title += 1\n",
        "    if len(a.strip()) == 0:\n",
        "        empty_abstract += 1\n",
        "\n",
        "    # Metadata distributions (for plausibility checks)\n",
        "    venues.append(str(p.get(\"venue\", \"\")).strip())\n",
        "    urls.append(str(p.get(\"url\", \"\")).strip())\n",
        "\n",
        "    y = p.get(\"year\", None)\n",
        "    try:\n",
        "        years.append(int(y))\n",
        "    except Exception:\n",
        "        years.append(None)\n",
        "\n",
        "# Report key presence\n",
        "if sum(missing_key_counts.values()) == 0:\n",
        "    print(\" - Key presence: OK (all required keys found in all records)\")\n",
        "else:\n",
        "    print(\" - Key presence issues:\")\n",
        "    for k in sorted(required_keys):\n",
        "        if missing_key_counts[k] > 0:\n",
        "            print(f\"   * Missing '{k}': {missing_key_counts[k]} records\")\n",
        "\n",
        "# Report title/abstract health\n",
        "print(f\" - Empty titles   : {empty_title}\")\n",
        "print(f\" - Empty abstracts: {empty_abstract}\")\n",
        "print(f\" - Non-string titles   : {non_string_title}\")\n",
        "print(f\" - Non-string abstracts: {non_string_abstract}\")\n",
        "\n",
        "# Year plausibility\n",
        "valid_years = [y for y in years if isinstance(y, int)]\n",
        "year_counts = Counter(valid_years)\n",
        "print(f\" - Year distribution (top): {year_counts.most_common(5)}\")\n",
        "\n",
        "outside = [y for y in valid_years if y < 2016 or y > 2018]\n",
        "print(f\" - Years outside 2016–2018: {len(outside)}\")\n",
        "\n",
        "# Venue plausibility\n",
        "venue_counts = Counter([v for v in venues if v])\n",
        "print(f\" - Venue distribution (top): {venue_counts.most_common(5)}\")\n",
        "\n",
        "# Duplicate URLs (very practical duplicate detector)\n",
        "url_counts = Counter([u for u in urls if u])\n",
        "dupe_urls = sum(1 for u, c in url_counts.items() if c > 1)\n",
        "print(f\" - Duplicate URL keys: {dupe_urls} (unique non-empty URLs: {len(url_counts)})\")\n",
        "\n",
        "# --- (6) Access check (prove we can read title/abstract) ---\n",
        "print(f\"\\n[Access check — first record]\")\n",
        "print(f\" - title   : {papers[0].get('title','')[:120]}{'...' if len(papers[0].get('title',''))>120 else ''}\")\n",
        "print(f\" - abstract: {papers[0].get('abstract','')[:200]}{'...' if len(papers[0].get('abstract',''))>200 else ''}\")\n",
        "print(f\" - url     : {papers[0].get('url','')}\")\n",
        "print(f\" - venue   : {papers[0].get('venue','')}\")\n",
        "print(f\" - year    : {papers[0].get('year','')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5a7FAOehkh-"
      },
      "source": [
        "## Part 1:\n",
        "From this point, students should be able to obtain BoW and TF-IDF representations of the dataset, and to obtain similar matches for the new scientific paper titles in the instructions of the exercise. Include here the code use to build the representations, as well as the discussion on the results obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i-tWKGq3LQd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (2.2.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05P6MYR3lLok"
      },
      "source": [
        "###Step 1: Preprocessing the Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section3_preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section3_preprocessing.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    _HAS_PANDAS = True\n",
        "except Exception:\n",
        "    pd = None  # type: ignore\n",
        "    _HAS_PANDAS = False\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PrepConfig:\n",
        "    title_key: str = \"title\"\n",
        "    abstract_key: str = \"abstract\"\n",
        "    joiner: str = \" \"\n",
        "    make_dataframe: bool = True\n",
        "\n",
        "\n",
        "def _as_text(x: Any) -> str:\n",
        "    \"\"\"Minimal, non-aggressive coercion: keep text as-is (no casing/punct stripping).\"\"\"\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    return str(x)\n",
        "\n",
        "\n",
        "def build_paper_texts(\n",
        "    records: Sequence[Dict[str, Any]],\n",
        "    config: PrepConfig = PrepConfig(),\n",
        ") -> Tuple[List[str], Optional[\"pd.DataFrame\"]]:\n",
        "    \"\"\"\n",
        "    Build the working 'text' field exactly as required:\n",
        "        text = title + \" \" + abstract\n",
        "\n",
        "    Document unit: one paper = one document.\n",
        "    Returns:\n",
        "      - paper_texts: list[str] with one concatenated document per paper\n",
        "      - df (optional): pandas DataFrame including title/abstract/url/venue/year/text\n",
        "    \"\"\"\n",
        "    if not isinstance(records, (list, tuple)):\n",
        "        raise TypeError(f\"records must be a sequence (list/tuple) of dicts, got {type(records)}\")\n",
        "\n",
        "    paper_texts: List[str] = []\n",
        "    rows_for_df: List[Dict[str, Any]] = []\n",
        "\n",
        "    for i, rec in enumerate(records):\n",
        "        if not isinstance(rec, dict):\n",
        "            raise TypeError(f\"Each record must be a dict. Found {type(rec)} at index {i}.\")\n",
        "\n",
        "        title = _as_text(rec.get(config.title_key, \"\"))\n",
        "        abstract = _as_text(rec.get(config.abstract_key, \"\"))\n",
        "\n",
        "        # Exact construction rule (single space joiner)\n",
        "        text = title + config.joiner + abstract\n",
        "\n",
        "        paper_texts.append(text)\n",
        "\n",
        "        if config.make_dataframe:\n",
        "            rows_for_df.append(\n",
        "                {\n",
        "                    \"title\": title,\n",
        "                    \"abstract\": abstract,\n",
        "                    \"url\": rec.get(\"url\", \"\"),\n",
        "                    \"venue\": rec.get(\"venue\", \"\"),\n",
        "                    \"year\": rec.get(\"year\", \"\"),\n",
        "                    \"text\": text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    df_out = None\n",
        "    if config.make_dataframe:\n",
        "        if not _HAS_PANDAS:\n",
        "            raise ImportError(\"pandas is required for make_dataframe=True, but it is not available.\")\n",
        "        df_out = pd.DataFrame(rows_for_df)\n",
        "\n",
        "    return paper_texts, df_out\n",
        "\n",
        "\n",
        "def validate_paper_texts(paper_texts: Sequence[str], expected_n: int) -> None:\n",
        "    \"\"\"Sanity checks for Section 3 outputs.\"\"\"\n",
        "    if len(paper_texts) != expected_n:\n",
        "        raise AssertionError(f\"len(paper_texts)={len(paper_texts)} != expected_n={expected_n}\")\n",
        "    if any(not isinstance(t, str) for t in paper_texts):\n",
        "        raise AssertionError(\"paper_texts must contain only strings.\")\n",
        "    if any(len(t) == 0 for t in paper_texts):\n",
        "        raise AssertionError(\"paper_texts contains empty strings (unexpected for this dataset).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 3] Preprocessing complete.\n",
            " - Document unit          : 1 paper = 1 document\n",
            " - N documents (paper_texts): 974\n",
            " - DataFrame created      : True\n",
            "\n",
            "[Section 3] Example document (first record):\n",
            "Rule Extraction for Tree-to-Tree Transducers by Cost Minimization Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the ...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>url</th>\n",
              "      <th>venue</th>\n",
              "      <th>year</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rule Extraction for Tree-to-Tree Transducers b...</td>\n",
              "      <td>Finite-state transducers give efficient repres...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1002</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>Rule Extraction for Tree-to-Tree Transducers b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Neural Network for Coordination Boundary Pre...</td>\n",
              "      <td>We propose a neural-network based model for co...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1003</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>A Neural Network for Coordination Boundary Pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Distinguishing Past, On-going, and Future Even...</td>\n",
              "      <td>The tremendous amount of user generated data t...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1005</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>Distinguishing Past, On-going, and Future Even...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  Rule Extraction for Tree-to-Tree Transducers b...   \n",
              "1  A Neural Network for Coordination Boundary Pre...   \n",
              "2  Distinguishing Past, On-going, and Future Even...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  Finite-state transducers give efficient repres...   \n",
              "1  We propose a neural-network based model for co...   \n",
              "2  The tremendous amount of user generated data t...   \n",
              "\n",
              "                                    url  venue  year  \\\n",
              "0  http://aclweb.org/anthology/D16-1002  EMNLP  2016   \n",
              "1  http://aclweb.org/anthology/D16-1003  EMNLP  2016   \n",
              "2  http://aclweb.org/anthology/D16-1005  EMNLP  2016   \n",
              "\n",
              "                                                text  \n",
              "0  Rule Extraction for Tree-to-Tree Transducers b...  \n",
              "1  A Neural Network for Coordination Boundary Pre...  \n",
              "2  Distinguishing Past, On-going, and Future Even...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 3 — Data Preparation for Vectorization (Part 1 – Step 1)\n",
        "# =========================\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "# Use the variable created in Section 2 (you already set: data = papers)\n",
        "records = data  # list of dicts, one paper per record\n",
        "\n",
        "config = PrepConfig(\n",
        "    title_key=\"title\",\n",
        "    abstract_key=\"abstract\",\n",
        "    joiner=\" \",          # MUST be exactly one space\n",
        "    make_dataframe=True  # optional, but very useful in a notebook\n",
        ")\n",
        "\n",
        "paper_texts, papers_df = build_paper_texts(records, config=config)\n",
        "\n",
        "# Robust sanity checks\n",
        "N = len(records)\n",
        "validate_paper_texts(paper_texts, expected_n=N)\n",
        "\n",
        "print(\"[Section 3] Preprocessing complete.\")\n",
        "print(f\" - Document unit          : 1 paper = 1 document\")\n",
        "print(f\" - N documents (paper_texts): {len(paper_texts)}\")\n",
        "print(f\" - DataFrame created      : {papers_df is not None}\")\n",
        "print(\"\\n[Section 3] Example document (first record):\")\n",
        "print(paper_texts[0][:400] + (\"...\" if len(paper_texts[0]) > 400 else \"\"))\n",
        "\n",
        "# Optional: quick inspection (keeps later steps easier)\n",
        "if papers_df is not None:\n",
        "    display(papers_df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_section3_preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section3_preprocessing.py\n",
        "import unittest\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)  # should not raise\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        # title becomes \"123\", abstract becomes \"\"\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "    def test_dataframe_creation_if_enabled(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"A\", \"url\": \"u\", \"venue\": \"EMNLP\", \"year\": 2016}]\n",
        "        cfg = PrepConfig(make_dataframe=True)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNotNone(df)\n",
        "        self.assertIn(\"text\", df.columns)\n",
        "        self.assertEqual(df.loc[0, \"text\"], paper_texts[0])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_section3_preprocessing.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_dataframe_creation_if_enabled (test_section3_preprocessing.TestSection3Preprocessing.test_dataframe_creation_if_enabled) ... ok\n",
            "test_rejects_non_dict_records (test_section3_preprocessing.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_section3_preprocessing.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_section3_preprocessing.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.002s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_section3_preprocessing.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLiO06ahlU5O"
      },
      "source": [
        "###Step 2: Building the BoW and TF-IDF Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section4_bow.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section4_bow.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BoWConfig:\n",
        "    \"\"\"\n",
        "    Configuration for a BoW (CountVectorizer) run.\n",
        "\n",
        "    Required toggles for the assignment:\n",
        "      - lowercase: ON vs OFF\n",
        "      - stop_words: None vs 'english'\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    lowercase: bool = True\n",
        "    stop_words: Optional[str] = None\n",
        "    max_features: Optional[int] = None\n",
        "    ngram_range: Tuple[int, int] = (1, 1)\n",
        "\n",
        "\n",
        "def _validate_texts(texts: Sequence[str]) -> None:\n",
        "    if not isinstance(texts, (list, tuple)):\n",
        "        raise TypeError(f\"texts must be a list/tuple of strings, got {type(texts)}\")\n",
        "    if len(texts) == 0:\n",
        "        raise ValueError(\"texts is empty (N=0).\")\n",
        "    if any(not isinstance(t, str) for t in texts):\n",
        "        bad = next(type(t) for t in texts if not isinstance(t, str))\n",
        "        raise TypeError(f\"All texts must be strings. Found non-string: {bad}\")\n",
        "    if any(len(t) == 0 for t in texts):\n",
        "        raise ValueError(\"texts contains empty strings (unexpected for this dataset).\")\n",
        "\n",
        "\n",
        "def build_bow_vectors(\n",
        "    texts: Sequence[str],\n",
        "    config: BoWConfig,\n",
        "):\n",
        "    \"\"\"\n",
        "    Fit a CountVectorizer and return:\n",
        "      - vectorizer (fitted)\n",
        "      - X (document-term matrix, sparse)\n",
        "    \"\"\"\n",
        "    _validate_texts(texts)\n",
        "\n",
        "    vectorizer = CountVectorizer(\n",
        "        lowercase=config.lowercase,\n",
        "        stop_words=config.stop_words,\n",
        "        max_features=config.max_features,\n",
        "        ngram_range=config.ngram_range,\n",
        "    )\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return vectorizer, X\n",
        "\n",
        "\n",
        "def sparse_matrix_stats(X) -> Dict[str, Any]:\n",
        "    \"\"\"Compute basic inspection stats for a sparse doc-term matrix.\"\"\"\n",
        "    n_docs, n_vocab = X.shape\n",
        "    nnz = int(X.nnz)\n",
        "    total = int(n_docs) * int(n_vocab)\n",
        "    density = float(nnz / total) if total > 0 else 0.0\n",
        "    avg_nnz_per_doc = float(nnz / n_docs) if n_docs > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"n_docs\": int(n_docs),\n",
        "        \"n_vocab\": int(n_vocab),\n",
        "        \"shape\": (int(n_docs), int(n_vocab)),\n",
        "        \"nnz\": nnz,\n",
        "        \"density\": density,\n",
        "        \"avg_nnz_per_doc\": avg_nnz_per_doc,\n",
        "        \"matrix_type\": type(X).__name__,\n",
        "    }\n",
        "\n",
        "\n",
        "def feature_examples(vectorizer: CountVectorizer, n: int = 20) -> List[str]:\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    return feats[:n].tolist()\n",
        "\n",
        "\n",
        "def case_sensitive_stopword_survivors(vectorizer: CountVectorizer) -> int:\n",
        "    \"\"\"\n",
        "    Detect the nuance: with lowercase=False and stop_words='english',\n",
        "    capitalized variants of stopwords (e.g., 'The') can survive.\n",
        "\n",
        "    Returns how many vocabulary terms are \"stopwords after lowercasing\"\n",
        "    but are not exactly in the stopword list (case mismatch).\n",
        "    \"\"\"\n",
        "    stop = vectorizer.get_stop_words()\n",
        "    if not stop:\n",
        "        return 0\n",
        "\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    stop_set = set(stop)\n",
        "\n",
        "    survivors = 0\n",
        "    for f in feats:\n",
        "        # survivors like \"The\" where f.lower() in stop_set but f not in stop_set\n",
        "        fl = f.lower()\n",
        "        if fl in stop_set and f not in stop_set:\n",
        "            survivors += 1\n",
        "    return survivors\n",
        "\n",
        "\n",
        "def run_bow_grid(texts: Sequence[str]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run the 2x2 grid required by the assignment:\n",
        "      - lowercase ON/OFF\n",
        "      - stopwords OFF/ON ('english')\n",
        "    Returns a dict keyed by config name with vectorizer, X, and stats.\n",
        "    \"\"\"\n",
        "    configs = [\n",
        "        BoWConfig(name=\"bow_lc_on_sw_off\", lowercase=True,  stop_words=None),\n",
        "        BoWConfig(name=\"bow_lc_on_sw_on\",  lowercase=True,  stop_words=\"english\"),\n",
        "        BoWConfig(name=\"bow_lc_off_sw_off\",lowercase=False, stop_words=None),\n",
        "        BoWConfig(name=\"bow_lc_off_sw_on\", lowercase=False, stop_words=\"english\"),\n",
        "    ]\n",
        "\n",
        "    results: Dict[str, Dict[str, Any]] = {}\n",
        "    for cfg in configs:\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        stats = sparse_matrix_stats(X)\n",
        "        survivors = case_sensitive_stopword_survivors(vect) if cfg.stop_words else 0\n",
        "\n",
        "        results[cfg.name] = {\n",
        "            \"config\": cfg,\n",
        "            \"vectorizer\": vect,\n",
        "            \"X\": X,\n",
        "            \"stats\": stats,\n",
        "            \"case_sensitive_stopword_survivors\": survivors,\n",
        "        }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "x31erbZHlp57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 4] BoW experiments complete.\n",
            "Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\n",
            "Each column is a token; each cell is the token count in that document.\n",
            "\n",
            "--- bow_lc_on_sw_off ---\n",
            "Config: lowercase=True, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 8404)  (#docs x #vocab)\n",
            "nnz         : 90157\n",
            "Density     : 0.011014\n",
            "Avg nnz/doc : 92.56\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n",
            "--- bow_lc_on_sw_on ---\n",
            "Config: lowercase=True, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 8153)  (#docs x #vocab)\n",
            "nnz         : 64523\n",
            "Density     : 0.008125\n",
            "Avg nnz/doc : 66.25\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n",
            "--- bow_lc_off_sw_off ---\n",
            "Config: lowercase=False, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 10342)  (#docs x #vocab)\n",
            "nnz         : 96257\n",
            "Density     : 0.009556\n",
            "Avg nnz/doc : 98.83\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n",
            "--- bow_lc_off_sw_on ---\n",
            "Config: lowercase=False, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 10094)  (#docs x #vocab)\n",
            "nnz         : 72143\n",
            "Density     : 0.007338\n",
            "Avg nnz/doc : 74.07\n",
            "Stopword casing nuance: 155 vocab terms look like stopwords after lowercasing (e.g., 'The')\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run</th>\n",
              "      <th>lowercase</th>\n",
              "      <th>stop_words</th>\n",
              "      <th>N_docs</th>\n",
              "      <th>V_vocab</th>\n",
              "      <th>nnz</th>\n",
              "      <th>density</th>\n",
              "      <th>avg_nnz_per_doc</th>\n",
              "      <th>case_sensitive_stopword_survivors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bow_lc_on_sw_off</td>\n",
              "      <td>True</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>8404</td>\n",
              "      <td>90157</td>\n",
              "      <td>0.011014</td>\n",
              "      <td>92.563655</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bow_lc_on_sw_on</td>\n",
              "      <td>True</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>8153</td>\n",
              "      <td>64523</td>\n",
              "      <td>0.008125</td>\n",
              "      <td>66.245380</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bow_lc_off_sw_off</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>10342</td>\n",
              "      <td>96257</td>\n",
              "      <td>0.009556</td>\n",
              "      <td>98.826489</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bow_lc_off_sw_on</td>\n",
              "      <td>False</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>10094</td>\n",
              "      <td>72143</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>74.068789</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 run  lowercase stop_words  N_docs  V_vocab    nnz   density  \\\n",
              "0   bow_lc_on_sw_off       True       None     974     8404  90157  0.011014   \n",
              "1    bow_lc_on_sw_on       True    english     974     8153  64523  0.008125   \n",
              "2  bow_lc_off_sw_off      False       None     974    10342  96257  0.009556   \n",
              "3   bow_lc_off_sw_on      False    english     974    10094  72143  0.007338   \n",
              "\n",
              "   avg_nnz_per_doc  case_sensitive_stopword_survivors  \n",
              "0        92.563655                                  0  \n",
              "1        66.245380                                  0  \n",
              "2        98.826489                                  0  \n",
              "3        74.068789                                155  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 4] Default BoW run set for later use: bow_lc_on_sw_off\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 4 — BoW Vectorization Experiments (Part 1 – Step 2, BoW branch)\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from section4_bow import run_bow_grid, feature_examples\n",
        "\n",
        "# Input from Section 3:\n",
        "# - paper_texts: list[str] where each element is \"title + ' ' + abstract\"\n",
        "texts = paper_texts\n",
        "\n",
        "bow_results = run_bow_grid(texts)\n",
        "\n",
        "# Deterministic order (avoids confusing output + makes comparison easier)\n",
        "RUN_ORDER = [\"bow_lc_on_sw_off\", \"bow_lc_on_sw_on\", \"bow_lc_off_sw_off\", \"bow_lc_off_sw_on\"]\n",
        "\n",
        "def vocab_letter_sample(vectorizer, n: int = 20):\n",
        "    \"\"\"Return the first n features that contain at least one alphabetic character.\"\"\"\n",
        "    feats = vectorizer.get_feature_names_out().tolist()\n",
        "    letter_feats = [f for f in feats if re.search(r\"[A-Za-z]\", f)]\n",
        "    return letter_feats[:n]\n",
        "\n",
        "print(\"[Section 4] BoW experiments complete.\")\n",
        "print(\"Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\")\n",
        "print(\"Each column is a token; each cell is the token count in that document.\\n\")\n",
        "\n",
        "# Print compact inspection per configuration (stable ordering + hard sanity checks)\n",
        "for name in RUN_ORDER:\n",
        "    out = bow_results[name]\n",
        "    cfg = out[\"config\"]\n",
        "    stats = out[\"stats\"]\n",
        "    survivors = out[\"case_sensitive_stopword_survivors\"]\n",
        "\n",
        "    # Robust invariants (fail fast if something is wrong)\n",
        "    if stats[\"n_docs\"] != len(texts):\n",
        "        raise RuntimeError(\n",
        "            f\"Row mismatch in {name}: X has {stats['n_docs']} rows but texts has {len(texts)} documents\"\n",
        "        )\n",
        "    if stats[\"n_vocab\"] <= 0 or stats[\"nnz\"] <= 0:\n",
        "        raise RuntimeError(\n",
        "            f\"Degenerate BoW matrix in {name}: n_vocab={stats['n_vocab']}, nnz={stats['nnz']}\"\n",
        "        )\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"Config: lowercase={cfg.lowercase}, stop_words={cfg.stop_words}\")\n",
        "    print(f\"Matrix type : {stats['matrix_type']} (sparse)\")\n",
        "    print(f\"Shape       : {stats['shape']}  (#docs x #vocab)\")\n",
        "    print(f\"nnz         : {stats['nnz']}\")\n",
        "    print(f\"Density     : {stats['density']:.6f}\")\n",
        "    print(f\"Avg nnz/doc : {stats['avg_nnz_per_doc']:.2f}\")\n",
        "\n",
        "    # Stopword casing nuance indicator (only relevant when lowercase=False and stopwords ON)\n",
        "    if cfg.stop_words is not None and cfg.lowercase is False:\n",
        "        print(\n",
        "            f\"Stopword casing nuance: {survivors} vocab terms look like stopwords after lowercasing (e.g., 'The')\"\n",
        "        )\n",
        "\n",
        "    # Two samples: (1) raw first tokens (often numeric), (2) a more meaningful sample with letters\n",
        "    print(f\"Vocabulary sample (first 20 terms): {feature_examples(out['vectorizer'], n=20)}\")\n",
        "    print(f\"Vocabulary sample with letters (first 20): {vocab_letter_sample(out['vectorizer'], n=20)}\\n\")\n",
        "\n",
        "# Optional: a small summary table (nice for the report)\n",
        "try:\n",
        "    import pandas as pd\n",
        "\n",
        "    rows = []\n",
        "    for name in RUN_ORDER:\n",
        "        out = bow_results[name]\n",
        "        cfg = out[\"config\"]\n",
        "        s = out[\"stats\"]\n",
        "        rows.append(\n",
        "            {\n",
        "                \"run\": name,\n",
        "                \"lowercase\": cfg.lowercase,\n",
        "                \"stop_words\": cfg.stop_words,\n",
        "                \"N_docs\": s[\"n_docs\"],\n",
        "                \"V_vocab\": s[\"n_vocab\"],\n",
        "                \"nnz\": s[\"nnz\"],\n",
        "                \"density\": s[\"density\"],\n",
        "                \"avg_nnz_per_doc\": s[\"avg_nnz_per_doc\"],\n",
        "                \"case_sensitive_stopword_survivors\": out[\"case_sensitive_stopword_survivors\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    bow_summary_df = pd.DataFrame(rows)\n",
        "    bow_summary_df[\"run\"] = pd.Categorical(bow_summary_df[\"run\"], categories=RUN_ORDER, ordered=True)\n",
        "    bow_summary_df = bow_summary_df.sort_values(by=[\"run\"]).reset_index(drop=True)\n",
        "    display(bow_summary_df)\n",
        "\n",
        "except Exception:\n",
        "    bow_summary_df = None\n",
        "\n",
        "# Provide a default BoW representation for later sections if you want:\n",
        "# (This prevents NameError later if you decide to use BoW first in Step 3.)\n",
        "DEFAULT_BOW_RUN = \"bow_lc_on_sw_off\"\n",
        "bow_vectorizer = bow_results[DEFAULT_BOW_RUN][\"vectorizer\"]\n",
        "bow_paper_vectors = bow_results[DEFAULT_BOW_RUN][\"X\"]\n",
        "print(f\"[Section 4] Default BoW run set for later use: {DEFAULT_BOW_RUN}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import unittest\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "from section4_bow import (\n",
        "    BoWConfig,\n",
        "    build_bow_vectors,\n",
        "    sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors,\n",
        ")\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)  # should not raise\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "\n",
        "class TestSection4BoW(unittest.TestCase):\n",
        "    def test_build_bow_vectors_shape_and_sparse(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = BoWConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))  # sparse matrices expose nnz\n",
        "        self.assertTrue(hasattr(vect, \"vocabulary_\"))\n",
        "\n",
        "    def test_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, X_off = build_bow_vectors(texts, BoWConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, X_on = build_bow_vectors(texts, BoWConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "        self.assertEqual(X_off.shape[0], X_on.shape[0])\n",
        "\n",
        "    def test_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_bow_vectors(texts, BoWConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_bow_vectors(texts, BoWConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]  # 'The' may survive when lowercase=False with english stopwords\n",
        "        vect, _ = build_bow_vectors(texts, BoWConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_sparse_stats_sane(self):\n",
        "        texts = [\"cat sat\", \"dog barked\"]\n",
        "        vect, X = build_bow_vectors(texts, BoWConfig(name=\"s\", lowercase=True, stop_words=None))\n",
        "        s = sparse_matrix_stats(X)\n",
        "        self.assertEqual(s[\"n_docs\"], 2)\n",
        "        self.assertGreater(s[\"n_vocab\"], 0)\n",
        "        self.assertGreaterEqual(s[\"density\"], 0.0)\n",
        "        self.assertLessEqual(s[\"density\"], 1.0)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_all.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_rejects_non_dict_records (test_all.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_all.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_all.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "test_build_bow_vectors_shape_and_sparse (test_all.TestSection4BoW.test_build_bow_vectors_shape_and_sparse) ... ok\n",
            "test_case_sensitive_stopword_survivors_detected (test_all.TestSection4BoW.test_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_lowercase_merges_tokens (test_all.TestSection4BoW.test_lowercase_merges_tokens) ... ok\n",
            "test_sparse_stats_sane (test_all.TestSection4BoW.test_sparse_stats_sane) ... ok\n",
            "test_stopwords_removed_when_lowercase_true (test_all.TestSection4BoW.test_stopwords_removed_when_lowercase_true) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 9 tests in 0.006s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_all.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62WFYhu7lqRi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section5_tfidf.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section5_tfidf.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TfidfConfig:\n",
        "    \"\"\"\n",
        "    Configuration for a TF-IDF (TfidfVectorizer) run.\n",
        "\n",
        "    Required toggles for the assignment:\n",
        "      - lowercase: ON vs OFF\n",
        "      - stop_words: None vs 'english'\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    lowercase: bool = True\n",
        "    stop_words: Optional[str] = None\n",
        "    max_features: Optional[int] = None\n",
        "    ngram_range: Tuple[int, int] = (1, 1)\n",
        "\n",
        "    # Keep sklearn defaults explicit for clarity in the report\n",
        "    norm: Optional[str] = \"l2\"\n",
        "    use_idf: bool = True\n",
        "    smooth_idf: bool = True\n",
        "    sublinear_tf: bool = False\n",
        "\n",
        "\n",
        "def _validate_texts(texts: Sequence[str]) -> None:\n",
        "    if not isinstance(texts, (list, tuple)):\n",
        "        raise TypeError(f\"texts must be a list/tuple of strings, got {type(texts)}\")\n",
        "    if len(texts) == 0:\n",
        "        raise ValueError(\"texts is empty (N=0).\")\n",
        "    if any(not isinstance(t, str) for t in texts):\n",
        "        bad = next(type(t) for t in texts if not isinstance(t, str))\n",
        "        raise TypeError(f\"All texts must be strings. Found non-string: {bad}\")\n",
        "    if any(len(t) == 0 for t in texts):\n",
        "        raise ValueError(\"texts contains empty strings (unexpected for this dataset).\")\n",
        "\n",
        "\n",
        "def build_tfidf_vectors(\n",
        "    texts: Sequence[str],\n",
        "    config: TfidfConfig,\n",
        "):\n",
        "    \"\"\"\n",
        "    Fit a TfidfVectorizer and return:\n",
        "      - vectorizer (fitted)\n",
        "      - X (document-term TF-IDF matrix, sparse)\n",
        "    \"\"\"\n",
        "    _validate_texts(texts)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        lowercase=config.lowercase,\n",
        "        stop_words=config.stop_words,\n",
        "        max_features=config.max_features,\n",
        "        ngram_range=config.ngram_range,\n",
        "        norm=config.norm,\n",
        "        use_idf=config.use_idf,\n",
        "        smooth_idf=config.smooth_idf,\n",
        "        sublinear_tf=config.sublinear_tf,\n",
        "    )\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return vectorizer, X\n",
        "\n",
        "\n",
        "def sparse_matrix_stats(X) -> Dict[str, Any]:\n",
        "    \"\"\"Compute basic inspection stats for a sparse doc-term matrix.\"\"\"\n",
        "    n_docs, n_vocab = X.shape\n",
        "    nnz = int(X.nnz)\n",
        "    total = int(n_docs) * int(n_vocab)\n",
        "    density = float(nnz / total) if total > 0 else 0.0\n",
        "    avg_nnz_per_doc = float(nnz / n_docs) if n_docs > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"n_docs\": int(n_docs),\n",
        "        \"n_vocab\": int(n_vocab),\n",
        "        \"shape\": (int(n_docs), int(n_vocab)),\n",
        "        \"nnz\": nnz,\n",
        "        \"density\": density,\n",
        "        \"avg_nnz_per_doc\": avg_nnz_per_doc,\n",
        "        \"matrix_type\": type(X).__name__,\n",
        "        \"dtype\": str(getattr(X, \"dtype\", \"unknown\")),\n",
        "    }\n",
        "\n",
        "\n",
        "def feature_examples(vectorizer: TfidfVectorizer, n: int = 20) -> List[str]:\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    return feats[:n].tolist()\n",
        "\n",
        "\n",
        "def case_sensitive_stopword_survivors(vectorizer: TfidfVectorizer) -> int:\n",
        "    \"\"\"\n",
        "    Detect the nuance: with lowercase=False and stop_words='english',\n",
        "    capitalized variants of stopwords (e.g., 'The') can survive.\n",
        "    \"\"\"\n",
        "    stop = vectorizer.get_stop_words()\n",
        "    if not stop:\n",
        "        return 0\n",
        "\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    stop_set = set(stop)\n",
        "\n",
        "    survivors = 0\n",
        "    for f in feats:\n",
        "        fl = f.lower()\n",
        "        if fl in stop_set and f not in stop_set:\n",
        "            survivors += 1\n",
        "    return survivors\n",
        "\n",
        "\n",
        "def l2_norm_sample_stats(X, sample_n: int = 50, seed: int = 0) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    TF-IDF vectors in sklearn are L2-normalized by default (norm='l2').\n",
        "    This helper checks that empirically on a sample of rows.\n",
        "    \"\"\"\n",
        "    n_docs = X.shape[0]\n",
        "    if n_docs == 0:\n",
        "        return {\"min\": 0.0, \"mean\": 0.0, \"max\": 0.0}\n",
        "\n",
        "    sample_n = int(min(sample_n, n_docs))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.choice(n_docs, size=sample_n, replace=False)\n",
        "\n",
        "    Xs = X[idx]\n",
        "    norms = np.sqrt(np.asarray(Xs.multiply(Xs).sum(axis=1)).ravel())\n",
        "\n",
        "    return {\n",
        "        \"min\": float(np.min(norms)),\n",
        "        \"mean\": float(np.mean(norms)),\n",
        "        \"max\": float(np.max(norms)),\n",
        "    }\n",
        "\n",
        "\n",
        "def top_terms_for_doc(\n",
        "    X,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    doc_index: int,\n",
        "    top_n: int = 10,\n",
        ") -> List[Tuple[str, float]]:\n",
        "    \"\"\"Return top-N (term, tf-idf weight) pairs for one document row.\"\"\"\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    row = X.getrow(doc_index)\n",
        "    if row.nnz == 0:\n",
        "        return []\n",
        "\n",
        "    order = np.argsort(row.data)[::-1][:top_n]\n",
        "    return [(feats[row.indices[i]], float(row.data[i])) for i in order]\n",
        "\n",
        "\n",
        "def run_tfidf_grid(texts: Sequence[str], sample_n_for_norms: int = 50) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run the 2x2 grid required by the assignment:\n",
        "      - lowercase ON/OFF\n",
        "      - stopwords OFF/ON ('english')\n",
        "    \"\"\"\n",
        "    configs = [\n",
        "        TfidfConfig(name=\"tfidf_lc_on_sw_off\",  lowercase=True,  stop_words=None),\n",
        "        TfidfConfig(name=\"tfidf_lc_on_sw_on\",   lowercase=True,  stop_words=\"english\"),\n",
        "        TfidfConfig(name=\"tfidf_lc_off_sw_off\", lowercase=False, stop_words=None),\n",
        "        TfidfConfig(name=\"tfidf_lc_off_sw_on\",  lowercase=False, stop_words=\"english\"),\n",
        "    ]\n",
        "\n",
        "    results: Dict[str, Dict[str, Any]] = {}\n",
        "    for cfg in configs:\n",
        "        vect, X = build_tfidf_vectors(texts, cfg)\n",
        "        stats = sparse_matrix_stats(X)\n",
        "        survivors = case_sensitive_stopword_survivors(vect) if cfg.stop_words else 0\n",
        "        norms = l2_norm_sample_stats(X, sample_n=sample_n_for_norms, seed=0)\n",
        "\n",
        "        results[cfg.name] = {\n",
        "            \"config\": cfg,\n",
        "            \"vectorizer\": vect,\n",
        "            \"X\": X,\n",
        "            \"stats\": stats,\n",
        "            \"l2_norm_sample\": norms,\n",
        "            \"case_sensitive_stopword_survivors\": survivors,\n",
        "        }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 5] TF-IDF experiments complete.\n",
            "Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\n",
            "Each column is a token; each cell is a TF-IDF weight (NOT a raw count).\n",
            "Note: sklearn TF-IDF uses L2-normalization by default, so ||doc_vector||_2 ≈ 1.\n",
            "\n",
            "--- tfidf_lc_on_sw_off ---\n",
            "Config: lowercase=True, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 8404)  (#docs x #vocab)\n",
            "nnz         : 90157\n",
            "Density     : 0.011014\n",
            "Avg nnz/doc : 92.56\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - transducers           0.3709\n",
            " - minimization          0.3430\n",
            " - tree                  0.2335\n",
            " - complex               0.2053\n",
            " - restrictions          0.1854\n",
            " - encountered           0.1715\n",
            " - representations       0.1600\n",
            " - compact               0.1525\n",
            " - involving             0.1499\n",
            " - finite                0.1475\n",
            "\n",
            "--- tfidf_lc_on_sw_on ---\n",
            "Config: lowercase=True, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 8153)  (#docs x #vocab)\n",
            "nnz         : 64523\n",
            "Density     : 0.008125\n",
            "Avg nnz/doc : 66.25\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - transducers           0.4070\n",
            " - minimization          0.3764\n",
            " - tree                  0.2563\n",
            " - complex               0.2253\n",
            " - restrictions          0.2035\n",
            " - encountered           0.1882\n",
            " - representations       0.1756\n",
            " - compact               0.1674\n",
            " - involving             0.1645\n",
            " - finite                0.1619\n",
            "\n",
            "--- tfidf_lc_off_sw_off ---\n",
            "Config: lowercase=False, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 10342)  (#docs x #vocab)\n",
            "nnz         : 96257\n",
            "Density     : 0.009556\n",
            "Avg nnz/doc : 98.83\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - Tree                  0.2795\n",
            " - complex               0.2054\n",
            " - Cost                  0.1966\n",
            " - Minimization          0.1966\n",
            " - Transducers           0.1966\n",
            " - Rule                  0.1855\n",
            " - restrictions          0.1855\n",
            " - transducers           0.1855\n",
            " - Finite                0.1855\n",
            " - minimization          0.1716\n",
            "\n",
            "--- tfidf_lc_off_sw_on ---\n",
            "Config: lowercase=False, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 10094)  (#docs x #vocab)\n",
            "nnz         : 72143\n",
            "Density     : 0.007338\n",
            "Avg nnz/doc : 74.07\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Stopword casing nuance: 155 vocab terms look like stopwords after lowercasing (e.g., 'The')\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - Tree                  0.3069\n",
            " - complex               0.2256\n",
            " - Minimization          0.2159\n",
            " - Cost                  0.2159\n",
            " - Transducers           0.2159\n",
            " - restrictions          0.2037\n",
            " - Rule                  0.2037\n",
            " - Finite                0.2037\n",
            " - transducers           0.2037\n",
            " - minimization          0.1884\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run</th>\n",
              "      <th>lowercase</th>\n",
              "      <th>stop_words</th>\n",
              "      <th>N_docs</th>\n",
              "      <th>V_vocab</th>\n",
              "      <th>nnz</th>\n",
              "      <th>density</th>\n",
              "      <th>avg_nnz_per_doc</th>\n",
              "      <th>l2_norm_mean(sample)</th>\n",
              "      <th>case_sensitive_stopword_survivors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tfidf_lc_on_sw_off</td>\n",
              "      <td>True</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>8404</td>\n",
              "      <td>90157</td>\n",
              "      <td>0.011014</td>\n",
              "      <td>92.563655</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tfidf_lc_on_sw_on</td>\n",
              "      <td>True</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>8153</td>\n",
              "      <td>64523</td>\n",
              "      <td>0.008125</td>\n",
              "      <td>66.245380</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tfidf_lc_off_sw_off</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>10342</td>\n",
              "      <td>96257</td>\n",
              "      <td>0.009556</td>\n",
              "      <td>98.826489</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tfidf_lc_off_sw_on</td>\n",
              "      <td>False</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>10094</td>\n",
              "      <td>72143</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>74.068789</td>\n",
              "      <td>1.0</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   run  lowercase stop_words  N_docs  V_vocab    nnz  \\\n",
              "0   tfidf_lc_on_sw_off       True       None     974     8404  90157   \n",
              "1    tfidf_lc_on_sw_on       True    english     974     8153  64523   \n",
              "2  tfidf_lc_off_sw_off      False       None     974    10342  96257   \n",
              "3   tfidf_lc_off_sw_on      False    english     974    10094  72143   \n",
              "\n",
              "    density  avg_nnz_per_doc  l2_norm_mean(sample)  \\\n",
              "0  0.011014        92.563655                   1.0   \n",
              "1  0.008125        66.245380                   1.0   \n",
              "2  0.009556        98.826489                   1.0   \n",
              "3  0.007338        74.068789                   1.0   \n",
              "\n",
              "   case_sensitive_stopword_survivors  \n",
              "0                                  0  \n",
              "1                                  0  \n",
              "2                                  0  \n",
              "3                                155  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 5] Default TF-IDF run set for later use: tfidf_lc_on_sw_off\n",
            "\n",
            "[Section 5] BoW vs TF-IDF (doc 0 example)\n",
            "Top BoW terms by COUNT (doc 0):\n",
            " - of                    4\n",
            " - to                    4\n",
            " - tree                  2\n",
            " - for                   2\n",
            " - minimization          2\n",
            " - representations       2\n",
            " - complex               2\n",
            " - the                   2\n",
            " - transducers           2\n",
            " - operation             1\n",
            "\n",
            "Top TF-IDF terms by WEIGHT (doc 0):\n",
            " - transducers           0.3709\n",
            " - minimization          0.3430\n",
            " - tree                  0.2335\n",
            " - complex               0.2053\n",
            " - restrictions          0.1854\n",
            " - encountered           0.1715\n",
            " - representations       0.1600\n",
            " - compact               0.1525\n",
            " - involving             0.1499\n",
            " - finite                0.1475\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 5 — TF-IDF Vectorization Experiments (Part 1 – Step 2, TF-IDF branch)\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from section5_tfidf import run_tfidf_grid, feature_examples, top_terms_for_doc\n",
        "\n",
        "# Input from Section 3:\n",
        "texts = paper_texts\n",
        "\n",
        "tfidf_results = run_tfidf_grid(texts, sample_n_for_norms=100)\n",
        "\n",
        "RUN_ORDER_TFIDF = [\"tfidf_lc_on_sw_off\", \"tfidf_lc_on_sw_on\", \"tfidf_lc_off_sw_off\", \"tfidf_lc_off_sw_on\"]\n",
        "\n",
        "def vocab_letter_sample(vectorizer, n: int = 20):\n",
        "    feats = vectorizer.get_feature_names_out().tolist()\n",
        "    letter_feats = [f for f in feats if re.search(r\"[A-Za-z]\", f)]\n",
        "    return letter_feats[:n]\n",
        "\n",
        "print(\"[Section 5] TF-IDF experiments complete.\")\n",
        "print(\"Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\")\n",
        "print(\"Each column is a token; each cell is a TF-IDF weight (NOT a raw count).\")\n",
        "print(\"Note: sklearn TF-IDF uses L2-normalization by default, so ||doc_vector||_2 ≈ 1.\\n\")\n",
        "\n",
        "for name in RUN_ORDER_TFIDF:\n",
        "    out = tfidf_results[name]\n",
        "    cfg = out[\"config\"]\n",
        "    s = out[\"stats\"]\n",
        "    norms = out[\"l2_norm_sample\"]\n",
        "    survivors = out[\"case_sensitive_stopword_survivors\"]\n",
        "\n",
        "    # Hard invariants\n",
        "    if s[\"n_docs\"] != len(texts):\n",
        "        raise RuntimeError(f\"Row mismatch in {name}: X has {s['n_docs']} rows but texts has {len(texts)} docs\")\n",
        "    if s[\"n_vocab\"] <= 0 or s[\"nnz\"] <= 0:\n",
        "        raise RuntimeError(f\"Degenerate TF-IDF matrix in {name}: n_vocab={s['n_vocab']}, nnz={s['nnz']}\")\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"Config: lowercase={cfg.lowercase}, stop_words={cfg.stop_words}\")\n",
        "    print(f\"Matrix type : {s['matrix_type']} (sparse)\")\n",
        "    print(f\"Dtype       : {s['dtype']} (TF-IDF weights are floating-point)\")\n",
        "    print(f\"Shape       : {s['shape']}  (#docs x #vocab)\")\n",
        "    print(f\"nnz         : {s['nnz']}\")\n",
        "    print(f\"Density     : {s['density']:.6f}\")\n",
        "    print(f\"Avg nnz/doc : {s['avg_nnz_per_doc']:.2f}\")\n",
        "    print(f\"L2-norm sample (min/mean/max): {norms['min']:.4f} / {norms['mean']:.4f} / {norms['max']:.4f}\")\n",
        "\n",
        "    if cfg.stop_words is not None and cfg.lowercase is False:\n",
        "        print(f\"Stopword casing nuance: {survivors} vocab terms look like stopwords after lowercasing (e.g., 'The')\")\n",
        "\n",
        "    print(f\"Vocabulary sample (first 20 terms): {feature_examples(out['vectorizer'], n=20)}\")\n",
        "    print(f\"Vocabulary sample with letters (first 20): {vocab_letter_sample(out['vectorizer'], n=20)}\")\n",
        "\n",
        "    # Show top TF-IDF terms for the first document as a concrete interpretability check\n",
        "    top_terms = top_terms_for_doc(out[\"X\"], out[\"vectorizer\"], doc_index=0, top_n=10)\n",
        "    print(\"Top TF-IDF terms (doc 0):\")\n",
        "    for term, val in top_terms:\n",
        "        print(f\" - {term:20s}  {val:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Summary table (nice for reporting)\n",
        "try:\n",
        "    import pandas as pd\n",
        "\n",
        "    rows = []\n",
        "    for name in RUN_ORDER_TFIDF:\n",
        "        out = tfidf_results[name]\n",
        "        cfg = out[\"config\"]\n",
        "        s = out[\"stats\"]\n",
        "        norms = out[\"l2_norm_sample\"]\n",
        "        rows.append(\n",
        "            {\n",
        "                \"run\": name,\n",
        "                \"lowercase\": cfg.lowercase,\n",
        "                \"stop_words\": cfg.stop_words,\n",
        "                \"N_docs\": s[\"n_docs\"],\n",
        "                \"V_vocab\": s[\"n_vocab\"],\n",
        "                \"nnz\": s[\"nnz\"],\n",
        "                \"density\": s[\"density\"],\n",
        "                \"avg_nnz_per_doc\": s[\"avg_nnz_per_doc\"],\n",
        "                \"l2_norm_mean(sample)\": norms[\"mean\"],\n",
        "                \"case_sensitive_stopword_survivors\": out[\"case_sensitive_stopword_survivors\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    tfidf_summary_df = pd.DataFrame(rows)\n",
        "    tfidf_summary_df[\"run\"] = pd.Categorical(tfidf_summary_df[\"run\"], categories=RUN_ORDER_TFIDF, ordered=True)\n",
        "    tfidf_summary_df = tfidf_summary_df.sort_values(by=[\"run\"]).reset_index(drop=True)\n",
        "    display(tfidf_summary_df)\n",
        "\n",
        "except Exception:\n",
        "    tfidf_summary_df = None\n",
        "\n",
        "# -------------------------\n",
        "# BoW vs TF-IDF comparison (interpretability-friendly, inside Section 5)\n",
        "# -------------------------\n",
        "# We compare (1) values meaning (counts vs weights) and (2) a concrete example on doc 0.\n",
        "\n",
        "def top_terms_bow_counts(X, vectorizer, doc_index: int, top_n: int = 10):\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    row = X.getrow(doc_index)\n",
        "    if row.nnz == 0:\n",
        "        return []\n",
        "    order = np.argsort(row.data)[::-1][:top_n]\n",
        "    return [(feats[row.indices[i]], int(row.data[i])) for i in order]\n",
        "\n",
        "DEFAULT_TFIDF_RUN = \"tfidf_lc_on_sw_off\"\n",
        "tfidf_vectorizer = tfidf_results[DEFAULT_TFIDF_RUN][\"vectorizer\"]\n",
        "tfidf_paper_vectors = tfidf_results[DEFAULT_TFIDF_RUN][\"X\"]\n",
        "\n",
        "print(f\"[Section 5] Default TF-IDF run set for later use: {DEFAULT_TFIDF_RUN}\")\n",
        "\n",
        "# If you already computed BoW in Section 4, these variables exist:\n",
        "#   bow_vectorizer, bow_paper_vectors\n",
        "if \"bow_vectorizer\" in globals() and \"bow_paper_vectors\" in globals():\n",
        "    print(\"\\n[Section 5] BoW vs TF-IDF (doc 0 example)\")\n",
        "    bow_top = top_terms_bow_counts(bow_paper_vectors, bow_vectorizer, doc_index=0, top_n=10)\n",
        "    tfidf_top = top_terms_for_doc(tfidf_paper_vectors, tfidf_vectorizer, doc_index=0, top_n=10)\n",
        "\n",
        "    print(\"Top BoW terms by COUNT (doc 0):\")\n",
        "    for term, cnt in bow_top:\n",
        "        print(f\" - {term:20s}  {cnt}\")\n",
        "\n",
        "    print(\"\\nTop TF-IDF terms by WEIGHT (doc 0):\")\n",
        "    for term, w in tfidf_top:\n",
        "        print(f\" - {term:20s}  {w:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "from section4_bow import (\n",
        "    BoWConfig,\n",
        "    build_bow_vectors,\n",
        "    sparse_matrix_stats as bow_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as bow_case_sensitive_stopword_survivors,\n",
        ")\n",
        "\n",
        "from section5_tfidf import (\n",
        "    TfidfConfig,\n",
        "    build_tfidf_vectors,\n",
        "    sparse_matrix_stats as tfidf_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as tfidf_case_sensitive_stopword_survivors,\n",
        "    l2_norm_sample_stats,\n",
        "    top_terms_for_doc,\n",
        ")\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)  # should not raise\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "\n",
        "class TestSection4BoW(unittest.TestCase):\n",
        "    def test_build_bow_vectors_shape_and_sparse(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = BoWConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(hasattr(vect, \"vocabulary_\"))\n",
        "\n",
        "    def test_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_bow_vectors(texts, BoWConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_bow_vectors(texts, BoWConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_bow_vectors(texts, BoWConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_bow_vectors(texts, BoWConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_bow_vectors(texts, BoWConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = bow_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_sparse_stats_sane(self):\n",
        "        texts = [\"cat sat\", \"dog barked\"]\n",
        "        _, X = build_bow_vectors(texts, BoWConfig(name=\"s\", lowercase=True, stop_words=None))\n",
        "        s = bow_sparse_matrix_stats(X)\n",
        "        self.assertEqual(s[\"n_docs\"], 2)\n",
        "        self.assertGreater(s[\"n_vocab\"], 0)\n",
        "        self.assertGreaterEqual(s[\"density\"], 0.0)\n",
        "        self.assertLessEqual(s[\"density\"], 1.0)\n",
        "\n",
        "\n",
        "class TestSection5Tfidf(unittest.TestCase):\n",
        "    def test_build_tfidf_vectors_shape_sparse_and_float(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = TfidfConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_tfidf_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(\"float\" in str(X.dtype))\n",
        "\n",
        "        stats = tfidf_sparse_matrix_stats(X)\n",
        "        self.assertEqual(stats[\"n_docs\"], 3)\n",
        "        self.assertGreater(stats[\"n_vocab\"], 0)\n",
        "\n",
        "    def test_tfidf_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_tfidf_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_tfidf_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = tfidf_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_tfidf_l2_norms_are_about_one(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"n\", lowercase=True, stop_words=None))\n",
        "        norms = l2_norm_sample_stats(X, sample_n=3, seed=0)\n",
        "        self.assertAlmostEqual(norms[\"mean\"], 1.0, places=6)\n",
        "\n",
        "    def test_tfidf_idf_downweights_common_terms_when_tf_equal(self):\n",
        "        # common appears in all docs; rare appears in only one doc => rare should get higher weight (tf equal)\n",
        "        texts = [\"common rare1\", \"common rare2\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"idf\", lowercase=True, stop_words=None))\n",
        "        feats = vect.get_feature_names_out().tolist()\n",
        "\n",
        "        i_common = feats.index(\"common\")\n",
        "        i_rare1 = feats.index(\"rare1\")\n",
        "\n",
        "        row0 = X.getrow(0).toarray().ravel()\n",
        "        self.assertGreater(row0[i_rare1], row0[i_common])\n",
        "\n",
        "    def test_top_terms_for_doc_returns_sorted(self):\n",
        "        texts = [\"cat sat sat\", \"dog sat\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"top\", lowercase=True, stop_words=None))\n",
        "        top = top_terms_for_doc(X, vect, doc_index=0, top_n=5)\n",
        "        self.assertTrue(len(top) > 0)\n",
        "        # weights should be non-increasing\n",
        "        weights = [w for _, w in top]\n",
        "        self.assertTrue(all(weights[i] >= weights[i+1] for i in range(len(weights)-1)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_all.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_rejects_non_dict_records (test_all.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_all.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_all.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "test_build_bow_vectors_shape_and_sparse (test_all.TestSection4BoW.test_build_bow_vectors_shape_and_sparse) ... ok\n",
            "test_case_sensitive_stopword_survivors_detected (test_all.TestSection4BoW.test_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_lowercase_merges_tokens (test_all.TestSection4BoW.test_lowercase_merges_tokens) ... ok\n",
            "test_sparse_stats_sane (test_all.TestSection4BoW.test_sparse_stats_sane) ... ok\n",
            "test_stopwords_removed_when_lowercase_true (test_all.TestSection4BoW.test_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_build_tfidf_vectors_shape_sparse_and_float (test_all.TestSection5Tfidf.test_build_tfidf_vectors_shape_sparse_and_float) ... ok\n",
            "test_tfidf_case_sensitive_stopword_survivors_detected (test_all.TestSection5Tfidf.test_tfidf_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_tfidf_idf_downweights_common_terms_when_tf_equal (test_all.TestSection5Tfidf.test_tfidf_idf_downweights_common_terms_when_tf_equal) ... ok\n",
            "test_tfidf_l2_norms_are_about_one (test_all.TestSection5Tfidf.test_tfidf_l2_norms_are_about_one) ... ok\n",
            "test_tfidf_lowercase_merges_tokens (test_all.TestSection5Tfidf.test_tfidf_lowercase_merges_tokens) ... ok\n",
            "test_tfidf_stopwords_removed_when_lowercase_true (test_all.TestSection5Tfidf.test_tfidf_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_top_terms_for_doc_returns_sorted (test_all.TestSection5Tfidf.test_top_terms_for_doc_returns_sorted) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.016s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_all.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCXK65aHlq9Y"
      },
      "source": [
        "###Step 3: Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O0xuwShjlur3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section6_queries.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section6_queries.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Sequence\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class QueryItem:\n",
        "    label: str\n",
        "    title: str\n",
        "    abstract: str\n",
        "    text: str\n",
        "\n",
        "\n",
        "def _as_text(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    return x if isinstance(x, str) else str(x)\n",
        "\n",
        "\n",
        "def build_query_item(label: str, title: str, abstract: str, joiner: str = \" \") -> QueryItem:\n",
        "    \"\"\"\n",
        "    Query construction MUST match dataset documents (Section 3):\n",
        "        text = title + \" \" + abstract\n",
        "    No extra cleaning here.\n",
        "    \"\"\"\n",
        "    if not isinstance(joiner, str) or joiner != \" \":\n",
        "        raise ValueError('joiner must be exactly one space: \" \"')\n",
        "\n",
        "    t = _as_text(title)\n",
        "    a = _as_text(abstract)\n",
        "\n",
        "    if len(t.strip()) == 0:\n",
        "        raise ValueError(f\"{label}: title is empty\")\n",
        "    if len(a.strip()) == 0:\n",
        "        raise ValueError(f\"{label}: abstract is empty\")\n",
        "\n",
        "    text = t + joiner + a\n",
        "    return QueryItem(label=label, title=t, abstract=a, text=text)\n",
        "\n",
        "\n",
        "def get_default_queries(joiner: str = \" \") -> List[QueryItem]:\n",
        "    \"\"\"\n",
        "    The three queries EXACTLY as provided by the assignment (title + abstract).\n",
        "    Stored in fixed order: Query 1, Query 2, Query 3.\n",
        "    \"\"\"\n",
        "    q1_title = \"QUALES: Machine translation quality estimation via supervised and unsupervised machine learning.\"\n",
        "    q1_abstract = (\n",
        "        \"The automatic quality estimation (QE) of machine translation consists in measuring the quality of translations \"\n",
        "        \"without access to human references, usually via machine learning approaches. A good QE system can help in three \"\n",
        "        \"aspects of translation processes involving machine translation and post-editing: increasing productivity (by ruling \"\n",
        "        \"out poor quality machine translation), estimating costs (by helping to forecast the cost of post-editing) and selecting \"\n",
        "        \"a provider (if several machine translation systems are available). Interest in this research area has grown significantly \"\n",
        "        \"in recent years, leading to regular shared tasks in the main machine translation conferences and intense scientific activity. \"\n",
        "        \"In this article we review the state of the art in this research area and present project QUALES, which is under development.\"\n",
        "    )\n",
        "\n",
        "    q2_title = \"Learning to Ask Unanswerable Questions for Machine Reading Comprehension\"\n",
        "    q2_abstract = (\n",
        "        \"Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation \"\n",
        "        \"technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding \"\n",
        "        \"paragraph that contains the answer. We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures \"\n",
        "        \"the interactions between the question and the paragraph. We also present a way to construct training data for our question generation models \"\n",
        "        \"by leveraging the existing reading comprehension dataset. Experimental results show that the pair-to-sequence model performs consistently better \"\n",
        "        \"compared with the sequence-to-sequence baseline. We further use the automatically generated unanswerable questions as a means of data augmentation \"\n",
        "        \"on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model.\"\n",
        "    )\n",
        "\n",
        "    q3_title = \"Unsupervised Neural Text Simplification\"\n",
        "    q3_abstract = (\n",
        "        \"The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabelled text corpora. \"\n",
        "        \"The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. \"\n",
        "        \"The framework is trained using unlabelled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) \"\n",
        "        \"on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. \"\n",
        "        \"It also outperforms viable unsupervised baselines. Adding a few labelled pairs helps improve the performance further.\"\n",
        "    )\n",
        "\n",
        "    queries = [\n",
        "        build_query_item(\"Query 1\", q1_title, q1_abstract, joiner=joiner),\n",
        "        build_query_item(\"Query 2\", q2_title, q2_abstract, joiner=joiner),\n",
        "        build_query_item(\"Query 3\", q3_title, q3_abstract, joiner=joiner),\n",
        "    ]\n",
        "    return queries\n",
        "\n",
        "\n",
        "def get_query_texts(queries: Sequence[QueryItem]) -> List[str]:\n",
        "    return [q.text for q in queries]\n",
        "\n",
        "\n",
        "def validate_queries(queries: Sequence[QueryItem]) -> None:\n",
        "    if not isinstance(queries, (list, tuple)):\n",
        "        raise TypeError(f\"queries must be a list/tuple, got {type(queries)}\")\n",
        "    if len(queries) != 3:\n",
        "        raise AssertionError(f\"Expected exactly 3 queries, got {len(queries)}\")\n",
        "\n",
        "    expected_labels = [\"Query 1\", \"Query 2\", \"Query 3\"]\n",
        "    got_labels = [q.label for q in queries]\n",
        "    if got_labels != expected_labels:\n",
        "        raise AssertionError(f\"Query order/labels must be {expected_labels}, got {got_labels}\")\n",
        "\n",
        "    for q in queries:\n",
        "        if any(not isinstance(x, str) for x in [q.label, q.title, q.abstract, q.text]):\n",
        "            raise AssertionError(\"All QueryItem fields must be strings.\")\n",
        "        if len(q.title.strip()) == 0 or len(q.abstract.strip()) == 0 or len(q.text.strip()) == 0:\n",
        "            raise AssertionError(f\"{q.label} has empty fields.\")\n",
        "\n",
        "        # Strict pipeline check: text must be EXACTLY title + \" \" + abstract\n",
        "        if q.text != (q.title + \" \" + q.abstract):\n",
        "            raise AssertionError(f\"{q.label}: text is not exactly title + ' ' + abstract\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 6] Queries ready.\n",
            " - N queries     : 3\n",
            " - Order/labels  : ['Query 1', 'Query 2', 'Query 3']\n",
            "\n",
            "[Section 6] Preview (first 300 chars each):\n",
            "\n",
            "Query 1: QUALES: Machine translation quality estimation via supervised and unsupervised machine learning.\n",
            "QUALES: Machine translation quality estimation via supervised and unsupervised machine learning. The automatic quality estimation (QE) of machine translation consists in measuring the quality of translations without access to human references, usually via machine learning approaches. A good QE syste...\n",
            "\n",
            "Query 2: Learning to Ask Unanswerable Questions for Machine Reading Comprehension\n",
            "Learning to Ask Unanswerable Questions for Machine Reading Comprehension Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable ques...\n",
            "\n",
            "Query 3: Unsupervised Neural Text Simplification\n",
            "Unsupervised Neural Text Simplification The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabelled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based lo...\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 6 — Query Set Definition (Part 1 – Step 3 Inputs)\n",
        "# =========================\n",
        "\n",
        "from section6_queries import get_default_queries, get_query_texts, validate_queries\n",
        "\n",
        "queries = get_default_queries(joiner=\" \")   # MUST be exactly one space\n",
        "validate_queries(queries)\n",
        "\n",
        "query_texts = get_query_texts(queries)      # list[str] in fixed order (Query 1, 2, 3)\n",
        "query_labels = [q.label for q in queries]   # [\"Query 1\", \"Query 2\", \"Query 3\"]\n",
        "\n",
        "print(\"[Section 6] Queries ready.\")\n",
        "print(f\" - N queries     : {len(queries)}\")\n",
        "print(f\" - Order/labels  : {query_labels}\")\n",
        "print(\"\\n[Section 6] Preview (first 300 chars each):\")\n",
        "for q in queries:\n",
        "    print(f\"\\n{q.label}: {q.title}\")\n",
        "    print(q.text[:300] + (\"...\" if len(q.text) > 300 else \"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "from section4_bow import (\n",
        "    BoWConfig,\n",
        "    build_bow_vectors,\n",
        "    sparse_matrix_stats as bow_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as bow_case_sensitive_stopword_survivors,\n",
        ")\n",
        "\n",
        "from section5_tfidf import (\n",
        "    TfidfConfig,\n",
        "    build_tfidf_vectors,\n",
        "    sparse_matrix_stats as tfidf_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as tfidf_case_sensitive_stopword_survivors,\n",
        "    l2_norm_sample_stats,\n",
        "    top_terms_for_doc,\n",
        ")\n",
        "\n",
        "from section6_queries import (\n",
        "    QueryItem,\n",
        "    build_query_item,\n",
        "    get_default_queries,\n",
        "    get_query_texts,\n",
        "    validate_queries,\n",
        ")\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "\n",
        "class TestSection4BoW(unittest.TestCase):\n",
        "    def test_build_bow_vectors_shape_and_sparse(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = BoWConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(hasattr(vect, \"vocabulary_\"))\n",
        "\n",
        "    def test_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_bow_vectors(texts, BoWConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_bow_vectors(texts, BoWConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_bow_vectors(texts, BoWConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_bow_vectors(texts, BoWConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_bow_vectors(texts, BoWConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = bow_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_sparse_stats_sane(self):\n",
        "        texts = [\"cat sat\", \"dog barked\"]\n",
        "        _, X = build_bow_vectors(texts, BoWConfig(name=\"s\", lowercase=True, stop_words=None))\n",
        "        s = bow_sparse_matrix_stats(X)\n",
        "        self.assertEqual(s[\"n_docs\"], 2)\n",
        "        self.assertGreater(s[\"n_vocab\"], 0)\n",
        "        self.assertGreaterEqual(s[\"density\"], 0.0)\n",
        "        self.assertLessEqual(s[\"density\"], 1.0)\n",
        "\n",
        "\n",
        "class TestSection5Tfidf(unittest.TestCase):\n",
        "    def test_build_tfidf_vectors_shape_sparse_and_float(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = TfidfConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_tfidf_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(\"float\" in str(X.dtype))\n",
        "\n",
        "        stats = tfidf_sparse_matrix_stats(X)\n",
        "        self.assertEqual(stats[\"n_docs\"], 3)\n",
        "        self.assertGreater(stats[\"n_vocab\"], 0)\n",
        "\n",
        "    def test_tfidf_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_tfidf_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_tfidf_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = tfidf_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_tfidf_l2_norms_are_about_one(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        _, X = build_tfidf_vectors(texts, TfidfConfig(name=\"n\", lowercase=True, stop_words=None))\n",
        "        norms = l2_norm_sample_stats(X, sample_n=3, seed=0)\n",
        "        self.assertAlmostEqual(norms[\"mean\"], 1.0, places=6)\n",
        "\n",
        "    def test_tfidf_idf_downweights_common_terms_when_tf_equal(self):\n",
        "        texts = [\"common rare1\", \"common rare2\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"idf\", lowercase=True, stop_words=None))\n",
        "        feats = vect.get_feature_names_out().tolist()\n",
        "\n",
        "        i_common = feats.index(\"common\")\n",
        "        i_rare1 = feats.index(\"rare1\")\n",
        "\n",
        "        row0 = X.getrow(0).toarray().ravel()\n",
        "        self.assertGreater(row0[i_rare1], row0[i_common])\n",
        "\n",
        "    def test_top_terms_for_doc_returns_sorted(self):\n",
        "        texts = [\"cat sat sat\", \"dog sat\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"top\", lowercase=True, stop_words=None))\n",
        "        top = top_terms_for_doc(X, vect, doc_index=0, top_n=5)\n",
        "        self.assertTrue(len(top) > 0)\n",
        "        weights = [w for _, w in top]\n",
        "        self.assertTrue(all(weights[i] >= weights[i + 1] for i in range(len(weights) - 1)))\n",
        "\n",
        "\n",
        "class TestSection6Queries(unittest.TestCase):\n",
        "    def test_default_queries_exist_and_valid(self):\n",
        "        queries = get_default_queries(joiner=\" \")\n",
        "        validate_queries(queries)  # should not raise\n",
        "        self.assertEqual(len(queries), 3)\n",
        "        self.assertEqual([q.label for q in queries], [\"Query 1\", \"Query 2\", \"Query 3\"])\n",
        "        self.assertTrue(all(isinstance(q, QueryItem) for q in queries))\n",
        "\n",
        "    def test_query_texts_are_in_order(self):\n",
        "        queries = get_default_queries(joiner=\" \")\n",
        "        texts = get_query_texts(queries)\n",
        "        self.assertEqual(len(texts), 3)\n",
        "        self.assertTrue(all(isinstance(t, str) and len(t) > 0 for t in texts))\n",
        "\n",
        "    def test_build_query_item_strict_joiner(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            build_query_item(\"Query 1\", \"T\", \"A\", joiner=\"  \")  # not exactly one space\n",
        "        with self.assertRaises(ValueError):\n",
        "            build_query_item(\"Query 1\", \"T\", \"A\", joiner=\"\")    # not exactly one space\n",
        "\n",
        "    def test_validate_queries_rejects_wrong_order(self):\n",
        "        q1 = build_query_item(\"Query 1\", \"T1\", \"A1\", joiner=\" \")\n",
        "        q2 = build_query_item(\"Query 2\", \"T2\", \"A2\", joiner=\" \")\n",
        "        q3 = build_query_item(\"Query 3\", \"T3\", \"A3\", joiner=\" \")\n",
        "        with self.assertRaises(AssertionError):\n",
        "            validate_queries([q2, q1, q3])  # wrong order/labels\n",
        "\n",
        "    def test_validate_queries_rejects_bad_text_pipeline(self):\n",
        "        bad = QueryItem(label=\"Query 1\", title=\"T\", abstract=\"A\", text=\"T  A\")\n",
        "        q2 = build_query_item(\"Query 2\", \"T2\", \"A2\", joiner=\" \")\n",
        "        q3 = build_query_item(\"Query 3\", \"T3\", \"A3\", joiner=\" \")\n",
        "        with self.assertRaises(AssertionError):\n",
        "            validate_queries([bad, q2, q3])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_all.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_rejects_non_dict_records (test_all.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_all.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_all.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "test_build_bow_vectors_shape_and_sparse (test_all.TestSection4BoW.test_build_bow_vectors_shape_and_sparse) ... ok\n",
            "test_case_sensitive_stopword_survivors_detected (test_all.TestSection4BoW.test_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_lowercase_merges_tokens (test_all.TestSection4BoW.test_lowercase_merges_tokens) ... ok\n",
            "test_sparse_stats_sane (test_all.TestSection4BoW.test_sparse_stats_sane) ... ok\n",
            "test_stopwords_removed_when_lowercase_true (test_all.TestSection4BoW.test_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_build_tfidf_vectors_shape_sparse_and_float (test_all.TestSection5Tfidf.test_build_tfidf_vectors_shape_sparse_and_float) ... ok\n",
            "test_tfidf_case_sensitive_stopword_survivors_detected (test_all.TestSection5Tfidf.test_tfidf_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_tfidf_idf_downweights_common_terms_when_tf_equal (test_all.TestSection5Tfidf.test_tfidf_idf_downweights_common_terms_when_tf_equal) ... ok\n",
            "test_tfidf_l2_norms_are_about_one (test_all.TestSection5Tfidf.test_tfidf_l2_norms_are_about_one) ... ok\n",
            "test_tfidf_lowercase_merges_tokens (test_all.TestSection5Tfidf.test_tfidf_lowercase_merges_tokens) ... ok\n",
            "test_tfidf_stopwords_removed_when_lowercase_true (test_all.TestSection5Tfidf.test_tfidf_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_top_terms_for_doc_returns_sorted (test_all.TestSection5Tfidf.test_top_terms_for_doc_returns_sorted) ... ok\n",
            "test_build_query_item_strict_joiner (test_all.TestSection6Queries.test_build_query_item_strict_joiner) ... ok\n",
            "test_default_queries_exist_and_valid (test_all.TestSection6Queries.test_default_queries_exist_and_valid) ... ok\n",
            "test_query_texts_are_in_order (test_all.TestSection6Queries.test_query_texts_are_in_order) ... ok\n",
            "test_validate_queries_rejects_bad_text_pipeline (test_all.TestSection6Queries.test_validate_queries_rejects_bad_text_pipeline) ... ok\n",
            "test_validate_queries_rejects_wrong_order (test_all.TestSection6Queries.test_validate_queries_rejects_wrong_order) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 21 tests in 0.017s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_all.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing section7_similarity_search.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section7_similarity_search.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Mapping, Optional, Sequence\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class RetrievedDoc:\n",
        "    rank: int\n",
        "    doc_index: int\n",
        "    score: float\n",
        "    text: str\n",
        "\n",
        "\n",
        "def top_k_indices(scores: np.ndarray, k: int = 3) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Return top-k indices sorted by:\n",
        "      1) score descending\n",
        "      2) index ascending (tie-break for determinism)\n",
        "    \"\"\"\n",
        "    s = np.asarray(scores).ravel()\n",
        "    if s.ndim != 1:\n",
        "        raise ValueError(\"scores must be a 1D array\")\n",
        "    if k <= 0:\n",
        "        raise ValueError(\"k must be >= 1\")\n",
        "\n",
        "    k_eff = int(min(k, s.size))\n",
        "    # Fast candidate selection\n",
        "    cand = np.argpartition(-s, k_eff - 1)[:k_eff]\n",
        "    # Deterministic sorting: score desc, index asc\n",
        "    order = np.lexsort((cand, -s[cand]))\n",
        "    return cand[order]\n",
        "\n",
        "\n",
        "def run_similarity_search(\n",
        "    *,\n",
        "    vectorizer: Any,\n",
        "    paper_vectors: Any,\n",
        "    paper_texts: Sequence[str],\n",
        "    query_texts: Sequence[str],\n",
        "    query_labels: Optional[Sequence[str]] = None,\n",
        "    top_k: int = 3,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Core requirement for Section 7:\n",
        "      - Use vectorizer.transform on queries (NOT fit_transform)\n",
        "      - cosine similarity(query_vectors, paper_vectors)\n",
        "      - retrieve top-k docs per query, returning their paper_texts[index] + score\n",
        "    \"\"\"\n",
        "    if query_labels is None:\n",
        "        query_labels = [f\"Query {i+1}\" for i in range(len(query_texts))]\n",
        "\n",
        "    if len(query_texts) != len(query_labels):\n",
        "        raise ValueError(\"query_texts and query_labels must have the same length\")\n",
        "\n",
        "    if not hasattr(paper_vectors, \"shape\"):\n",
        "        raise TypeError(\"paper_vectors must be a matrix with .shape\")\n",
        "\n",
        "    n_docs = int(paper_vectors.shape[0])\n",
        "    if len(paper_texts) != n_docs:\n",
        "        raise ValueError(f\"len(paper_texts)={len(paper_texts)} must match paper_vectors rows={n_docs}\")\n",
        "\n",
        "    # IMPORTANT: transform (re-use learned vocabulary)\n",
        "    query_vectors = vectorizer.transform(list(query_texts))\n",
        "\n",
        "    if int(query_vectors.shape[1]) != int(paper_vectors.shape[1]):\n",
        "        raise RuntimeError(\n",
        "            f\"Vector dimension mismatch: query_vectors has {query_vectors.shape[1]} cols \"\n",
        "            f\"but paper_vectors has {paper_vectors.shape[1]} cols\"\n",
        "        )\n",
        "\n",
        "    sim = cosine_similarity(query_vectors, paper_vectors)  # (n_queries, n_docs) dense\n",
        "    results: Dict[str, List[RetrievedDoc]] = {}\n",
        "\n",
        "    for qi, qlabel in enumerate(query_labels):\n",
        "        idxs = top_k_indices(sim[qi], k=top_k)\n",
        "        matches: List[RetrievedDoc] = []\n",
        "        for rank, di in enumerate(idxs, start=1):\n",
        "            d = int(di)\n",
        "            matches.append(\n",
        "                RetrievedDoc(\n",
        "                    rank=rank,\n",
        "                    doc_index=d,\n",
        "                    score=float(sim[qi, d]),\n",
        "                    text=str(paper_texts[d]),\n",
        "                )\n",
        "            )\n",
        "        results[str(qlabel)] = matches\n",
        "\n",
        "    return {\n",
        "        \"query_vectors\": query_vectors,\n",
        "        \"similarity_matrix\": sim,\n",
        "        \"matches\": results,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_similarity_for_grid(\n",
        "    *,\n",
        "    grid_results: Mapping[str, Mapping[str, Any]],\n",
        "    run_order: Sequence[str],\n",
        "    representation_name: str,\n",
        "    paper_texts: Sequence[str],\n",
        "    query_texts: Sequence[str],\n",
        "    query_labels: Sequence[str],\n",
        "    top_k: int = 3,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Runs similarity search for each run inside bow_results / tfidf_results.\n",
        "    Expects each grid_results[run_name] to contain keys: 'vectorizer', 'X', 'config' (optional).\n",
        "    \"\"\"\n",
        "    out: Dict[str, Any] = {\"representation\": representation_name, \"runs\": {}}\n",
        "\n",
        "    for run_name in run_order:\n",
        "        if run_name not in grid_results:\n",
        "            raise KeyError(f\"run_name '{run_name}' not found in grid_results\")\n",
        "\n",
        "        run_obj = grid_results[run_name]\n",
        "        vect = run_obj[\"vectorizer\"]\n",
        "        X = run_obj[\"X\"]\n",
        "        cfg = run_obj.get(\"config\", None)\n",
        "\n",
        "        res = run_similarity_search(\n",
        "            vectorizer=vect,\n",
        "            paper_vectors=X,\n",
        "            paper_texts=paper_texts,\n",
        "            query_texts=query_texts,\n",
        "            query_labels=query_labels,\n",
        "            top_k=top_k,\n",
        "        )\n",
        "\n",
        "        out[\"runs\"][run_name] = {\n",
        "            \"config\": cfg,\n",
        "            \"matches\": res[\"matches\"],\n",
        "        }\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _truncate(text: str, max_chars: Optional[int]) -> str:\n",
        "    if max_chars is None:\n",
        "        return text\n",
        "    t = str(text)\n",
        "    return t if len(t) <= max_chars else (t[:max_chars] + \"...\")\n",
        "\n",
        "\n",
        "def flatten_results(\n",
        "    *,\n",
        "    bow_block: Dict[str, Any],\n",
        "    tfidf_block: Dict[str, Any],\n",
        "    max_chars: Optional[int] = 350,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Flatten nested results into rows (nice for pandas display).\n",
        "    \"\"\"\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    for block in (bow_block, tfidf_block):\n",
        "        rep = block[\"representation\"]\n",
        "        for run_name, run_data in block[\"runs\"].items():\n",
        "            cfg = run_data.get(\"config\", None)\n",
        "            lowercase = getattr(cfg, \"lowercase\", None)\n",
        "            stop_words = getattr(cfg, \"stop_words\", None)\n",
        "\n",
        "            matches_by_query = run_data[\"matches\"]\n",
        "            for qlabel, matches in matches_by_query.items():\n",
        "                for m in matches:\n",
        "                    rows.append(\n",
        "                        {\n",
        "                            \"representation\": rep,\n",
        "                            \"run\": run_name,\n",
        "                            \"lowercase\": lowercase,\n",
        "                            \"stop_words\": stop_words,\n",
        "                            \"query\": qlabel,\n",
        "                            \"rank\": m.rank,\n",
        "                            \"score\": m.score,\n",
        "                            \"doc_index\": m.doc_index,\n",
        "                            \"paper_text\": _truncate(m.text, max_chars=max_chars),\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "def results_to_dataframe(rows: List[Dict[str, Any]]):\n",
        "    \"\"\"\n",
        "    Optional: convert rows to a pandas DataFrame for clean comparison in Colab.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import pandas as pd  # type: ignore\n",
        "    except Exception:\n",
        "        return None\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "def print_results(\n",
        "    *,\n",
        "    bow_block: Dict[str, Any],\n",
        "    tfidf_block: Dict[str, Any],\n",
        "    max_chars: Optional[int] = 350,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Console-style report, organized to compare BoW vs TF-IDF and preprocessing toggles.\n",
        "    \"\"\"\n",
        "    for block in (bow_block, tfidf_block):\n",
        "        rep = block[\"representation\"]\n",
        "        print(f\"\\n==================== {rep} ====================\")\n",
        "        for run_name, run_data in block[\"runs\"].items():\n",
        "            cfg = run_data.get(\"config\", None)\n",
        "            lc = getattr(cfg, \"lowercase\", None)\n",
        "            sw = getattr(cfg, \"stop_words\", None)\n",
        "\n",
        "            print(f\"\\n--- {run_name} (lowercase={lc}, stop_words={sw}) ---\")\n",
        "            matches_by_query = run_data[\"matches\"]\n",
        "            for qlabel, matches in matches_by_query.items():\n",
        "                print(f\"\\n{qlabel}:\")\n",
        "                for m in matches:\n",
        "                    txt = _truncate(m.text, max_chars=max_chars)\n",
        "                    print(f\"{m.rank}. Text: '{txt}' (Score: {m.score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwFOkv1syVnM"
      },
      "source": [
        "Use the following code snipet to compute the pairwise cosine similarity, and to sort the top 3 candidates for each query. In this example, `query_vectors` and `paper_vectors` correspond to the vectorized collections of queries and papers (as stored in the JSON file previously downloaded), respectively. The variable `paper_texts` contain the list of concatenated titles+abstracts of the papers loaded from the JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_OlJS4Ubx7IF"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'query_vectors' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m similarity_matrix = cosine_similarity(\u001b[43mquery_vectors\u001b[49m, paper_vectors)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nquery \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m]:\n\u001b[32m      8\u001b[39m   similarity_scores = similarity_matrix[nquery]\n",
            "\u001b[31mNameError\u001b[39m: name 'query_vectors' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#...\n",
        "\n",
        "similarity_matrix = cosine_similarity(query_vectors, paper_vectors)\n",
        "\n",
        "for nquery in [0, 1, 2]:\n",
        "  similarity_scores = similarity_matrix[nquery]\n",
        "  top_indices = np.argsort(similarity_scores)[::-1][:3]  # Indices of top 3 scores\n",
        "  for i, index in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. Text: '{paper_texts[index]}' (Score: {similarity_scores[index]:.4f})\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYDuwvtrlvCy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiX0btXHlvwx"
      },
      "source": [
        "###Step 4: Analysis of the results obtained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4huZZ3yl4uu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9_ZGeoih9AD"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "In this section students should use `setence-embeddings` to obtain sentence-embedding representations of the dataset and to peform searches for best matches regarding the examples proposed in the instructions of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEVig0EkcCm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr74dlaTkzeG"
      },
      "source": [
        "###Step 1: Trying a general purpose small monolingual model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5sToE6jl-e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBydGHYMl-t-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xHKqT5TmDGF"
      },
      "source": [
        "###Step 2: Comparing other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WPRck-OmB-Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6gmrCnmCWM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orh6Qh9OmHhB"
      },
      "source": [
        "###Step 3: Moving to a multilingual environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBXNcaK-mNa_"
      },
      "source": [
        "When trying multilingual models, you will have to build a multilingual collection of papers. To do so, extend the collection of papers from the EMNLP conference used in the first part of this notebook with an extra collection of papers from the SEPLN conference, which are both in English and Spanish. Create a new dataset that concatenates both collections to try a multilingual search. The collection of SEPLN papers can be downloaded from [https://www.dlsi.ua.es/~mespla/sepln.json](https://www.dlsi.ua.es/~mespla/sepln.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzXQs6-FmOkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeJ3So5UmO1T"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4Xcu1Dmeiy"
      },
      "source": [
        "##Concluding remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izHrjfNBmg5z"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
