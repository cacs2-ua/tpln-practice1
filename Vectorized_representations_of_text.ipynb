{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBki8x4YoCi"
      },
      "source": [
        "# Using vectorized representations of text for semantic text similarity\n",
        "\n",
        "This notebook has been created to allow students of the TNLP 25/26 course to complete their assignment on vectorized representations of text. This notebook is provided with the minimal information to start working on the assignment. Students will have to follow the [instructions](https://mespla.github.io/tpln2526/assignment-searchinvectorialspace/) of the assignment reflecting in this notebook the work done.\n",
        "\n",
        "The starting point will be to install both the `scikit-learn` and the `sentence-embeddings` python libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3_yWE7DX_eD",
        "outputId": "6d65e29e-3902-49bb-fa9c-adb47e567c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.6.0+cpu)\n",
            "Requirement already satisfied: scipy in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.34.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.4)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
            "Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
            "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 0.8/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.0/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.0/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 1.3/8.7 MB 1.1 MB/s eta 0:00:07\n",
            "   ------- -------------------------------- 1.6/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 1.8/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 2.1/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   ---------- ----------------------------- 2.4/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   ------------ --------------------------- 2.6/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   ------------- -------------------------- 2.9/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 3.1/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 3.4/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 3.7/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------ --------------------- 3.9/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 4.2/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 4.5/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 4.7/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 5.0/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   ------------------------ --------------- 5.2/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 5.5/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 5.8/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 6.0/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 6.3/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   ------------------------------ --------- 6.6/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 6.8/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 6.8/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 7.1/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 7.3/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 7.6/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 7.9/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.1/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 8.4/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.7/8.7 MB 1.2 MB/s eta 0:00:00\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn, sentence-transformers\n",
            "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 sentence-transformers-5.1.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tgcVmtaYmkN"
      },
      "source": [
        "Once this is done, students will have to download and read the file containing the dataset consisting of a list of scientific paper's title and abstract.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWRIDdL4YknO",
        "outputId": "f1e5a49b-e8e3-4a77-d5c3-021740a6963a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Cache] Using existing file: emnlp2016-2018.json\n",
            "\n",
            "[Provenance]\n",
            " - Source URL: https://sbert.net/datasets/emnlp2016-2018.json\n",
            " - Local path: emnlp2016-2018.json\n",
            " - File size : 1.05 MB\n",
            " - SHA-256   : 9e6020503e5f0dd0e91dbb970d47f43c7621f67321249daed5990057e159961c\n",
            "\n",
            "[Sanity checks]\n",
            " - N papers loaded: 974\n",
            " - Key presence: OK (all required keys found in all records)\n",
            " - Empty titles   : 0\n",
            " - Empty abstracts: 0\n",
            " - Non-string titles   : 0\n",
            " - Non-string abstracts: 0\n",
            " - Year distribution (top): [(2018, 549), (2017, 230), (2016, 195)]\n",
            " - Years outside 2016–2018: 0\n",
            " - Venue distribution (top): [('EMNLP', 974)]\n",
            " - Duplicate URL keys: 0 (unique non-empty URLs: 974)\n",
            "\n",
            "[Access check — first record]\n",
            " - title   : Rule Extraction for Tree-to-Tree Transducers by Cost Minimization\n",
            " - abstract: Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set...\n",
            " - url     : http://aclweb.org/anthology/D16-1002\n",
            " - venue   : EMNLP\n",
            " - year    : 2016\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 2 — Dataset Acquisition & Loading (EMNLP 2016–2018 JSON)\n",
        "# =========================\n",
        "\n",
        "import os, json, hashlib\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "DATA_URL = \"https://sbert.net/datasets/emnlp2016-2018.json\"\n",
        "DATA_PATH = \"emnlp2016-2018.json\"   # Feel free to rename, but keep it consistent across the notebook\n",
        "FORCE_DOWNLOAD = False             # Set True if you want to re-download even if the file exists\n",
        "\n",
        "def sha256_of_file(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def download_json(url: str, out_path: str, timeout: int = 120) -> None:\n",
        "    r = requests.get(url, stream=True, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "# --- (1) Acquire the dataset file (download if missing) ---\n",
        "if FORCE_DOWNLOAD or (not os.path.exists(DATA_PATH)):\n",
        "    print(f\"[Download] Fetching dataset from: {DATA_URL}\")\n",
        "    download_json(DATA_URL, DATA_PATH)\n",
        "    print(f\"[Download] Saved to: {DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"[Cache] Using existing file: {DATA_PATH}\")\n",
        "\n",
        "# --- (2) Load JSON into memory ---\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    papers = json.load(f)\n",
        "\n",
        "# Alias for compatibility with the starter template cells\n",
        "data = papers\n",
        "\n",
        "# --- (3) Validate expected JSON structure: root must be a list of dicts ---\n",
        "if not isinstance(papers, list):\n",
        "    raise TypeError(f\"Expected JSON root to be a list, got: {type(papers)}\")\n",
        "\n",
        "if len(papers) == 0:\n",
        "    raise ValueError(\"Dataset loaded but it is empty (N=0). File may be corrupted or not the expected dataset.\")\n",
        "\n",
        "if not isinstance(papers[0], dict):\n",
        "    raise TypeError(f\"Expected each item to be a dict, got first item type: {type(papers[0])}\")\n",
        "\n",
        "# --- (4) Provenance info (practical traceability) ---\n",
        "file_size_mb = os.path.getsize(DATA_PATH) / (1024 * 1024)\n",
        "print(f\"\\n[Provenance]\")\n",
        "print(f\" - Source URL: {DATA_URL}\")\n",
        "print(f\" - Local path: {DATA_PATH}\")\n",
        "print(f\" - File size : {file_size_mb:.2f} MB\")\n",
        "print(f\" - SHA-256   : {sha256_of_file(DATA_PATH)}\")\n",
        "\n",
        "# --- (5) Mandatory sanity checks ---\n",
        "N = len(papers)\n",
        "print(f\"\\n[Sanity checks]\")\n",
        "print(f\" - N papers loaded: {N}\")\n",
        "\n",
        "required_keys = {\"title\", \"abstract\", \"url\", \"venue\", \"year\"}\n",
        "missing_key_counts = Counter()\n",
        "\n",
        "empty_title = 0\n",
        "empty_abstract = 0\n",
        "non_string_title = 0\n",
        "non_string_abstract = 0\n",
        "\n",
        "years = []\n",
        "venues = []\n",
        "urls = []\n",
        "\n",
        "for p in papers:\n",
        "    # Key presence\n",
        "    for k in required_keys:\n",
        "        if k not in p:\n",
        "            missing_key_counts[k] += 1\n",
        "\n",
        "    # Title / abstract existence + type + emptiness\n",
        "    t = p.get(\"title\", None)\n",
        "    a = p.get(\"abstract\", None)\n",
        "\n",
        "    if not isinstance(t, str):\n",
        "        non_string_title += 1\n",
        "        t = \"\" if t is None else str(t)\n",
        "    if not isinstance(a, str):\n",
        "        non_string_abstract += 1\n",
        "        a = \"\" if a is None else str(a)\n",
        "\n",
        "    if len(t.strip()) == 0:\n",
        "        empty_title += 1\n",
        "    if len(a.strip()) == 0:\n",
        "        empty_abstract += 1\n",
        "\n",
        "    # Metadata distributions (for plausibility checks)\n",
        "    venues.append(str(p.get(\"venue\", \"\")).strip())\n",
        "    urls.append(str(p.get(\"url\", \"\")).strip())\n",
        "\n",
        "    y = p.get(\"year\", None)\n",
        "    try:\n",
        "        years.append(int(y))\n",
        "    except Exception:\n",
        "        years.append(None)\n",
        "\n",
        "# Report key presence\n",
        "if sum(missing_key_counts.values()) == 0:\n",
        "    print(\" - Key presence: OK (all required keys found in all records)\")\n",
        "else:\n",
        "    print(\" - Key presence issues:\")\n",
        "    for k in sorted(required_keys):\n",
        "        if missing_key_counts[k] > 0:\n",
        "            print(f\"   * Missing '{k}': {missing_key_counts[k]} records\")\n",
        "\n",
        "# Report title/abstract health\n",
        "print(f\" - Empty titles   : {empty_title}\")\n",
        "print(f\" - Empty abstracts: {empty_abstract}\")\n",
        "print(f\" - Non-string titles   : {non_string_title}\")\n",
        "print(f\" - Non-string abstracts: {non_string_abstract}\")\n",
        "\n",
        "# Year plausibility\n",
        "valid_years = [y for y in years if isinstance(y, int)]\n",
        "year_counts = Counter(valid_years)\n",
        "print(f\" - Year distribution (top): {year_counts.most_common(5)}\")\n",
        "\n",
        "outside = [y for y in valid_years if y < 2016 or y > 2018]\n",
        "print(f\" - Years outside 2016–2018: {len(outside)}\")\n",
        "\n",
        "# Venue plausibility\n",
        "venue_counts = Counter([v for v in venues if v])\n",
        "print(f\" - Venue distribution (top): {venue_counts.most_common(5)}\")\n",
        "\n",
        "# Duplicate URLs (very practical duplicate detector)\n",
        "url_counts = Counter([u for u in urls if u])\n",
        "dupe_urls = sum(1 for u, c in url_counts.items() if c > 1)\n",
        "print(f\" - Duplicate URL keys: {dupe_urls} (unique non-empty URLs: {len(url_counts)})\")\n",
        "\n",
        "# --- (6) Access check (prove we can read title/abstract) ---\n",
        "print(f\"\\n[Access check — first record]\")\n",
        "print(f\" - title   : {papers[0].get('title','')[:120]}{'...' if len(papers[0].get('title',''))>120 else ''}\")\n",
        "print(f\" - abstract: {papers[0].get('abstract','')[:200]}{'...' if len(papers[0].get('abstract',''))>200 else ''}\")\n",
        "print(f\" - url     : {papers[0].get('url','')}\")\n",
        "print(f\" - venue   : {papers[0].get('venue','')}\")\n",
        "print(f\" - year    : {papers[0].get('year','')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5a7FAOehkh-"
      },
      "source": [
        "## Part 1:\n",
        "From this point, students should be able to obtain BoW and TF-IDF representations of the dataset, and to obtain similar matches for the new scientific paper titles in the instructions of the exercise. Include here the code use to build the representations, as well as the discussion on the results obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-tWKGq3LQd6"
      },
      "outputs": [],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05P6MYR3lLok"
      },
      "source": [
        "###Step 1: Preprocessing the Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLiO06ahlU5O"
      },
      "source": [
        "###Step 2: Building the BoW and TF-IDF Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x31erbZHlp57"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62WFYhu7lqRi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCXK65aHlq9Y"
      },
      "source": [
        "###Step 3: Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0xuwShjlur3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwFOkv1syVnM"
      },
      "source": [
        "Use the following code snipet to compute the pairwise cosine similarity, and to sort the top 3 candidates for each query. In this example, `query_vectors` and `paper_vectors` correspond to the vectorized collections of queries and papers (as stored in the JSON file previously downloaded), respectively. The variable `paper_texts` contain the list of concatenated titles+abstracts of the papers loaded from the JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OlJS4Ubx7IF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#...\n",
        "\n",
        "similarity_matrix = cosine_similarity(query_vectors, paper_vectors)\n",
        "\n",
        "for nquery in [0, 1, 2]:\n",
        "  similarity_scores = similarity_matrix[nquery]\n",
        "  top_indices = np.argsort(similarity_scores)[::-1][:3]  # Indices of top 3 scores\n",
        "  for i, index in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. Text: '{paper_texts[index]}' (Score: {similarity_scores[index]:.4f})\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYDuwvtrlvCy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiX0btXHlvwx"
      },
      "source": [
        "###Step 4: Analysis of the results obtained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4huZZ3yl4uu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9_ZGeoih9AD"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "In this section students should use `setence-embeddings` to obtain sentence-embedding representations of the dataset and to peform searches for best matches regarding the examples proposed in the instructions of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEVig0EkcCm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr74dlaTkzeG"
      },
      "source": [
        "###Step 1: Trying a general purpose small monolingual model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5sToE6jl-e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBydGHYMl-t-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xHKqT5TmDGF"
      },
      "source": [
        "###Step 2: Comparing other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WPRck-OmB-Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6gmrCnmCWM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orh6Qh9OmHhB"
      },
      "source": [
        "###Step 3: Moving to a multilingual environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBXNcaK-mNa_"
      },
      "source": [
        "When trying multilingual models, you will have to build a multilingual collection of papers. To do so, extend the collection of papers from the EMNLP conference used in the first part of this notebook with an extra collection of papers from the SEPLN conference, which are both in English and Spanish. Create a new dataset that concatenates both collections to try a multilingual search. The collection of SEPLN papers can be downloaded from [https://www.dlsi.ua.es/~mespla/sepln.json](https://www.dlsi.ua.es/~mespla/sepln.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzXQs6-FmOkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeJ3So5UmO1T"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4Xcu1Dmeiy"
      },
      "source": [
        "##Concluding remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izHrjfNBmg5z"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
