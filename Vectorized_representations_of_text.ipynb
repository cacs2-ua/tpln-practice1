{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBki8x4YoCi"
      },
      "source": [
        "# Using vectorized representations of text for semantic text similarity\n",
        "\n",
        "This notebook has been created to allow students of the TNLP 25/26 course to complete their assignment on vectorized representations of text. This notebook is provided with the minimal information to start working on the assignment. Students will have to follow the [instructions](https://mespla.github.io/tpln2526/assignment-searchinvectorialspace/) of the assignment reflecting in this notebook the work done.\n",
        "\n",
        "The starting point will be to install both the `scikit-learn` and the `sentence-embeddings` python libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3_yWE7DX_eD",
        "outputId": "6d65e29e-3902-49bb-fa9c-adb47e567c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (1.7.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.6.0+cpu)\n",
            "Requirement already satisfied: scipy in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.34.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tgcVmtaYmkN"
      },
      "source": [
        "Once this is done, students will have to download and read the file containing the dataset consisting of a list of scientific paper's title and abstract.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWRIDdL4YknO",
        "outputId": "f1e5a49b-e8e3-4a77-d5c3-021740a6963a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Cache] Using existing file: emnlp2016-2018.json\n",
            "\n",
            "[Provenance]\n",
            " - Source URL: https://sbert.net/datasets/emnlp2016-2018.json\n",
            " - Local path: emnlp2016-2018.json\n",
            " - File size : 1.05 MB\n",
            " - SHA-256   : 9e6020503e5f0dd0e91dbb970d47f43c7621f67321249daed5990057e159961c\n",
            "\n",
            "[Sanity checks]\n",
            " - N papers loaded: 974\n",
            " - Key presence: OK (all required keys found in all records)\n",
            " - Empty titles   : 0\n",
            " - Empty abstracts: 0\n",
            " - Non-string titles   : 0\n",
            " - Non-string abstracts: 0\n",
            " - Year distribution (top): [(2018, 549), (2017, 230), (2016, 195)]\n",
            " - Years outside 2016–2018: 0\n",
            " - Venue distribution (top): [('EMNLP', 974)]\n",
            " - Duplicate URL keys: 0 (unique non-empty URLs: 974)\n",
            "\n",
            "[Access check — first record]\n",
            " - title   : Rule Extraction for Tree-to-Tree Transducers by Cost Minimization\n",
            " - abstract: Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set...\n",
            " - url     : http://aclweb.org/anthology/D16-1002\n",
            " - venue   : EMNLP\n",
            " - year    : 2016\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 2 — Dataset Acquisition & Loading (EMNLP 2016–2018 JSON)\n",
        "# =========================\n",
        "\n",
        "import os, json, hashlib\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "DATA_URL = \"https://sbert.net/datasets/emnlp2016-2018.json\"\n",
        "DATA_PATH = \"emnlp2016-2018.json\"   # Feel free to rename, but keep it consistent across the notebook\n",
        "FORCE_DOWNLOAD = False             # Set True if you want to re-download even if the file exists\n",
        "\n",
        "def sha256_of_file(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def download_json(url: str, out_path: str, timeout: int = 120) -> None:\n",
        "    r = requests.get(url, stream=True, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "# --- (1) Acquire the dataset file (download if missing) ---\n",
        "if FORCE_DOWNLOAD or (not os.path.exists(DATA_PATH)):\n",
        "    print(f\"[Download] Fetching dataset from: {DATA_URL}\")\n",
        "    download_json(DATA_URL, DATA_PATH)\n",
        "    print(f\"[Download] Saved to: {DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"[Cache] Using existing file: {DATA_PATH}\")\n",
        "\n",
        "# --- (2) Load JSON into memory ---\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    papers = json.load(f)\n",
        "\n",
        "# Alias for compatibility with the starter template cells\n",
        "data = papers\n",
        "\n",
        "# --- (3) Validate expected JSON structure: root must be a list of dicts ---\n",
        "if not isinstance(papers, list):\n",
        "    raise TypeError(f\"Expected JSON root to be a list, got: {type(papers)}\")\n",
        "\n",
        "if len(papers) == 0:\n",
        "    raise ValueError(\"Dataset loaded but it is empty (N=0). File may be corrupted or not the expected dataset.\")\n",
        "\n",
        "if not isinstance(papers[0], dict):\n",
        "    raise TypeError(f\"Expected each item to be a dict, got first item type: {type(papers[0])}\")\n",
        "\n",
        "# --- (4) Provenance info (practical traceability) ---\n",
        "file_size_mb = os.path.getsize(DATA_PATH) / (1024 * 1024)\n",
        "print(f\"\\n[Provenance]\")\n",
        "print(f\" - Source URL: {DATA_URL}\")\n",
        "print(f\" - Local path: {DATA_PATH}\")\n",
        "print(f\" - File size : {file_size_mb:.2f} MB\")\n",
        "print(f\" - SHA-256   : {sha256_of_file(DATA_PATH)}\")\n",
        "\n",
        "# --- (5) Mandatory sanity checks ---\n",
        "N = len(papers)\n",
        "print(f\"\\n[Sanity checks]\")\n",
        "print(f\" - N papers loaded: {N}\")\n",
        "\n",
        "required_keys = {\"title\", \"abstract\", \"url\", \"venue\", \"year\"}\n",
        "missing_key_counts = Counter()\n",
        "\n",
        "empty_title = 0\n",
        "empty_abstract = 0\n",
        "non_string_title = 0\n",
        "non_string_abstract = 0\n",
        "\n",
        "years = []\n",
        "venues = []\n",
        "urls = []\n",
        "\n",
        "for p in papers:\n",
        "    # Key presence\n",
        "    for k in required_keys:\n",
        "        if k not in p:\n",
        "            missing_key_counts[k] += 1\n",
        "\n",
        "    # Title / abstract existence + type + emptiness\n",
        "    t = p.get(\"title\", None)\n",
        "    a = p.get(\"abstract\", None)\n",
        "\n",
        "    if not isinstance(t, str):\n",
        "        non_string_title += 1\n",
        "        t = \"\" if t is None else str(t)\n",
        "    if not isinstance(a, str):\n",
        "        non_string_abstract += 1\n",
        "        a = \"\" if a is None else str(a)\n",
        "\n",
        "    if len(t.strip()) == 0:\n",
        "        empty_title += 1\n",
        "    if len(a.strip()) == 0:\n",
        "        empty_abstract += 1\n",
        "\n",
        "    # Metadata distributions (for plausibility checks)\n",
        "    venues.append(str(p.get(\"venue\", \"\")).strip())\n",
        "    urls.append(str(p.get(\"url\", \"\")).strip())\n",
        "\n",
        "    y = p.get(\"year\", None)\n",
        "    try:\n",
        "        years.append(int(y))\n",
        "    except Exception:\n",
        "        years.append(None)\n",
        "\n",
        "# Report key presence\n",
        "if sum(missing_key_counts.values()) == 0:\n",
        "    print(\" - Key presence: OK (all required keys found in all records)\")\n",
        "else:\n",
        "    print(\" - Key presence issues:\")\n",
        "    for k in sorted(required_keys):\n",
        "        if missing_key_counts[k] > 0:\n",
        "            print(f\"   * Missing '{k}': {missing_key_counts[k]} records\")\n",
        "\n",
        "# Report title/abstract health\n",
        "print(f\" - Empty titles   : {empty_title}\")\n",
        "print(f\" - Empty abstracts: {empty_abstract}\")\n",
        "print(f\" - Non-string titles   : {non_string_title}\")\n",
        "print(f\" - Non-string abstracts: {non_string_abstract}\")\n",
        "\n",
        "# Year plausibility\n",
        "valid_years = [y for y in years if isinstance(y, int)]\n",
        "year_counts = Counter(valid_years)\n",
        "print(f\" - Year distribution (top): {year_counts.most_common(5)}\")\n",
        "\n",
        "outside = [y for y in valid_years if y < 2016 or y > 2018]\n",
        "print(f\" - Years outside 2016–2018: {len(outside)}\")\n",
        "\n",
        "# Venue plausibility\n",
        "venue_counts = Counter([v for v in venues if v])\n",
        "print(f\" - Venue distribution (top): {venue_counts.most_common(5)}\")\n",
        "\n",
        "# Duplicate URLs (very practical duplicate detector)\n",
        "url_counts = Counter([u for u in urls if u])\n",
        "dupe_urls = sum(1 for u, c in url_counts.items() if c > 1)\n",
        "print(f\" - Duplicate URL keys: {dupe_urls} (unique non-empty URLs: {len(url_counts)})\")\n",
        "\n",
        "# --- (6) Access check (prove we can read title/abstract) ---\n",
        "print(f\"\\n[Access check — first record]\")\n",
        "print(f\" - title   : {papers[0].get('title','')[:120]}{'...' if len(papers[0].get('title',''))>120 else ''}\")\n",
        "print(f\" - abstract: {papers[0].get('abstract','')[:200]}{'...' if len(papers[0].get('abstract',''))>200 else ''}\")\n",
        "print(f\" - url     : {papers[0].get('url','')}\")\n",
        "print(f\" - venue   : {papers[0].get('venue','')}\")\n",
        "print(f\" - year    : {papers[0].get('year','')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5a7FAOehkh-"
      },
      "source": [
        "## Part 1:\n",
        "From this point, students should be able to obtain BoW and TF-IDF representations of the dataset, and to obtain similar matches for the new scientific paper titles in the instructions of the exercise. Include here the code use to build the representations, as well as the discussion on the results obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i-tWKGq3LQd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (2.2.4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05P6MYR3lLok"
      },
      "source": [
        "###Step 1: Preprocessing the Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section3_preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section3_preprocessing.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    _HAS_PANDAS = True\n",
        "except Exception:\n",
        "    pd = None  # type: ignore\n",
        "    _HAS_PANDAS = False\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PrepConfig:\n",
        "    title_key: str = \"title\"\n",
        "    abstract_key: str = \"abstract\"\n",
        "    joiner: str = \" \"\n",
        "    make_dataframe: bool = True\n",
        "\n",
        "\n",
        "def _as_text(x: Any) -> str:\n",
        "    \"\"\"Minimal, non-aggressive coercion: keep text as-is (no casing/punct stripping).\"\"\"\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    return str(x)\n",
        "\n",
        "\n",
        "def build_paper_texts(\n",
        "    records: Sequence[Dict[str, Any]],\n",
        "    config: PrepConfig = PrepConfig(),\n",
        ") -> Tuple[List[str], Optional[\"pd.DataFrame\"]]:\n",
        "    \"\"\"\n",
        "    Build the working 'text' field exactly as required:\n",
        "        text = title + \" \" + abstract\n",
        "\n",
        "    Document unit: one paper = one document.\n",
        "    Returns:\n",
        "      - paper_texts: list[str] with one concatenated document per paper\n",
        "      - df (optional): pandas DataFrame including title/abstract/url/venue/year/text\n",
        "    \"\"\"\n",
        "    if not isinstance(records, (list, tuple)):\n",
        "        raise TypeError(f\"records must be a sequence (list/tuple) of dicts, got {type(records)}\")\n",
        "\n",
        "    paper_texts: List[str] = []\n",
        "    rows_for_df: List[Dict[str, Any]] = []\n",
        "\n",
        "    for i, rec in enumerate(records):\n",
        "        if not isinstance(rec, dict):\n",
        "            raise TypeError(f\"Each record must be a dict. Found {type(rec)} at index {i}.\")\n",
        "\n",
        "        title = _as_text(rec.get(config.title_key, \"\"))\n",
        "        abstract = _as_text(rec.get(config.abstract_key, \"\"))\n",
        "\n",
        "        # Exact construction rule (single space joiner)\n",
        "        text = title + config.joiner + abstract\n",
        "\n",
        "        paper_texts.append(text)\n",
        "\n",
        "        if config.make_dataframe:\n",
        "            rows_for_df.append(\n",
        "                {\n",
        "                    \"title\": title,\n",
        "                    \"abstract\": abstract,\n",
        "                    \"url\": rec.get(\"url\", \"\"),\n",
        "                    \"venue\": rec.get(\"venue\", \"\"),\n",
        "                    \"year\": rec.get(\"year\", \"\"),\n",
        "                    \"text\": text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    df_out = None\n",
        "    if config.make_dataframe:\n",
        "        if not _HAS_PANDAS:\n",
        "            raise ImportError(\"pandas is required for make_dataframe=True, but it is not available.\")\n",
        "        df_out = pd.DataFrame(rows_for_df)\n",
        "\n",
        "    return paper_texts, df_out\n",
        "\n",
        "\n",
        "def validate_paper_texts(paper_texts: Sequence[str], expected_n: int) -> None:\n",
        "    \"\"\"Sanity checks for Section 3 outputs.\"\"\"\n",
        "    if len(paper_texts) != expected_n:\n",
        "        raise AssertionError(f\"len(paper_texts)={len(paper_texts)} != expected_n={expected_n}\")\n",
        "    if any(not isinstance(t, str) for t in paper_texts):\n",
        "        raise AssertionError(\"paper_texts must contain only strings.\")\n",
        "    if any(len(t) == 0 for t in paper_texts):\n",
        "        raise AssertionError(\"paper_texts contains empty strings (unexpected for this dataset).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 3] Preprocessing complete.\n",
            " - Document unit          : 1 paper = 1 document\n",
            " - N documents (paper_texts): 974\n",
            " - DataFrame created      : True\n",
            "\n",
            "[Section 3] Example document (first record):\n",
            "Rule Extraction for Tree-to-Tree Transducers by Cost Minimization Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the ...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>url</th>\n",
              "      <th>venue</th>\n",
              "      <th>year</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rule Extraction for Tree-to-Tree Transducers b...</td>\n",
              "      <td>Finite-state transducers give efficient repres...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1002</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>Rule Extraction for Tree-to-Tree Transducers b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Neural Network for Coordination Boundary Pre...</td>\n",
              "      <td>We propose a neural-network based model for co...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1003</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>A Neural Network for Coordination Boundary Pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Distinguishing Past, On-going, and Future Even...</td>\n",
              "      <td>The tremendous amount of user generated data t...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1005</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>Distinguishing Past, On-going, and Future Even...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  Rule Extraction for Tree-to-Tree Transducers b...   \n",
              "1  A Neural Network for Coordination Boundary Pre...   \n",
              "2  Distinguishing Past, On-going, and Future Even...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  Finite-state transducers give efficient repres...   \n",
              "1  We propose a neural-network based model for co...   \n",
              "2  The tremendous amount of user generated data t...   \n",
              "\n",
              "                                    url  venue  year  \\\n",
              "0  http://aclweb.org/anthology/D16-1002  EMNLP  2016   \n",
              "1  http://aclweb.org/anthology/D16-1003  EMNLP  2016   \n",
              "2  http://aclweb.org/anthology/D16-1005  EMNLP  2016   \n",
              "\n",
              "                                                text  \n",
              "0  Rule Extraction for Tree-to-Tree Transducers b...  \n",
              "1  A Neural Network for Coordination Boundary Pre...  \n",
              "2  Distinguishing Past, On-going, and Future Even...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 3 — Data Preparation for Vectorization (Part 1 – Step 1)\n",
        "# =========================\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "# Use the variable created in Section 2 (you already set: data = papers)\n",
        "records = data  # list of dicts, one paper per record\n",
        "\n",
        "config = PrepConfig(\n",
        "    title_key=\"title\",\n",
        "    abstract_key=\"abstract\",\n",
        "    joiner=\" \",          # MUST be exactly one space\n",
        "    make_dataframe=True  # optional, but very useful in a notebook\n",
        ")\n",
        "\n",
        "paper_texts, papers_df = build_paper_texts(records, config=config)\n",
        "\n",
        "# Robust sanity checks\n",
        "N = len(records)\n",
        "validate_paper_texts(paper_texts, expected_n=N)\n",
        "\n",
        "print(\"[Section 3] Preprocessing complete.\")\n",
        "print(f\" - Document unit          : 1 paper = 1 document\")\n",
        "print(f\" - N documents (paper_texts): {len(paper_texts)}\")\n",
        "print(f\" - DataFrame created      : {papers_df is not None}\")\n",
        "print(\"\\n[Section 3] Example document (first record):\")\n",
        "print(paper_texts[0][:400] + (\"...\" if len(paper_texts[0]) > 400 else \"\"))\n",
        "\n",
        "# Optional: quick inspection (keeps later steps easier)\n",
        "if papers_df is not None:\n",
        "    display(papers_df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_section3_preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section3_preprocessing.py\n",
        "import unittest\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)  # should not raise\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        # title becomes \"123\", abstract becomes \"\"\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "    def test_dataframe_creation_if_enabled(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"A\", \"url\": \"u\", \"venue\": \"EMNLP\", \"year\": 2016}]\n",
        "        cfg = PrepConfig(make_dataframe=True)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNotNone(df)\n",
        "        self.assertIn(\"text\", df.columns)\n",
        "        self.assertEqual(df.loc[0, \"text\"], paper_texts[0])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_section3_preprocessing.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_dataframe_creation_if_enabled (test_section3_preprocessing.TestSection3Preprocessing.test_dataframe_creation_if_enabled) ... ok\n",
            "test_rejects_non_dict_records (test_section3_preprocessing.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_section3_preprocessing.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_section3_preprocessing.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.002s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_section3_preprocessing.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLiO06ahlU5O"
      },
      "source": [
        "###Step 2: Building the BoW and TF-IDF Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section4_bow.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section4_bow.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BoWConfig:\n",
        "    \"\"\"\n",
        "    Configuration for a BoW (CountVectorizer) run.\n",
        "\n",
        "    Required toggles for the assignment:\n",
        "      - lowercase: ON vs OFF\n",
        "      - stop_words: None vs 'english'\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    lowercase: bool = True\n",
        "    stop_words: Optional[str] = None\n",
        "    max_features: Optional[int] = None\n",
        "    ngram_range: Tuple[int, int] = (1, 1)\n",
        "\n",
        "\n",
        "def _validate_texts(texts: Sequence[str]) -> None:\n",
        "    if not isinstance(texts, (list, tuple)):\n",
        "        raise TypeError(f\"texts must be a list/tuple of strings, got {type(texts)}\")\n",
        "    if len(texts) == 0:\n",
        "        raise ValueError(\"texts is empty (N=0).\")\n",
        "    if any(not isinstance(t, str) for t in texts):\n",
        "        bad = next(type(t) for t in texts if not isinstance(t, str))\n",
        "        raise TypeError(f\"All texts must be strings. Found non-string: {bad}\")\n",
        "    if any(len(t) == 0 for t in texts):\n",
        "        raise ValueError(\"texts contains empty strings (unexpected for this dataset).\")\n",
        "\n",
        "\n",
        "def build_bow_vectors(\n",
        "    texts: Sequence[str],\n",
        "    config: BoWConfig,\n",
        "):\n",
        "    \"\"\"\n",
        "    Fit a CountVectorizer and return:\n",
        "      - vectorizer (fitted)\n",
        "      - X (document-term matrix, sparse)\n",
        "    \"\"\"\n",
        "    _validate_texts(texts)\n",
        "\n",
        "    vectorizer = CountVectorizer(\n",
        "        lowercase=config.lowercase,\n",
        "        stop_words=config.stop_words,\n",
        "        max_features=config.max_features,\n",
        "        ngram_range=config.ngram_range,\n",
        "    )\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return vectorizer, X\n",
        "\n",
        "\n",
        "def sparse_matrix_stats(X) -> Dict[str, Any]:\n",
        "    \"\"\"Compute basic inspection stats for a sparse doc-term matrix.\"\"\"\n",
        "    n_docs, n_vocab = X.shape\n",
        "    nnz = int(X.nnz)\n",
        "    total = int(n_docs) * int(n_vocab)\n",
        "    density = float(nnz / total) if total > 0 else 0.0\n",
        "    avg_nnz_per_doc = float(nnz / n_docs) if n_docs > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"n_docs\": int(n_docs),\n",
        "        \"n_vocab\": int(n_vocab),\n",
        "        \"shape\": (int(n_docs), int(n_vocab)),\n",
        "        \"nnz\": nnz,\n",
        "        \"density\": density,\n",
        "        \"avg_nnz_per_doc\": avg_nnz_per_doc,\n",
        "        \"matrix_type\": type(X).__name__,\n",
        "    }\n",
        "\n",
        "\n",
        "def feature_examples(vectorizer: CountVectorizer, n: int = 20) -> List[str]:\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    return feats[:n].tolist()\n",
        "\n",
        "\n",
        "def case_sensitive_stopword_survivors(vectorizer: CountVectorizer) -> int:\n",
        "    \"\"\"\n",
        "    Detect the nuance: with lowercase=False and stop_words='english',\n",
        "    capitalized variants of stopwords (e.g., 'The') can survive.\n",
        "\n",
        "    Returns how many vocabulary terms are \"stopwords after lowercasing\"\n",
        "    but are not exactly in the stopword list (case mismatch).\n",
        "    \"\"\"\n",
        "    stop = vectorizer.get_stop_words()\n",
        "    if not stop:\n",
        "        return 0\n",
        "\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    stop_set = set(stop)\n",
        "\n",
        "    survivors = 0\n",
        "    for f in feats:\n",
        "        # survivors like \"The\" where f.lower() in stop_set but f not in stop_set\n",
        "        fl = f.lower()\n",
        "        if fl in stop_set and f not in stop_set:\n",
        "            survivors += 1\n",
        "    return survivors\n",
        "\n",
        "\n",
        "def run_bow_grid(texts: Sequence[str]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run the 2x2 grid required by the assignment:\n",
        "      - lowercase ON/OFF\n",
        "      - stopwords OFF/ON ('english')\n",
        "    Returns a dict keyed by config name with vectorizer, X, and stats.\n",
        "    \"\"\"\n",
        "    configs = [\n",
        "        BoWConfig(name=\"bow_lc_on_sw_off\", lowercase=True,  stop_words=None),\n",
        "        BoWConfig(name=\"bow_lc_on_sw_on\",  lowercase=True,  stop_words=\"english\"),\n",
        "        BoWConfig(name=\"bow_lc_off_sw_off\",lowercase=False, stop_words=None),\n",
        "        BoWConfig(name=\"bow_lc_off_sw_on\", lowercase=False, stop_words=\"english\"),\n",
        "    ]\n",
        "\n",
        "    results: Dict[str, Dict[str, Any]] = {}\n",
        "    for cfg in configs:\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        stats = sparse_matrix_stats(X)\n",
        "        survivors = case_sensitive_stopword_survivors(vect) if cfg.stop_words else 0\n",
        "\n",
        "        results[cfg.name] = {\n",
        "            \"config\": cfg,\n",
        "            \"vectorizer\": vect,\n",
        "            \"X\": X,\n",
        "            \"stats\": stats,\n",
        "            \"case_sensitive_stopword_survivors\": survivors,\n",
        "        }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "x31erbZHlp57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 4] BoW experiments complete.\n",
            "Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\n",
            "Each column is a token; each cell is the token count in that document.\n",
            "\n",
            "--- bow_lc_on_sw_off ---\n",
            "Config: lowercase=True, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 8404)  (#docs x #vocab)\n",
            "nnz         : 90157\n",
            "Density     : 0.011014\n",
            "Avg nnz/doc : 92.56\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n",
            "--- bow_lc_on_sw_on ---\n",
            "Config: lowercase=True, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 8153)  (#docs x #vocab)\n",
            "nnz         : 64523\n",
            "Density     : 0.008125\n",
            "Avg nnz/doc : 66.25\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n",
            "--- bow_lc_off_sw_off ---\n",
            "Config: lowercase=False, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 10342)  (#docs x #vocab)\n",
            "nnz         : 96257\n",
            "Density     : 0.009556\n",
            "Avg nnz/doc : 98.83\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n",
            "--- bow_lc_off_sw_on ---\n",
            "Config: lowercase=False, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Shape       : (974, 10094)  (#docs x #vocab)\n",
            "nnz         : 72143\n",
            "Density     : 0.007338\n",
            "Avg nnz/doc : 74.07\n",
            "Stopword casing nuance: 155 vocab terms look like stopwords after lowercasing (e.g., 'The')\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run</th>\n",
              "      <th>lowercase</th>\n",
              "      <th>stop_words</th>\n",
              "      <th>N_docs</th>\n",
              "      <th>V_vocab</th>\n",
              "      <th>nnz</th>\n",
              "      <th>density</th>\n",
              "      <th>avg_nnz_per_doc</th>\n",
              "      <th>case_sensitive_stopword_survivors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bow_lc_on_sw_off</td>\n",
              "      <td>True</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>8404</td>\n",
              "      <td>90157</td>\n",
              "      <td>0.011014</td>\n",
              "      <td>92.563655</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bow_lc_on_sw_on</td>\n",
              "      <td>True</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>8153</td>\n",
              "      <td>64523</td>\n",
              "      <td>0.008125</td>\n",
              "      <td>66.245380</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bow_lc_off_sw_off</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>10342</td>\n",
              "      <td>96257</td>\n",
              "      <td>0.009556</td>\n",
              "      <td>98.826489</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bow_lc_off_sw_on</td>\n",
              "      <td>False</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>10094</td>\n",
              "      <td>72143</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>74.068789</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 run  lowercase stop_words  N_docs  V_vocab    nnz   density  \\\n",
              "0   bow_lc_on_sw_off       True       None     974     8404  90157  0.011014   \n",
              "1    bow_lc_on_sw_on       True    english     974     8153  64523  0.008125   \n",
              "2  bow_lc_off_sw_off      False       None     974    10342  96257  0.009556   \n",
              "3   bow_lc_off_sw_on      False    english     974    10094  72143  0.007338   \n",
              "\n",
              "   avg_nnz_per_doc  case_sensitive_stopword_survivors  \n",
              "0        92.563655                                  0  \n",
              "1        66.245380                                  0  \n",
              "2        98.826489                                  0  \n",
              "3        74.068789                                155  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 4] Default BoW run set for later use: bow_lc_on_sw_off\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 4 — BoW Vectorization Experiments (Part 1 – Step 2, BoW branch)\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from section4_bow import run_bow_grid, feature_examples\n",
        "\n",
        "# Input from Section 3:\n",
        "# - paper_texts: list[str] where each element is \"title + ' ' + abstract\"\n",
        "texts = paper_texts\n",
        "\n",
        "bow_results = run_bow_grid(texts)\n",
        "\n",
        "# Deterministic order (avoids confusing output + makes comparison easier)\n",
        "RUN_ORDER = [\"bow_lc_on_sw_off\", \"bow_lc_on_sw_on\", \"bow_lc_off_sw_off\", \"bow_lc_off_sw_on\"]\n",
        "\n",
        "def vocab_letter_sample(vectorizer, n: int = 20):\n",
        "    \"\"\"Return the first n features that contain at least one alphabetic character.\"\"\"\n",
        "    feats = vectorizer.get_feature_names_out().tolist()\n",
        "    letter_feats = [f for f in feats if re.search(r\"[A-Za-z]\", f)]\n",
        "    return letter_feats[:n]\n",
        "\n",
        "print(\"[Section 4] BoW experiments complete.\")\n",
        "print(\"Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\")\n",
        "print(\"Each column is a token; each cell is the token count in that document.\\n\")\n",
        "\n",
        "# Print compact inspection per configuration (stable ordering + hard sanity checks)\n",
        "for name in RUN_ORDER:\n",
        "    out = bow_results[name]\n",
        "    cfg = out[\"config\"]\n",
        "    stats = out[\"stats\"]\n",
        "    survivors = out[\"case_sensitive_stopword_survivors\"]\n",
        "\n",
        "    # Robust invariants (fail fast if something is wrong)\n",
        "    if stats[\"n_docs\"] != len(texts):\n",
        "        raise RuntimeError(\n",
        "            f\"Row mismatch in {name}: X has {stats['n_docs']} rows but texts has {len(texts)} documents\"\n",
        "        )\n",
        "    if stats[\"n_vocab\"] <= 0 or stats[\"nnz\"] <= 0:\n",
        "        raise RuntimeError(\n",
        "            f\"Degenerate BoW matrix in {name}: n_vocab={stats['n_vocab']}, nnz={stats['nnz']}\"\n",
        "        )\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"Config: lowercase={cfg.lowercase}, stop_words={cfg.stop_words}\")\n",
        "    print(f\"Matrix type : {stats['matrix_type']} (sparse)\")\n",
        "    print(f\"Shape       : {stats['shape']}  (#docs x #vocab)\")\n",
        "    print(f\"nnz         : {stats['nnz']}\")\n",
        "    print(f\"Density     : {stats['density']:.6f}\")\n",
        "    print(f\"Avg nnz/doc : {stats['avg_nnz_per_doc']:.2f}\")\n",
        "\n",
        "    # Stopword casing nuance indicator (only relevant when lowercase=False and stopwords ON)\n",
        "    if cfg.stop_words is not None and cfg.lowercase is False:\n",
        "        print(\n",
        "            f\"Stopword casing nuance: {survivors} vocab terms look like stopwords after lowercasing (e.g., 'The')\"\n",
        "        )\n",
        "\n",
        "    # Two samples: (1) raw first tokens (often numeric), (2) a more meaningful sample with letters\n",
        "    print(f\"Vocabulary sample (first 20 terms): {feature_examples(out['vectorizer'], n=20)}\")\n",
        "    print(f\"Vocabulary sample with letters (first 20): {vocab_letter_sample(out['vectorizer'], n=20)}\\n\")\n",
        "\n",
        "# Optional: a small summary table (nice for the report)\n",
        "try:\n",
        "    import pandas as pd\n",
        "\n",
        "    rows = []\n",
        "    for name in RUN_ORDER:\n",
        "        out = bow_results[name]\n",
        "        cfg = out[\"config\"]\n",
        "        s = out[\"stats\"]\n",
        "        rows.append(\n",
        "            {\n",
        "                \"run\": name,\n",
        "                \"lowercase\": cfg.lowercase,\n",
        "                \"stop_words\": cfg.stop_words,\n",
        "                \"N_docs\": s[\"n_docs\"],\n",
        "                \"V_vocab\": s[\"n_vocab\"],\n",
        "                \"nnz\": s[\"nnz\"],\n",
        "                \"density\": s[\"density\"],\n",
        "                \"avg_nnz_per_doc\": s[\"avg_nnz_per_doc\"],\n",
        "                \"case_sensitive_stopword_survivors\": out[\"case_sensitive_stopword_survivors\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    bow_summary_df = pd.DataFrame(rows)\n",
        "    bow_summary_df[\"run\"] = pd.Categorical(bow_summary_df[\"run\"], categories=RUN_ORDER, ordered=True)\n",
        "    bow_summary_df = bow_summary_df.sort_values(by=[\"run\"]).reset_index(drop=True)\n",
        "    display(bow_summary_df)\n",
        "\n",
        "except Exception:\n",
        "    bow_summary_df = None\n",
        "\n",
        "# Provide a default BoW representation for later sections if you want:\n",
        "# (This prevents NameError later if you decide to use BoW first in Step 3.)\n",
        "DEFAULT_BOW_RUN = \"bow_lc_on_sw_off\"\n",
        "bow_vectorizer = bow_results[DEFAULT_BOW_RUN][\"vectorizer\"]\n",
        "bow_paper_vectors = bow_results[DEFAULT_BOW_RUN][\"X\"]\n",
        "print(f\"[Section 4] Default BoW run set for later use: {DEFAULT_BOW_RUN}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import unittest\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "from section4_bow import (\n",
        "    BoWConfig,\n",
        "    build_bow_vectors,\n",
        "    sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors,\n",
        ")\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)  # should not raise\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "\n",
        "class TestSection4BoW(unittest.TestCase):\n",
        "    def test_build_bow_vectors_shape_and_sparse(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = BoWConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))  # sparse matrices expose nnz\n",
        "        self.assertTrue(hasattr(vect, \"vocabulary_\"))\n",
        "\n",
        "    def test_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, X_off = build_bow_vectors(texts, BoWConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, X_on = build_bow_vectors(texts, BoWConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "        self.assertEqual(X_off.shape[0], X_on.shape[0])\n",
        "\n",
        "    def test_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_bow_vectors(texts, BoWConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_bow_vectors(texts, BoWConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]  # 'The' may survive when lowercase=False with english stopwords\n",
        "        vect, _ = build_bow_vectors(texts, BoWConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_sparse_stats_sane(self):\n",
        "        texts = [\"cat sat\", \"dog barked\"]\n",
        "        vect, X = build_bow_vectors(texts, BoWConfig(name=\"s\", lowercase=True, stop_words=None))\n",
        "        s = sparse_matrix_stats(X)\n",
        "        self.assertEqual(s[\"n_docs\"], 2)\n",
        "        self.assertGreater(s[\"n_vocab\"], 0)\n",
        "        self.assertGreaterEqual(s[\"density\"], 0.0)\n",
        "        self.assertLessEqual(s[\"density\"], 1.0)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_all.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_rejects_non_dict_records (test_all.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_all.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_all.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "test_build_bow_vectors_shape_and_sparse (test_all.TestSection4BoW.test_build_bow_vectors_shape_and_sparse) ... ok\n",
            "test_case_sensitive_stopword_survivors_detected (test_all.TestSection4BoW.test_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_lowercase_merges_tokens (test_all.TestSection4BoW.test_lowercase_merges_tokens) ... ok\n",
            "test_sparse_stats_sane (test_all.TestSection4BoW.test_sparse_stats_sane) ... ok\n",
            "test_stopwords_removed_when_lowercase_true (test_all.TestSection4BoW.test_stopwords_removed_when_lowercase_true) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 9 tests in 0.006s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_all.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62WFYhu7lqRi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section5_tfidf.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section5_tfidf.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TfidfConfig:\n",
        "    \"\"\"\n",
        "    Configuration for a TF-IDF (TfidfVectorizer) run.\n",
        "\n",
        "    Required toggles for the assignment:\n",
        "      - lowercase: ON vs OFF\n",
        "      - stop_words: None vs 'english'\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    lowercase: bool = True\n",
        "    stop_words: Optional[str] = None\n",
        "    max_features: Optional[int] = None\n",
        "    ngram_range: Tuple[int, int] = (1, 1)\n",
        "\n",
        "    # Keep sklearn defaults explicit for clarity in the report\n",
        "    norm: Optional[str] = \"l2\"\n",
        "    use_idf: bool = True\n",
        "    smooth_idf: bool = True\n",
        "    sublinear_tf: bool = False\n",
        "\n",
        "\n",
        "def _validate_texts(texts: Sequence[str]) -> None:\n",
        "    if not isinstance(texts, (list, tuple)):\n",
        "        raise TypeError(f\"texts must be a list/tuple of strings, got {type(texts)}\")\n",
        "    if len(texts) == 0:\n",
        "        raise ValueError(\"texts is empty (N=0).\")\n",
        "    if any(not isinstance(t, str) for t in texts):\n",
        "        bad = next(type(t) for t in texts if not isinstance(t, str))\n",
        "        raise TypeError(f\"All texts must be strings. Found non-string: {bad}\")\n",
        "    if any(len(t) == 0 for t in texts):\n",
        "        raise ValueError(\"texts contains empty strings (unexpected for this dataset).\")\n",
        "\n",
        "\n",
        "def build_tfidf_vectors(\n",
        "    texts: Sequence[str],\n",
        "    config: TfidfConfig,\n",
        "):\n",
        "    \"\"\"\n",
        "    Fit a TfidfVectorizer and return:\n",
        "      - vectorizer (fitted)\n",
        "      - X (document-term TF-IDF matrix, sparse)\n",
        "    \"\"\"\n",
        "    _validate_texts(texts)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        lowercase=config.lowercase,\n",
        "        stop_words=config.stop_words,\n",
        "        max_features=config.max_features,\n",
        "        ngram_range=config.ngram_range,\n",
        "        norm=config.norm,\n",
        "        use_idf=config.use_idf,\n",
        "        smooth_idf=config.smooth_idf,\n",
        "        sublinear_tf=config.sublinear_tf,\n",
        "    )\n",
        "    X = vectorizer.fit_transform(texts)\n",
        "    return vectorizer, X\n",
        "\n",
        "\n",
        "def sparse_matrix_stats(X) -> Dict[str, Any]:\n",
        "    \"\"\"Compute basic inspection stats for a sparse doc-term matrix.\"\"\"\n",
        "    n_docs, n_vocab = X.shape\n",
        "    nnz = int(X.nnz)\n",
        "    total = int(n_docs) * int(n_vocab)\n",
        "    density = float(nnz / total) if total > 0 else 0.0\n",
        "    avg_nnz_per_doc = float(nnz / n_docs) if n_docs > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        \"n_docs\": int(n_docs),\n",
        "        \"n_vocab\": int(n_vocab),\n",
        "        \"shape\": (int(n_docs), int(n_vocab)),\n",
        "        \"nnz\": nnz,\n",
        "        \"density\": density,\n",
        "        \"avg_nnz_per_doc\": avg_nnz_per_doc,\n",
        "        \"matrix_type\": type(X).__name__,\n",
        "        \"dtype\": str(getattr(X, \"dtype\", \"unknown\")),\n",
        "    }\n",
        "\n",
        "\n",
        "def feature_examples(vectorizer: TfidfVectorizer, n: int = 20) -> List[str]:\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    return feats[:n].tolist()\n",
        "\n",
        "\n",
        "def case_sensitive_stopword_survivors(vectorizer: TfidfVectorizer) -> int:\n",
        "    \"\"\"\n",
        "    Detect the nuance: with lowercase=False and stop_words='english',\n",
        "    capitalized variants of stopwords (e.g., 'The') can survive.\n",
        "    \"\"\"\n",
        "    stop = vectorizer.get_stop_words()\n",
        "    if not stop:\n",
        "        return 0\n",
        "\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    stop_set = set(stop)\n",
        "\n",
        "    survivors = 0\n",
        "    for f in feats:\n",
        "        fl = f.lower()\n",
        "        if fl in stop_set and f not in stop_set:\n",
        "            survivors += 1\n",
        "    return survivors\n",
        "\n",
        "\n",
        "def l2_norm_sample_stats(X, sample_n: int = 50, seed: int = 0) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    TF-IDF vectors in sklearn are L2-normalized by default (norm='l2').\n",
        "    This helper checks that empirically on a sample of rows.\n",
        "    \"\"\"\n",
        "    n_docs = X.shape[0]\n",
        "    if n_docs == 0:\n",
        "        return {\"min\": 0.0, \"mean\": 0.0, \"max\": 0.0}\n",
        "\n",
        "    sample_n = int(min(sample_n, n_docs))\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = rng.choice(n_docs, size=sample_n, replace=False)\n",
        "\n",
        "    Xs = X[idx]\n",
        "    norms = np.sqrt(np.asarray(Xs.multiply(Xs).sum(axis=1)).ravel())\n",
        "\n",
        "    return {\n",
        "        \"min\": float(np.min(norms)),\n",
        "        \"mean\": float(np.mean(norms)),\n",
        "        \"max\": float(np.max(norms)),\n",
        "    }\n",
        "\n",
        "\n",
        "def top_terms_for_doc(\n",
        "    X,\n",
        "    vectorizer: TfidfVectorizer,\n",
        "    doc_index: int,\n",
        "    top_n: int = 10,\n",
        ") -> List[Tuple[str, float]]:\n",
        "    \"\"\"Return top-N (term, tf-idf weight) pairs for one document row.\"\"\"\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    row = X.getrow(doc_index)\n",
        "    if row.nnz == 0:\n",
        "        return []\n",
        "\n",
        "    order = np.argsort(row.data)[::-1][:top_n]\n",
        "    return [(feats[row.indices[i]], float(row.data[i])) for i in order]\n",
        "\n",
        "\n",
        "def run_tfidf_grid(texts: Sequence[str], sample_n_for_norms: int = 50) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run the 2x2 grid required by the assignment:\n",
        "      - lowercase ON/OFF\n",
        "      - stopwords OFF/ON ('english')\n",
        "    \"\"\"\n",
        "    configs = [\n",
        "        TfidfConfig(name=\"tfidf_lc_on_sw_off\",  lowercase=True,  stop_words=None),\n",
        "        TfidfConfig(name=\"tfidf_lc_on_sw_on\",   lowercase=True,  stop_words=\"english\"),\n",
        "        TfidfConfig(name=\"tfidf_lc_off_sw_off\", lowercase=False, stop_words=None),\n",
        "        TfidfConfig(name=\"tfidf_lc_off_sw_on\",  lowercase=False, stop_words=\"english\"),\n",
        "    ]\n",
        "\n",
        "    results: Dict[str, Dict[str, Any]] = {}\n",
        "    for cfg in configs:\n",
        "        vect, X = build_tfidf_vectors(texts, cfg)\n",
        "        stats = sparse_matrix_stats(X)\n",
        "        survivors = case_sensitive_stopword_survivors(vect) if cfg.stop_words else 0\n",
        "        norms = l2_norm_sample_stats(X, sample_n=sample_n_for_norms, seed=0)\n",
        "\n",
        "        results[cfg.name] = {\n",
        "            \"config\": cfg,\n",
        "            \"vectorizer\": vect,\n",
        "            \"X\": X,\n",
        "            \"stats\": stats,\n",
        "            \"l2_norm_sample\": norms,\n",
        "            \"case_sensitive_stopword_survivors\": survivors,\n",
        "        }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 5] TF-IDF experiments complete.\n",
            "Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\n",
            "Each column is a token; each cell is a TF-IDF weight (NOT a raw count).\n",
            "Note: sklearn TF-IDF uses L2-normalization by default, so ||doc_vector||_2 ≈ 1.\n",
            "\n",
            "--- tfidf_lc_on_sw_off ---\n",
            "Config: lowercase=True, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 8404)  (#docs x #vocab)\n",
            "nnz         : 90157\n",
            "Density     : 0.011014\n",
            "Avg nnz/doc : 92.56\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - transducers           0.3709\n",
            " - minimization          0.3430\n",
            " - tree                  0.2335\n",
            " - complex               0.2053\n",
            " - restrictions          0.1854\n",
            " - encountered           0.1715\n",
            " - representations       0.1600\n",
            " - compact               0.1525\n",
            " - involving             0.1499\n",
            " - finite                0.1475\n",
            "\n",
            "--- tfidf_lc_on_sw_on ---\n",
            "Config: lowercase=True, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 8153)  (#docs x #vocab)\n",
            "nnz         : 64523\n",
            "Density     : 0.008125\n",
            "Avg nnz/doc : 66.25\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100k', '10m', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100k', '10m', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1a', '1d', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - transducers           0.4070\n",
            " - minimization          0.3764\n",
            " - tree                  0.2563\n",
            " - complex               0.2253\n",
            " - restrictions          0.2035\n",
            " - encountered           0.1882\n",
            " - representations       0.1756\n",
            " - compact               0.1674\n",
            " - involving             0.1645\n",
            " - finite                0.1619\n",
            "\n",
            "--- tfidf_lc_off_sw_off ---\n",
            "Config: lowercase=False, stop_words=None\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 10342)  (#docs x #vocab)\n",
            "nnz         : 96257\n",
            "Density     : 0.009556\n",
            "Avg nnz/doc : 98.83\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - Tree                  0.2795\n",
            " - complex               0.2054\n",
            " - Cost                  0.1966\n",
            " - Minimization          0.1966\n",
            " - Transducers           0.1966\n",
            " - Rule                  0.1855\n",
            " - restrictions          0.1855\n",
            " - transducers           0.1855\n",
            " - Finite                0.1855\n",
            " - minimization          0.1716\n",
            "\n",
            "--- tfidf_lc_off_sw_on ---\n",
            "Config: lowercase=False, stop_words=english\n",
            "Matrix type : csr_matrix (sparse)\n",
            "Dtype       : float64 (TF-IDF weights are floating-point)\n",
            "Shape       : (974, 10094)  (#docs x #vocab)\n",
            "nnz         : 72143\n",
            "Density     : 0.007338\n",
            "Avg nnz/doc : 74.07\n",
            "L2-norm sample (min/mean/max): 1.0000 / 1.0000 / 1.0000\n",
            "Stopword casing nuance: 155 vocab terms look like stopwords after lowercasing (e.g., 'The')\n",
            "Vocabulary sample (first 20 terms): ['000', '01', '02', '05', '050', '05365v2', '06', '06318v4', '09', '091', '094', '10', '100', '100K', '10M', '11', '110', '113k', '117', '118']\n",
            "Vocabulary sample with letters (first 20): ['05365v2', '06318v4', '100K', '10M', '113k', '11x', '128x128', '12th', '12x', '14x', '1971a', '1A', '1D', '1k', '1st', '2005a', '2007b', '2013b', '2015a', '2016a']\n",
            "Top TF-IDF terms (doc 0):\n",
            " - Tree                  0.3069\n",
            " - complex               0.2256\n",
            " - Minimization          0.2159\n",
            " - Cost                  0.2159\n",
            " - Transducers           0.2159\n",
            " - restrictions          0.2037\n",
            " - Rule                  0.2037\n",
            " - Finite                0.2037\n",
            " - transducers           0.2037\n",
            " - minimization          0.1884\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run</th>\n",
              "      <th>lowercase</th>\n",
              "      <th>stop_words</th>\n",
              "      <th>N_docs</th>\n",
              "      <th>V_vocab</th>\n",
              "      <th>nnz</th>\n",
              "      <th>density</th>\n",
              "      <th>avg_nnz_per_doc</th>\n",
              "      <th>l2_norm_mean(sample)</th>\n",
              "      <th>case_sensitive_stopword_survivors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tfidf_lc_on_sw_off</td>\n",
              "      <td>True</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>8404</td>\n",
              "      <td>90157</td>\n",
              "      <td>0.011014</td>\n",
              "      <td>92.563655</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tfidf_lc_on_sw_on</td>\n",
              "      <td>True</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>8153</td>\n",
              "      <td>64523</td>\n",
              "      <td>0.008125</td>\n",
              "      <td>66.245380</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tfidf_lc_off_sw_off</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>974</td>\n",
              "      <td>10342</td>\n",
              "      <td>96257</td>\n",
              "      <td>0.009556</td>\n",
              "      <td>98.826489</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tfidf_lc_off_sw_on</td>\n",
              "      <td>False</td>\n",
              "      <td>english</td>\n",
              "      <td>974</td>\n",
              "      <td>10094</td>\n",
              "      <td>72143</td>\n",
              "      <td>0.007338</td>\n",
              "      <td>74.068789</td>\n",
              "      <td>1.0</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   run  lowercase stop_words  N_docs  V_vocab    nnz  \\\n",
              "0   tfidf_lc_on_sw_off       True       None     974     8404  90157   \n",
              "1    tfidf_lc_on_sw_on       True    english     974     8153  64523   \n",
              "2  tfidf_lc_off_sw_off      False       None     974    10342  96257   \n",
              "3   tfidf_lc_off_sw_on      False    english     974    10094  72143   \n",
              "\n",
              "    density  avg_nnz_per_doc  l2_norm_mean(sample)  \\\n",
              "0  0.011014        92.563655                   1.0   \n",
              "1  0.008125        66.245380                   1.0   \n",
              "2  0.009556        98.826489                   1.0   \n",
              "3  0.007338        74.068789                   1.0   \n",
              "\n",
              "   case_sensitive_stopword_survivors  \n",
              "0                                  0  \n",
              "1                                  0  \n",
              "2                                  0  \n",
              "3                                155  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 5] Default TF-IDF run set for later use: tfidf_lc_on_sw_off\n",
            "\n",
            "[Section 5] BoW vs TF-IDF (doc 0 example)\n",
            "Top BoW terms by COUNT (doc 0):\n",
            " - of                    4\n",
            " - to                    4\n",
            " - tree                  2\n",
            " - for                   2\n",
            " - minimization          2\n",
            " - representations       2\n",
            " - complex               2\n",
            " - the                   2\n",
            " - transducers           2\n",
            " - operation             1\n",
            "\n",
            "Top TF-IDF terms by WEIGHT (doc 0):\n",
            " - transducers           0.3709\n",
            " - minimization          0.3430\n",
            " - tree                  0.2335\n",
            " - complex               0.2053\n",
            " - restrictions          0.1854\n",
            " - encountered           0.1715\n",
            " - representations       0.1600\n",
            " - compact               0.1525\n",
            " - involving             0.1499\n",
            " - finite                0.1475\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 5 — TF-IDF Vectorization Experiments (Part 1 – Step 2, TF-IDF branch)\n",
        "# =========================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "from section5_tfidf import run_tfidf_grid, feature_examples, top_terms_for_doc\n",
        "\n",
        "# Input from Section 3:\n",
        "texts = paper_texts\n",
        "\n",
        "tfidf_results = run_tfidf_grid(texts, sample_n_for_norms=100)\n",
        "\n",
        "RUN_ORDER_TFIDF = [\"tfidf_lc_on_sw_off\", \"tfidf_lc_on_sw_on\", \"tfidf_lc_off_sw_off\", \"tfidf_lc_off_sw_on\"]\n",
        "\n",
        "def vocab_letter_sample(vectorizer, n: int = 20):\n",
        "    feats = vectorizer.get_feature_names_out().tolist()\n",
        "    letter_feats = [f for f in feats if re.search(r\"[A-Za-z]\", f)]\n",
        "    return letter_feats[:n]\n",
        "\n",
        "print(\"[Section 5] TF-IDF experiments complete.\")\n",
        "print(\"Each run builds a sparse document-term matrix X with shape (N_documents, V_vocabulary).\")\n",
        "print(\"Each column is a token; each cell is a TF-IDF weight (NOT a raw count).\")\n",
        "print(\"Note: sklearn TF-IDF uses L2-normalization by default, so ||doc_vector||_2 ≈ 1.\\n\")\n",
        "\n",
        "for name in RUN_ORDER_TFIDF:\n",
        "    out = tfidf_results[name]\n",
        "    cfg = out[\"config\"]\n",
        "    s = out[\"stats\"]\n",
        "    norms = out[\"l2_norm_sample\"]\n",
        "    survivors = out[\"case_sensitive_stopword_survivors\"]\n",
        "\n",
        "    # Hard invariants\n",
        "    if s[\"n_docs\"] != len(texts):\n",
        "        raise RuntimeError(f\"Row mismatch in {name}: X has {s['n_docs']} rows but texts has {len(texts)} docs\")\n",
        "    if s[\"n_vocab\"] <= 0 or s[\"nnz\"] <= 0:\n",
        "        raise RuntimeError(f\"Degenerate TF-IDF matrix in {name}: n_vocab={s['n_vocab']}, nnz={s['nnz']}\")\n",
        "\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"Config: lowercase={cfg.lowercase}, stop_words={cfg.stop_words}\")\n",
        "    print(f\"Matrix type : {s['matrix_type']} (sparse)\")\n",
        "    print(f\"Dtype       : {s['dtype']} (TF-IDF weights are floating-point)\")\n",
        "    print(f\"Shape       : {s['shape']}  (#docs x #vocab)\")\n",
        "    print(f\"nnz         : {s['nnz']}\")\n",
        "    print(f\"Density     : {s['density']:.6f}\")\n",
        "    print(f\"Avg nnz/doc : {s['avg_nnz_per_doc']:.2f}\")\n",
        "    print(f\"L2-norm sample (min/mean/max): {norms['min']:.4f} / {norms['mean']:.4f} / {norms['max']:.4f}\")\n",
        "\n",
        "    if cfg.stop_words is not None and cfg.lowercase is False:\n",
        "        print(f\"Stopword casing nuance: {survivors} vocab terms look like stopwords after lowercasing (e.g., 'The')\")\n",
        "\n",
        "    print(f\"Vocabulary sample (first 20 terms): {feature_examples(out['vectorizer'], n=20)}\")\n",
        "    print(f\"Vocabulary sample with letters (first 20): {vocab_letter_sample(out['vectorizer'], n=20)}\")\n",
        "\n",
        "    # Show top TF-IDF terms for the first document as a concrete interpretability check\n",
        "    top_terms = top_terms_for_doc(out[\"X\"], out[\"vectorizer\"], doc_index=0, top_n=10)\n",
        "    print(\"Top TF-IDF terms (doc 0):\")\n",
        "    for term, val in top_terms:\n",
        "        print(f\" - {term:20s}  {val:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Summary table (nice for reporting)\n",
        "try:\n",
        "    import pandas as pd\n",
        "\n",
        "    rows = []\n",
        "    for name in RUN_ORDER_TFIDF:\n",
        "        out = tfidf_results[name]\n",
        "        cfg = out[\"config\"]\n",
        "        s = out[\"stats\"]\n",
        "        norms = out[\"l2_norm_sample\"]\n",
        "        rows.append(\n",
        "            {\n",
        "                \"run\": name,\n",
        "                \"lowercase\": cfg.lowercase,\n",
        "                \"stop_words\": cfg.stop_words,\n",
        "                \"N_docs\": s[\"n_docs\"],\n",
        "                \"V_vocab\": s[\"n_vocab\"],\n",
        "                \"nnz\": s[\"nnz\"],\n",
        "                \"density\": s[\"density\"],\n",
        "                \"avg_nnz_per_doc\": s[\"avg_nnz_per_doc\"],\n",
        "                \"l2_norm_mean(sample)\": norms[\"mean\"],\n",
        "                \"case_sensitive_stopword_survivors\": out[\"case_sensitive_stopword_survivors\"],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    tfidf_summary_df = pd.DataFrame(rows)\n",
        "    tfidf_summary_df[\"run\"] = pd.Categorical(tfidf_summary_df[\"run\"], categories=RUN_ORDER_TFIDF, ordered=True)\n",
        "    tfidf_summary_df = tfidf_summary_df.sort_values(by=[\"run\"]).reset_index(drop=True)\n",
        "    display(tfidf_summary_df)\n",
        "\n",
        "except Exception:\n",
        "    tfidf_summary_df = None\n",
        "\n",
        "# -------------------------\n",
        "# BoW vs TF-IDF comparison (interpretability-friendly, inside Section 5)\n",
        "# -------------------------\n",
        "# We compare (1) values meaning (counts vs weights) and (2) a concrete example on doc 0.\n",
        "\n",
        "def top_terms_bow_counts(X, vectorizer, doc_index: int, top_n: int = 10):\n",
        "    feats = vectorizer.get_feature_names_out()\n",
        "    row = X.getrow(doc_index)\n",
        "    if row.nnz == 0:\n",
        "        return []\n",
        "    order = np.argsort(row.data)[::-1][:top_n]\n",
        "    return [(feats[row.indices[i]], int(row.data[i])) for i in order]\n",
        "\n",
        "DEFAULT_TFIDF_RUN = \"tfidf_lc_on_sw_off\"\n",
        "tfidf_vectorizer = tfidf_results[DEFAULT_TFIDF_RUN][\"vectorizer\"]\n",
        "tfidf_paper_vectors = tfidf_results[DEFAULT_TFIDF_RUN][\"X\"]\n",
        "\n",
        "print(f\"[Section 5] Default TF-IDF run set for later use: {DEFAULT_TFIDF_RUN}\")\n",
        "\n",
        "# If you already computed BoW in Section 4, these variables exist:\n",
        "#   bow_vectorizer, bow_paper_vectors\n",
        "if \"bow_vectorizer\" in globals() and \"bow_paper_vectors\" in globals():\n",
        "    print(\"\\n[Section 5] BoW vs TF-IDF (doc 0 example)\")\n",
        "    bow_top = top_terms_bow_counts(bow_paper_vectors, bow_vectorizer, doc_index=0, top_n=10)\n",
        "    tfidf_top = top_terms_for_doc(tfidf_paper_vectors, tfidf_vectorizer, doc_index=0, top_n=10)\n",
        "\n",
        "    print(\"Top BoW terms by COUNT (doc 0):\")\n",
        "    for term, cnt in bow_top:\n",
        "        print(f\" - {term:20s}  {cnt}\")\n",
        "\n",
        "    print(\"\\nTop TF-IDF terms by WEIGHT (doc 0):\")\n",
        "    for term, w in tfidf_top:\n",
        "        print(f\" - {term:20s}  {w:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "from section4_bow import (\n",
        "    BoWConfig,\n",
        "    build_bow_vectors,\n",
        "    sparse_matrix_stats as bow_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as bow_case_sensitive_stopword_survivors,\n",
        ")\n",
        "\n",
        "from section5_tfidf import (\n",
        "    TfidfConfig,\n",
        "    build_tfidf_vectors,\n",
        "    sparse_matrix_stats as tfidf_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as tfidf_case_sensitive_stopword_survivors,\n",
        "    l2_norm_sample_stats,\n",
        "    top_terms_for_doc,\n",
        ")\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)  # should not raise\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "\n",
        "class TestSection4BoW(unittest.TestCase):\n",
        "    def test_build_bow_vectors_shape_and_sparse(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = BoWConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(hasattr(vect, \"vocabulary_\"))\n",
        "\n",
        "    def test_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_bow_vectors(texts, BoWConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_bow_vectors(texts, BoWConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_bow_vectors(texts, BoWConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_bow_vectors(texts, BoWConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_bow_vectors(texts, BoWConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = bow_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_sparse_stats_sane(self):\n",
        "        texts = [\"cat sat\", \"dog barked\"]\n",
        "        _, X = build_bow_vectors(texts, BoWConfig(name=\"s\", lowercase=True, stop_words=None))\n",
        "        s = bow_sparse_matrix_stats(X)\n",
        "        self.assertEqual(s[\"n_docs\"], 2)\n",
        "        self.assertGreater(s[\"n_vocab\"], 0)\n",
        "        self.assertGreaterEqual(s[\"density\"], 0.0)\n",
        "        self.assertLessEqual(s[\"density\"], 1.0)\n",
        "\n",
        "\n",
        "class TestSection5Tfidf(unittest.TestCase):\n",
        "    def test_build_tfidf_vectors_shape_sparse_and_float(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = TfidfConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_tfidf_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(\"float\" in str(X.dtype))\n",
        "\n",
        "        stats = tfidf_sparse_matrix_stats(X)\n",
        "        self.assertEqual(stats[\"n_docs\"], 3)\n",
        "        self.assertGreater(stats[\"n_vocab\"], 0)\n",
        "\n",
        "    def test_tfidf_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_tfidf_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_tfidf_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = tfidf_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_tfidf_l2_norms_are_about_one(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"n\", lowercase=True, stop_words=None))\n",
        "        norms = l2_norm_sample_stats(X, sample_n=3, seed=0)\n",
        "        self.assertAlmostEqual(norms[\"mean\"], 1.0, places=6)\n",
        "\n",
        "    def test_tfidf_idf_downweights_common_terms_when_tf_equal(self):\n",
        "        # common appears in all docs; rare appears in only one doc => rare should get higher weight (tf equal)\n",
        "        texts = [\"common rare1\", \"common rare2\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"idf\", lowercase=True, stop_words=None))\n",
        "        feats = vect.get_feature_names_out().tolist()\n",
        "\n",
        "        i_common = feats.index(\"common\")\n",
        "        i_rare1 = feats.index(\"rare1\")\n",
        "\n",
        "        row0 = X.getrow(0).toarray().ravel()\n",
        "        self.assertGreater(row0[i_rare1], row0[i_common])\n",
        "\n",
        "    def test_top_terms_for_doc_returns_sorted(self):\n",
        "        texts = [\"cat sat sat\", \"dog sat\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"top\", lowercase=True, stop_words=None))\n",
        "        top = top_terms_for_doc(X, vect, doc_index=0, top_n=5)\n",
        "        self.assertTrue(len(top) > 0)\n",
        "        # weights should be non-increasing\n",
        "        weights = [w for _, w in top]\n",
        "        self.assertTrue(all(weights[i] >= weights[i+1] for i in range(len(weights)-1)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_all.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_rejects_non_dict_records (test_all.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_all.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_all.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "test_build_bow_vectors_shape_and_sparse (test_all.TestSection4BoW.test_build_bow_vectors_shape_and_sparse) ... ok\n",
            "test_case_sensitive_stopword_survivors_detected (test_all.TestSection4BoW.test_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_lowercase_merges_tokens (test_all.TestSection4BoW.test_lowercase_merges_tokens) ... ok\n",
            "test_sparse_stats_sane (test_all.TestSection4BoW.test_sparse_stats_sane) ... ok\n",
            "test_stopwords_removed_when_lowercase_true (test_all.TestSection4BoW.test_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_build_tfidf_vectors_shape_sparse_and_float (test_all.TestSection5Tfidf.test_build_tfidf_vectors_shape_sparse_and_float) ... ok\n",
            "test_tfidf_case_sensitive_stopword_survivors_detected (test_all.TestSection5Tfidf.test_tfidf_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_tfidf_idf_downweights_common_terms_when_tf_equal (test_all.TestSection5Tfidf.test_tfidf_idf_downweights_common_terms_when_tf_equal) ... ok\n",
            "test_tfidf_l2_norms_are_about_one (test_all.TestSection5Tfidf.test_tfidf_l2_norms_are_about_one) ... ok\n",
            "test_tfidf_lowercase_merges_tokens (test_all.TestSection5Tfidf.test_tfidf_lowercase_merges_tokens) ... ok\n",
            "test_tfidf_stopwords_removed_when_lowercase_true (test_all.TestSection5Tfidf.test_tfidf_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_top_terms_for_doc_returns_sorted (test_all.TestSection5Tfidf.test_top_terms_for_doc_returns_sorted) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 0.018s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_all.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCXK65aHlq9Y"
      },
      "source": [
        "###Step 3: Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "O0xuwShjlur3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section6_queries.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section6_queries.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Sequence\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class QueryItem:\n",
        "    label: str\n",
        "    title: str\n",
        "    abstract: str\n",
        "    text: str\n",
        "\n",
        "\n",
        "def _as_text(x) -> str:\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    return x if isinstance(x, str) else str(x)\n",
        "\n",
        "\n",
        "def build_query_item(label: str, title: str, abstract: str, joiner: str = \" \") -> QueryItem:\n",
        "    \"\"\"\n",
        "    Query construction MUST match dataset documents (Section 3):\n",
        "        text = title + \" \" + abstract\n",
        "    No extra cleaning here.\n",
        "    \"\"\"\n",
        "    if not isinstance(joiner, str) or joiner != \" \":\n",
        "        raise ValueError('joiner must be exactly one space: \" \"')\n",
        "\n",
        "    t = _as_text(title)\n",
        "    a = _as_text(abstract)\n",
        "\n",
        "    if len(t.strip()) == 0:\n",
        "        raise ValueError(f\"{label}: title is empty\")\n",
        "    if len(a.strip()) == 0:\n",
        "        raise ValueError(f\"{label}: abstract is empty\")\n",
        "\n",
        "    text = t + joiner + a\n",
        "    return QueryItem(label=label, title=t, abstract=a, text=text)\n",
        "\n",
        "\n",
        "def get_default_queries(joiner: str = \" \") -> List[QueryItem]:\n",
        "    \"\"\"\n",
        "    The three queries EXACTLY as provided by the assignment (title + abstract).\n",
        "    Stored in fixed order: Query 1, Query 2, Query 3.\n",
        "    \"\"\"\n",
        "    q1_title = \"QUALES: Machine translation quality estimation via supervised and unsupervised machine learning.\"\n",
        "    q1_abstract = (\n",
        "        \"The automatic quality estimation (QE) of machine translation consists in measuring the quality of translations \"\n",
        "        \"without access to human references, usually via machine learning approaches. A good QE system can help in three \"\n",
        "        \"aspects of translation processes involving machine translation and post-editing: increasing productivity (by ruling \"\n",
        "        \"out poor quality machine translation), estimating costs (by helping to forecast the cost of post-editing) and selecting \"\n",
        "        \"a provider (if several machine translation systems are available). Interest in this research area has grown significantly \"\n",
        "        \"in recent years, leading to regular shared tasks in the main machine translation conferences and intense scientific activity. \"\n",
        "        \"In this article we review the state of the art in this research area and present project QUALES, which is under development.\"\n",
        "    )\n",
        "\n",
        "    q2_title = \"Learning to Ask Unanswerable Questions for Machine Reading Comprehension\"\n",
        "    q2_abstract = (\n",
        "        \"Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation \"\n",
        "        \"technique by automatically generating relevant unanswerable questions according to an answerable question paired with its corresponding \"\n",
        "        \"paragraph that contains the answer. We introduce a pair-to-sequence model for unanswerable question generation, which effectively captures \"\n",
        "        \"the interactions between the question and the paragraph. We also present a way to construct training data for our question generation models \"\n",
        "        \"by leveraging the existing reading comprehension dataset. Experimental results show that the pair-to-sequence model performs consistently better \"\n",
        "        \"compared with the sequence-to-sequence baseline. We further use the automatically generated unanswerable questions as a means of data augmentation \"\n",
        "        \"on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model.\"\n",
        "    )\n",
        "\n",
        "    q3_title = \"Unsupervised Neural Text Simplification\"\n",
        "    q3_abstract = (\n",
        "        \"The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabelled text corpora. \"\n",
        "        \"The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based losses and denoising. \"\n",
        "        \"The framework is trained using unlabelled text collected from en-Wikipedia dump. Our analysis (both quantitative and qualitative involving human evaluators) \"\n",
        "        \"on public test data shows that the proposed model can perform text-simplification at both lexical and syntactic levels, competitive to existing supervised methods. \"\n",
        "        \"It also outperforms viable unsupervised baselines. Adding a few labelled pairs helps improve the performance further.\"\n",
        "    )\n",
        "\n",
        "    queries = [\n",
        "        build_query_item(\"Query 1\", q1_title, q1_abstract, joiner=joiner),\n",
        "        build_query_item(\"Query 2\", q2_title, q2_abstract, joiner=joiner),\n",
        "        build_query_item(\"Query 3\", q3_title, q3_abstract, joiner=joiner),\n",
        "    ]\n",
        "    return queries\n",
        "\n",
        "\n",
        "def get_query_texts(queries: Sequence[QueryItem]) -> List[str]:\n",
        "    return [q.text for q in queries]\n",
        "\n",
        "\n",
        "def validate_queries(queries: Sequence[QueryItem]) -> None:\n",
        "    if not isinstance(queries, (list, tuple)):\n",
        "        raise TypeError(f\"queries must be a list/tuple, got {type(queries)}\")\n",
        "    if len(queries) != 3:\n",
        "        raise AssertionError(f\"Expected exactly 3 queries, got {len(queries)}\")\n",
        "\n",
        "    expected_labels = [\"Query 1\", \"Query 2\", \"Query 3\"]\n",
        "    got_labels = [q.label for q in queries]\n",
        "    if got_labels != expected_labels:\n",
        "        raise AssertionError(f\"Query order/labels must be {expected_labels}, got {got_labels}\")\n",
        "\n",
        "    for q in queries:\n",
        "        if any(not isinstance(x, str) for x in [q.label, q.title, q.abstract, q.text]):\n",
        "            raise AssertionError(\"All QueryItem fields must be strings.\")\n",
        "        if len(q.title.strip()) == 0 or len(q.abstract.strip()) == 0 or len(q.text.strip()) == 0:\n",
        "            raise AssertionError(f\"{q.label} has empty fields.\")\n",
        "\n",
        "        # Strict pipeline check: text must be EXACTLY title + \" \" + abstract\n",
        "        if q.text != (q.title + \" \" + q.abstract):\n",
        "            raise AssertionError(f\"{q.label}: text is not exactly title + ' ' + abstract\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 6] Queries ready.\n",
            " - N queries     : 3\n",
            " - Order/labels  : ['Query 1', 'Query 2', 'Query 3']\n",
            "\n",
            "[Section 6] Preview (first 300 chars each):\n",
            "\n",
            "Query 1: QUALES: Machine translation quality estimation via supervised and unsupervised machine learning.\n",
            "QUALES: Machine translation quality estimation via supervised and unsupervised machine learning. The automatic quality estimation (QE) of machine translation consists in measuring the quality of translations without access to human references, usually via machine learning approaches. A good QE syste...\n",
            "\n",
            "Query 2: Learning to Ask Unanswerable Questions for Machine Reading Comprehension\n",
            "Learning to Ask Unanswerable Questions for Machine Reading Comprehension Machine reading comprehension with unanswerable questions is a challenging task. In this work, we propose a data augmentation technique by automatically generating relevant unanswerable questions according to an answerable ques...\n",
            "\n",
            "Query 3: Unsupervised Neural Text Simplification\n",
            "Unsupervised Neural Text Simplification The paper presents a first attempt towards unsupervised neural text simplification that relies only on unlabelled text corpora. The core framework is composed of a shared encoder and a pair of attentional-decoders, crucially assisted by discrimination-based lo...\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 6 — Query Set Definition (Part 1 – Step 3 Inputs)\n",
        "# =========================\n",
        "\n",
        "from section6_queries import get_default_queries, get_query_texts, validate_queries\n",
        "\n",
        "queries = get_default_queries(joiner=\" \")   # MUST be exactly one space\n",
        "validate_queries(queries)\n",
        "\n",
        "query_texts = get_query_texts(queries)      # list[str] in fixed order (Query 1, 2, 3)\n",
        "query_labels = [q.label for q in queries]   # [\"Query 1\", \"Query 2\", \"Query 3\"]\n",
        "\n",
        "print(\"[Section 6] Queries ready.\")\n",
        "print(f\" - N queries     : {len(queries)}\")\n",
        "print(f\" - Order/labels  : {query_labels}\")\n",
        "print(\"\\n[Section 6] Preview (first 300 chars each):\")\n",
        "for q in queries:\n",
        "    print(f\"\\n{q.label}: {q.title}\")\n",
        "    print(q.text[:300] + (\"...\" if len(q.text) > 300 else \"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "from section4_bow import (\n",
        "    BoWConfig,\n",
        "    build_bow_vectors,\n",
        "    sparse_matrix_stats as bow_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as bow_case_sensitive_stopword_survivors,\n",
        ")\n",
        "\n",
        "from section5_tfidf import (\n",
        "    TfidfConfig,\n",
        "    build_tfidf_vectors,\n",
        "    sparse_matrix_stats as tfidf_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as tfidf_case_sensitive_stopword_survivors,\n",
        "    l2_norm_sample_stats,\n",
        "    top_terms_for_doc,\n",
        ")\n",
        "\n",
        "from section6_queries import (\n",
        "    QueryItem,\n",
        "    build_query_item,\n",
        "    get_default_queries,\n",
        "    get_query_texts,\n",
        "    validate_queries,\n",
        ")\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "\n",
        "class TestSection4BoW(unittest.TestCase):\n",
        "    def test_build_bow_vectors_shape_and_sparse(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = BoWConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(hasattr(vect, \"vocabulary_\"))\n",
        "\n",
        "    def test_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_bow_vectors(texts, BoWConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_bow_vectors(texts, BoWConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_bow_vectors(texts, BoWConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_bow_vectors(texts, BoWConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_bow_vectors(texts, BoWConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = bow_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_sparse_stats_sane(self):\n",
        "        texts = [\"cat sat\", \"dog barked\"]\n",
        "        _, X = build_bow_vectors(texts, BoWConfig(name=\"s\", lowercase=True, stop_words=None))\n",
        "        s = bow_sparse_matrix_stats(X)\n",
        "        self.assertEqual(s[\"n_docs\"], 2)\n",
        "        self.assertGreater(s[\"n_vocab\"], 0)\n",
        "        self.assertGreaterEqual(s[\"density\"], 0.0)\n",
        "        self.assertLessEqual(s[\"density\"], 1.0)\n",
        "\n",
        "\n",
        "class TestSection5Tfidf(unittest.TestCase):\n",
        "    def test_build_tfidf_vectors_shape_sparse_and_float(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = TfidfConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_tfidf_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(\"float\" in str(X.dtype))\n",
        "\n",
        "        stats = tfidf_sparse_matrix_stats(X)\n",
        "        self.assertEqual(stats[\"n_docs\"], 3)\n",
        "        self.assertGreater(stats[\"n_vocab\"], 0)\n",
        "\n",
        "    def test_tfidf_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_tfidf_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_tfidf_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = tfidf_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_tfidf_l2_norms_are_about_one(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        _, X = build_tfidf_vectors(texts, TfidfConfig(name=\"n\", lowercase=True, stop_words=None))\n",
        "        norms = l2_norm_sample_stats(X, sample_n=3, seed=0)\n",
        "        self.assertAlmostEqual(norms[\"mean\"], 1.0, places=6)\n",
        "\n",
        "    def test_tfidf_idf_downweights_common_terms_when_tf_equal(self):\n",
        "        texts = [\"common rare1\", \"common rare2\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"idf\", lowercase=True, stop_words=None))\n",
        "        feats = vect.get_feature_names_out().tolist()\n",
        "\n",
        "        i_common = feats.index(\"common\")\n",
        "        i_rare1 = feats.index(\"rare1\")\n",
        "\n",
        "        row0 = X.getrow(0).toarray().ravel()\n",
        "        self.assertGreater(row0[i_rare1], row0[i_common])\n",
        "\n",
        "    def test_top_terms_for_doc_returns_sorted(self):\n",
        "        texts = [\"cat sat sat\", \"dog sat\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"top\", lowercase=True, stop_words=None))\n",
        "        top = top_terms_for_doc(X, vect, doc_index=0, top_n=5)\n",
        "        self.assertTrue(len(top) > 0)\n",
        "        weights = [w for _, w in top]\n",
        "        self.assertTrue(all(weights[i] >= weights[i + 1] for i in range(len(weights) - 1)))\n",
        "\n",
        "\n",
        "class TestSection6Queries(unittest.TestCase):\n",
        "    def test_default_queries_exist_and_valid(self):\n",
        "        queries = get_default_queries(joiner=\" \")\n",
        "        validate_queries(queries)  # should not raise\n",
        "        self.assertEqual(len(queries), 3)\n",
        "        self.assertEqual([q.label for q in queries], [\"Query 1\", \"Query 2\", \"Query 3\"])\n",
        "        self.assertTrue(all(isinstance(q, QueryItem) for q in queries))\n",
        "\n",
        "    def test_query_texts_are_in_order(self):\n",
        "        queries = get_default_queries(joiner=\" \")\n",
        "        texts = get_query_texts(queries)\n",
        "        self.assertEqual(len(texts), 3)\n",
        "        self.assertTrue(all(isinstance(t, str) and len(t) > 0 for t in texts))\n",
        "\n",
        "    def test_build_query_item_strict_joiner(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            build_query_item(\"Query 1\", \"T\", \"A\", joiner=\"  \")  # not exactly one space\n",
        "        with self.assertRaises(ValueError):\n",
        "            build_query_item(\"Query 1\", \"T\", \"A\", joiner=\"\")    # not exactly one space\n",
        "\n",
        "    def test_validate_queries_rejects_wrong_order(self):\n",
        "        q1 = build_query_item(\"Query 1\", \"T1\", \"A1\", joiner=\" \")\n",
        "        q2 = build_query_item(\"Query 2\", \"T2\", \"A2\", joiner=\" \")\n",
        "        q3 = build_query_item(\"Query 3\", \"T3\", \"A3\", joiner=\" \")\n",
        "        with self.assertRaises(AssertionError):\n",
        "            validate_queries([q2, q1, q3])  # wrong order/labels\n",
        "\n",
        "    def test_validate_queries_rejects_bad_text_pipeline(self):\n",
        "        bad = QueryItem(label=\"Query 1\", title=\"T\", abstract=\"A\", text=\"T  A\")\n",
        "        q2 = build_query_item(\"Query 2\", \"T2\", \"A2\", joiner=\" \")\n",
        "        q3 = build_query_item(\"Query 3\", \"T3\", \"A3\", joiner=\" \")\n",
        "        with self.assertRaises(AssertionError):\n",
        "            validate_queries([bad, q2, q3])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_all.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_rejects_non_dict_records (test_all.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_all.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_all.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "test_build_bow_vectors_shape_and_sparse (test_all.TestSection4BoW.test_build_bow_vectors_shape_and_sparse) ... ok\n",
            "test_case_sensitive_stopword_survivors_detected (test_all.TestSection4BoW.test_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_lowercase_merges_tokens (test_all.TestSection4BoW.test_lowercase_merges_tokens) ... ok\n",
            "test_sparse_stats_sane (test_all.TestSection4BoW.test_sparse_stats_sane) ... ok\n",
            "test_stopwords_removed_when_lowercase_true (test_all.TestSection4BoW.test_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_build_tfidf_vectors_shape_sparse_and_float (test_all.TestSection5Tfidf.test_build_tfidf_vectors_shape_sparse_and_float) ... ok\n",
            "test_tfidf_case_sensitive_stopword_survivors_detected (test_all.TestSection5Tfidf.test_tfidf_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_tfidf_idf_downweights_common_terms_when_tf_equal (test_all.TestSection5Tfidf.test_tfidf_idf_downweights_common_terms_when_tf_equal) ... ok\n",
            "test_tfidf_l2_norms_are_about_one (test_all.TestSection5Tfidf.test_tfidf_l2_norms_are_about_one) ... ok\n",
            "test_tfidf_lowercase_merges_tokens (test_all.TestSection5Tfidf.test_tfidf_lowercase_merges_tokens) ... ok\n",
            "test_tfidf_stopwords_removed_when_lowercase_true (test_all.TestSection5Tfidf.test_tfidf_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_top_terms_for_doc_returns_sorted (test_all.TestSection5Tfidf.test_top_terms_for_doc_returns_sorted) ... ok\n",
            "test_build_query_item_strict_joiner (test_all.TestSection6Queries.test_build_query_item_strict_joiner) ... ok\n",
            "test_default_queries_exist_and_valid (test_all.TestSection6Queries.test_default_queries_exist_and_valid) ... ok\n",
            "test_query_texts_are_in_order (test_all.TestSection6Queries.test_query_texts_are_in_order) ... ok\n",
            "test_validate_queries_rejects_bad_text_pipeline (test_all.TestSection6Queries.test_validate_queries_rejects_bad_text_pipeline) ... ok\n",
            "test_validate_queries_rejects_wrong_order (test_all.TestSection6Queries.test_validate_queries_rejects_wrong_order) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 21 tests in 0.018s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_all.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting section7_similarity.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section7_similarity.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Sequence\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class TopKResult:\n",
        "    query_index: int\n",
        "    top_indices: List[int]\n",
        "    top_scores: List[float]\n",
        "\n",
        "\n",
        "def compute_similarity_matrix(query_vectors, paper_vectors) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns a dense similarity matrix of shape (n_queries, n_papers).\n",
        "    Works with sparse inputs too (sklearn handles it).\n",
        "    \"\"\"\n",
        "    if getattr(query_vectors, \"shape\", None) is None or getattr(paper_vectors, \"shape\", None) is None:\n",
        "        raise TypeError(\"query_vectors and paper_vectors must expose a .shape attribute\")\n",
        "\n",
        "    if query_vectors.shape[1] != paper_vectors.shape[1]:\n",
        "        raise ValueError(\n",
        "            f\"Feature dimension mismatch: query_vectors has {query_vectors.shape[1]} features, \"\n",
        "            f\"paper_vectors has {paper_vectors.shape[1]} features.\"\n",
        "        )\n",
        "\n",
        "    return cosine_similarity(query_vectors, paper_vectors)\n",
        "\n",
        "\n",
        "def top_k_indices(scores: Sequence[float], k: int = 3) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Deterministic top-k indices by:\n",
        "      1) higher score first\n",
        "      2) lower index first (tie-break)\n",
        "    \"\"\"\n",
        "    if not isinstance(k, int) or k <= 0:\n",
        "        raise ValueError(\"k must be a positive integer\")\n",
        "\n",
        "    s = np.asarray(scores, dtype=float).ravel()\n",
        "    if s.size == 0:\n",
        "        raise ValueError(\"scores is empty\")\n",
        "\n",
        "    k = min(k, s.size)\n",
        "    idx = np.arange(s.size)\n",
        "    order = np.lexsort((idx, -s))  # primary: -score asc => score desc, tie: idx asc\n",
        "    return order[:k]\n",
        "\n",
        "\n",
        "def top_k_for_all_queries(similarity_matrix: np.ndarray, k: int = 3) -> List[TopKResult]:\n",
        "    \"\"\"\n",
        "    For each query (row) in similarity_matrix, return a TopKResult with:\n",
        "      - query_index\n",
        "      - top_indices (length k, or <=k if fewer papers)\n",
        "      - top_scores  (aligned with top_indices)\n",
        "\n",
        "    Includes extra debug prints ONLY if something crashes inside this function.\n",
        "    \"\"\"\n",
        "    if getattr(similarity_matrix, \"ndim\", None) is None:\n",
        "        raise TypeError(\"similarity_matrix must be array-like and expose .ndim/.shape\")\n",
        "\n",
        "    if similarity_matrix.ndim != 2:\n",
        "        raise ValueError(\"similarity_matrix must be 2D\")\n",
        "\n",
        "    if not isinstance(k, int) or k <= 0:\n",
        "        raise ValueError(\"k must be a positive integer\")\n",
        "\n",
        "    n_queries, n_papers = similarity_matrix.shape\n",
        "    results: List[TopKResult] = []\n",
        "\n",
        "    for qi in range(n_queries):\n",
        "        try:\n",
        "            # Force 1D numeric row even if input is np.matrix or weird array-like\n",
        "            scores = np.asarray(similarity_matrix[qi], dtype=float).ravel()\n",
        "\n",
        "            if scores.size != n_papers:\n",
        "                raise ValueError(\n",
        "                    f\"Row size mismatch at query {qi}: got {scores.size} scores, expected {n_papers}\"\n",
        "                )\n",
        "\n",
        "            top_idx = top_k_indices(scores, k=k)\n",
        "            top_scores = [float(scores[j]) for j in top_idx]\n",
        "\n",
        "            results.append(\n",
        "                TopKResult(\n",
        "                    query_index=int(qi),\n",
        "                    top_indices=[int(x) for x in top_idx.tolist()],\n",
        "                    top_scores=top_scores,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            # ---- DEBUG (only when something actually fails) ----\n",
        "            print(\"\\n[section7_similarity] ERROR inside top_k_for_all_queries()\", file=sys.stderr)\n",
        "            print(f\"  - Exception: {type(e).__name__}: {e}\", file=sys.stderr)\n",
        "            print(f\"  - similarity_matrix type : {type(similarity_matrix)}\", file=sys.stderr)\n",
        "            print(f\"  - similarity_matrix shape: {getattr(similarity_matrix, 'shape', None)}\", file=sys.stderr)\n",
        "            print(f\"  - similarity_matrix dtype: {getattr(similarity_matrix, 'dtype', None)}\", file=sys.stderr)\n",
        "            print(f\"  - qi (row index)         : {qi}\", file=sys.stderr)\n",
        "            try:\n",
        "                sample = scores[:10].tolist()  # may fail if scores wasn't created\n",
        "                print(f\"  - scores sample (first 10): {sample}\", file=sys.stderr)\n",
        "            except Exception:\n",
        "                print(\"  - scores sample (first 10): <unavailable>\", file=sys.stderr)\n",
        "            print(\"------------------------------------------------------------\\n\", file=sys.stderr)\n",
        "            # ---------------------------------------------\n",
        "            raise\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def validate_text_list(texts: Sequence[str], name: str) -> None:\n",
        "    if not isinstance(texts, (list, tuple)):\n",
        "        raise TypeError(f\"{name} must be a list/tuple of strings\")\n",
        "    if len(texts) == 0:\n",
        "        raise ValueError(f\"{name} is empty\")\n",
        "    if any(not isinstance(t, str) for t in texts):\n",
        "        raise TypeError(f\"{name} must contain only strings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwFOkv1syVnM"
      },
      "source": [
        "Use the following code snipet to compute the pairwise cosine similarity, and to sort the top 3 candidates for each query. In this example, `query_vectors` and `paper_vectors` correspond to the vectorized collections of queries and papers (as stored in the JSON file previously downloaded), respectively. The variable `paper_texts` contain the list of concatenated titles+abstracts of the papers loaded from the JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_OlJS4Ubx7IF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "############################\n",
            "### BoW similarity search ###\n",
            "############################\n",
            "\n",
            "====================================================================================================\n",
            "[bow_lc_on_sw_off] lowercase=True, stop_words=None\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'Neural Machine Translation with Source Dependency Representation We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.' (Score: 0.6091)\n",
            "2. Text: 'HUME: Human UCCA-Based Evaluation of Machine Translation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.5913)\n",
            "3. Text: 'Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.5678)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'A Nil-Aware Answer Extraction Framework for Question Answering In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defines the task as answering a question from multiple passages and the words in the answer are not necessary in the passages. We therefore develop an extraction-then-synthesis framework to synthesize answers from extraction results. Specifically, the answer extraction model is first employed to predict the most important sub-spans from the passage as evidence, and the answer synthesis model takes the evidence as additional features along with the question and passage to further elaborate the final answers. We build the answer extraction model with state-of-the-art neural networks for single passage reading comprehension, and propose an additional task of passage ranking to help answer extraction in multiple passages. The answer synthesis model is based on the sequence-to-sequence neural networks with extracted evidences as features. Experiments show that our extraction-then-synthesis method outperforms state-of-the-art methods.' (Score: 0.6452)\n",
            "2. Text: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, instead, we endeavor to generate longer and semantic-rich distractors which are closer to distractors in real reading comprehension from examinations. Taking a reading comprehension article, a pair of question and its correct option as input, our goal is to generate several distractors which are somehow related to the answer, consistent with the semantic context of the question and have some trace in the article. We propose a hierarchical encoder-decoder framework with static and dynamic attention mechanisms to tackle this task. Specifically, the dynamic attention can combine sentence-level and word-level attention varying at each recurrent time step to generate a more readable sequence. The static attention is to modulate the dynamic attention not to focus on question irrelevant sentences or sentences which contribute to the correct option. Our proposed framework outperforms several strong baselines on the first prepared distractor generation dataset of real reading comprehension questions. For human evaluation, compared with those distractors generated by baselines, our generated distractors are more functional to confuse the annotators.' (Score: 0.6210)\n",
            "3. Text: 'Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established models in this task. However, in languages other than English, there has been little exploration in this direction. Both the scarcity of annotated data and the complexity of the language increase the difficulty of the problem. To address these challenges, we use a sequence-to-sequence model with character-based attention, which in addition to its self-learned character embeddings, uses word embeddings pre-trained with an approach that also models subword information. This provides the neural model with access to more linguistic information especially suitable for text normalization, without large parallel corpora. We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.' (Score: 0.6131)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.6132)\n",
            "2. Text: 'Learning Unsupervised Word Translations Without Adversaries Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.' (Score: 0.5508)\n",
            "3. Text: 'Learning Robust Representations of Text Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this work, we present a new system for scene text detection by proposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/nontext information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates main task of text/non-text classification. In addition, a powerful low-level detector called Contrast- Enhancement Maximally Stable Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 dataset, with a F-measure of 0.82, improving the state-of-the-art results substantially.' (Score: 0.5206)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "[bow_lc_on_sw_on] lowercase=True, stop_words=english\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'Memory-augmented Neural Machine Translation We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.' (Score: 0.5237)\n",
            "2. Text: 'Semi-Autoregressive Neural Machine Translation We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.' (Score: 0.5237)\n",
            "3. Text: 'Prediction Improves Simultaneous Neural Machine Translation We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.' (Score: 0.5146)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model from the SQuAD dataset on the challenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single model and 46.6% with an ensemble, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%, without use of provided annotations.' (Score: 0.3747)\n",
            "2. Text: 'A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset.' (Score: 0.3712)\n",
            "3. Text: 'Answer-focused and Position-aware Neural Question Generation Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions include words in the question target, resulting in the generation of unintended questions. In this paper, we propose answer-separated seq2seq, which better utilizes the information from both the passage and the target answer. By replacing the target answer in the original passage with a special token, our model learns to identify which interrogative word should be used. We also propose a new module termed keyword-net, which helps the model better capture the key information in the target answer and generate an appropriate question. Experimental results demonstrate that our answer separation method significantly reduces the number of improper questions which include answers. Consequently, our model significantly outperforms previous state-of-the-art NQG models.' (Score: 0.3527)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.4738)\n",
            "2. Text: 'Learning Robust Representations of Text Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature computed globally from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this work, we present a new system for scene text detection by proposing a novel Text-Attentional Convolutional Neural Network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/nontext information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates main task of text/non-text classification. In addition, a powerful low-level detector called Contrast- Enhancement Maximally Stable Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 dataset, with a F-measure of 0.82, improving the state-of-the-art results substantially.' (Score: 0.3688)\n",
            "3. Text: 'Learning Unsupervised Word Translations Without Adversaries Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.' (Score: 0.3639)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "[bow_lc_off_sw_off] lowercase=False, stop_words=None\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'Neural Machine Translation with Source Dependency Representation We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.' (Score: 0.5862)\n",
            "2. Text: 'HUME: Human UCCA-Based Evaluation of Machine Translation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.5487)\n",
            "3. Text: 'The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation The advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. In this paper, the efficiencies of translation using bidirectional encoder attention decoder models were studied with respect to translation involving morphologically rich languages. The English - Tamil language pair was selected for this analysis. First, the use of Word2Vec embedding for both the English and Tamil words improved the translation results by 0.73 BLEU points over the baseline RNNSearch model with 4.84 BLEU score. The use of morphological segmentation before word vectorization to split the morphologically rich Tamil words into their respective morphemes before the translation, caused a reduction in the target vocabulary size by a factor of 8. Also, this model (RNNMorph) improved the performance of neural machine translation by 7.05 BLEU points over the RNNSearch model used over the same corpus. Since the BLEU evaluation of the RNNMorph model might be unreliable due to an increase in the number of matching tokens per sentence, the performances of the translations were also compared by means of human evaluation metrics of adequacy, fluency and relative ranking. Further, the use of morphological segmentation also improved the efficacy of the attention mechanism.' (Score: 0.5378)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'A Nil-Aware Answer Extraction Framework for Question Answering In this paper, we present a novel approach to machine reading comprehension for the MS-MARCO dataset. Unlike the SQuAD dataset that aims to answer a question with exact text spans in a passage, the MS-MARCO dataset defines the task as answering a question from multiple passages and the words in the answer are not necessary in the passages. We therefore develop an extraction-then-synthesis framework to synthesize answers from extraction results. Specifically, the answer extraction model is first employed to predict the most important sub-spans from the passage as evidence, and the answer synthesis model takes the evidence as additional features along with the question and passage to further elaborate the final answers. We build the answer extraction model with state-of-the-art neural networks for single passage reading comprehension, and propose an additional task of passage ranking to help answer extraction in multiple passages. The answer synthesis model is based on the sequence-to-sequence neural networks with extracted evidences as features. Experiments show that our extraction-then-synthesis method outperforms state-of-the-art methods.' (Score: 0.6435)\n",
            "2. Text: 'Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established models in this task. However, in languages other than English, there has been little exploration in this direction. Both the scarcity of annotated data and the complexity of the language increase the difficulty of the problem. To address these challenges, we use a sequence-to-sequence model with character-based attention, which in addition to its self-learned character embeddings, uses word embeddings pre-trained with an approach that also models subword information. This provides the neural model with access to more linguistic information especially suitable for text normalization, without large parallel corpora. We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.' (Score: 0.6039)\n",
            "3. Text: 'Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation Most of the Neural Machine Translation (NMT) models are based on the sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped with the attention mechanism. However, the conventional attention mechanism treats the decoding at each time step equally with the same matrix, which is problematic since the softness of the attention for different types of words (e.g. content words and function words) should differ. Therefore, we propose a new model with a mechanism called Self-Adaptive Control of Temperature (SACT) to control the softness of attention by means of an attention temperature. Experimental results on the Chinese-English translation and English-Vietnamese translation demonstrate that our model outperforms the baseline models, and the analysis and the case study show that our model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality.' (Score: 0.6008)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.4768)\n",
            "2. Text: 'Learning Unsupervised Word Translations Without Adversaries Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.' (Score: 0.4735)\n",
            "3. Text: 'A Joint Sequential and Relational Model for Frame-Semantic Parsing We propose a framework for parsing video and text jointly for understanding events and answering user queries. Our framework produces a parse graph that represents the compositional structures of spatial information (objects and scenes), temporal information (actions and events) and causal information (causalities between events and fluents) in the video and text. The knowledge representation of our framework is based on a spatial-temporal-causal And-Or graph (S/T/C-AOG), which jointly models possible hierarchical compositions of objects, scenes and events as well as their interactions and mutual contexts, and specifies the prior probabilistic distribution of the parse graphs. We present a probabilistic generative model for joint parsing that captures the relations between the input video/text, their corresponding parse graphs and the joint parse graph. Based on the probabilistic model, we propose a joint parsing system consisting of three modules: video parsing, text parsing and joint inference. Video parsing and text parsing produce two parse graphs from the input video and text respectively. The joint inference module produces a joint parse graph by performing matching, deduction and revision on the video and text parse graphs. The proposed framework has the following objectives: Firstly, we aim at deep semantic parsing of video and text that goes beyond the traditional bag-of-words approaches; Secondly, we perform parsing and reasoning across the spatial, temporal and causal dimensions based on the joint S/T/C-AOG representation; Thirdly, we show that deep joint parsing facilitates subsequent applications such as generating narrative text descriptions and answering queries in the forms of who, what, when, where and why. We empirically evaluated our system based on comparison against ground-truth as well as accuracy of query answering and obtained satisfactory results.' (Score: 0.4619)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "[bow_lc_off_sw_on] lowercase=False, stop_words=english\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'Prediction Improves Simultaneous Neural Machine Translation We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.' (Score: 0.4771)\n",
            "2. Text: 'HUME: Human UCCA-Based Evaluation of Machine Translation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.4675)\n",
            "3. Text: 'Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.4557)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model from the SQuAD dataset on the challenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single model and 46.6% with an ensemble, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%, without use of provided annotations.' (Score: 0.3711)\n",
            "2. Text: 'Answer-focused and Position-aware Neural Question Generation Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions include words in the question target, resulting in the generation of unintended questions. In this paper, we propose answer-separated seq2seq, which better utilizes the information from both the passage and the target answer. By replacing the target answer in the original passage with a special token, our model learns to identify which interrogative word should be used. We also propose a new module termed keyword-net, which helps the model better capture the key information in the target answer and generate an appropriate question. Experimental results demonstrate that our answer separation method significantly reduces the number of improper questions which include answers. Consequently, our model significantly outperforms previous state-of-the-art NQG models.' (Score: 0.3472)\n",
            "3. Text: 'A dataset and baselines for sequential open-domain question answering We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million question-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.' (Score: 0.3243)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.3821)\n",
            "2. Text: 'Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation Existing text generation methods tend to produce repeated and \"boring\" expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for \"novel\" and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines. The code is available at https://github.com/lancopku/DPGAN' (Score: 0.3596)\n",
            "3. Text: 'Learning Unsupervised Word Translations Without Adversaries Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.' (Score: 0.3400)\n",
            "\n",
            "\n",
            "\n",
            "#############################\n",
            "### TF-IDF similarity search ###\n",
            "#############################\n",
            "\n",
            "====================================================================================================\n",
            "[tfidf_lc_on_sw_off] lowercase=True, stop_words=None\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'Memory-augmented Neural Machine Translation We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.' (Score: 0.3227)\n",
            "2. Text: 'HUME: Human UCCA-Based Evaluation of Machine Translation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.3214)\n",
            "3. Text: 'Semi-Autoregressive Neural Machine Translation We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.' (Score: 0.3185)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset.' (Score: 0.3714)\n",
            "2. Text: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, instead, we endeavor to generate longer and semantic-rich distractors which are closer to distractors in real reading comprehension from examinations. Taking a reading comprehension article, a pair of question and its correct option as input, our goal is to generate several distractors which are somehow related to the answer, consistent with the semantic context of the question and have some trace in the article. We propose a hierarchical encoder-decoder framework with static and dynamic attention mechanisms to tackle this task. Specifically, the dynamic attention can combine sentence-level and word-level attention varying at each recurrent time step to generate a more readable sequence. The static attention is to modulate the dynamic attention not to focus on question irrelevant sentences or sentences which contribute to the correct option. Our proposed framework outperforms several strong baselines on the first prepared distractor generation dataset of real reading comprehension questions. For human evaluation, compared with those distractors generated by baselines, our generated distractors are more functional to confuse the annotators.' (Score: 0.3146)\n",
            "3. Text: 'SQuAD: 100,000+ Questions for Machine Comprehension of Text We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.' (Score: 0.2975)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.3190)\n",
            "2. Text: 'Integrating Transformer and Paraphrase Rules for Sentence Simplification Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned simplification mapping rules from normal- simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we pro- pose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state- of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text_simplification.' (Score: 0.2951)\n",
            "3. Text: 'Sentence Simplification with Deep Reinforcement Learning Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call {\\sc Dress} (as shorthand for {\\bf D}eep {\\bf RE}inforcement {\\bf S}entence {\\bf S}implification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.' (Score: 0.2336)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "[tfidf_lc_on_sw_on] lowercase=True, stop_words=english\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'Memory-augmented Neural Machine Translation We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.' (Score: 0.3443)\n",
            "2. Text: 'Semi-Autoregressive Neural Machine Translation We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.' (Score: 0.3393)\n",
            "3. Text: 'HUME: Human UCCA-Based Evaluation of Machine Translation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.3149)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset.' (Score: 0.3661)\n",
            "2. Text: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, instead, we endeavor to generate longer and semantic-rich distractors which are closer to distractors in real reading comprehension from examinations. Taking a reading comprehension article, a pair of question and its correct option as input, our goal is to generate several distractors which are somehow related to the answer, consistent with the semantic context of the question and have some trace in the article. We propose a hierarchical encoder-decoder framework with static and dynamic attention mechanisms to tackle this task. Specifically, the dynamic attention can combine sentence-level and word-level attention varying at each recurrent time step to generate a more readable sequence. The static attention is to modulate the dynamic attention not to focus on question irrelevant sentences or sentences which contribute to the correct option. Our proposed framework outperforms several strong baselines on the first prepared distractor generation dataset of real reading comprehension questions. For human evaluation, compared with those distractors generated by baselines, our generated distractors are more functional to confuse the annotators.' (Score: 0.2770)\n",
            "3. Text: 'Interpretation of Natural Language Rules in Conversational Machine Reading Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader's background knowledge. One example is the task of interpreting regulations to answer \"Can I...?\" or \"Do I have to...?\" questions such as \"I am working in Canada. Do I have to carry on paying UK National Insurance?\" after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as \"How long have you been working abroad?\" when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 32k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.' (Score: 0.2639)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.3025)\n",
            "2. Text: 'Integrating Transformer and Paraphrase Rules for Sentence Simplification Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned simplification mapping rules from normal- simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we pro- pose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state- of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text_simplification.' (Score: 0.2824)\n",
            "3. Text: 'Sentence Simplification with Deep Reinforcement Learning Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call {\\sc Dress} (as shorthand for {\\bf D}eep {\\bf RE}inforcement {\\bf S}entence {\\bf S}implification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.' (Score: 0.2247)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "[tfidf_lc_off_sw_off] lowercase=False, stop_words=None\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'HUME: Human UCCA-Based Evaluation of Machine Translation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.2908)\n",
            "2. Text: 'Prediction Improves Simultaneous Neural Machine Translation We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.' (Score: 0.2906)\n",
            "3. Text: 'Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.2803)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset.' (Score: 0.2979)\n",
            "2. Text: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, instead, we endeavor to generate longer and semantic-rich distractors which are closer to distractors in real reading comprehension from examinations. Taking a reading comprehension article, a pair of question and its correct option as input, our goal is to generate several distractors which are somehow related to the answer, consistent with the semantic context of the question and have some trace in the article. We propose a hierarchical encoder-decoder framework with static and dynamic attention mechanisms to tackle this task. Specifically, the dynamic attention can combine sentence-level and word-level attention varying at each recurrent time step to generate a more readable sequence. The static attention is to modulate the dynamic attention not to focus on question irrelevant sentences or sentences which contribute to the correct option. Our proposed framework outperforms several strong baselines on the first prepared distractor generation dataset of real reading comprehension questions. For human evaluation, compared with those distractors generated by baselines, our generated distractors are more functional to confuse the annotators.' (Score: 0.2635)\n",
            "3. Text: 'Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model from the SQuAD dataset on the challenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single model and 46.6% with an ensemble, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%, without use of provided annotations.' (Score: 0.2565)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.2526)\n",
            "2. Text: 'Integrating Transformer and Paraphrase Rules for Sentence Simplification Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned simplification mapping rules from normal- simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we pro- pose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state- of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text_simplification.' (Score: 0.2306)\n",
            "3. Text: 'Learning Unsupervised Word Translations Without Adversaries Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.' (Score: 0.1961)\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "[tfidf_lc_off_sw_on] lowercase=False, stop_words=english\n",
            "====================================================================================================\n",
            "Query 1\n",
            "1. Text: 'Prediction Improves Simultaneous Neural Machine Translation We investigate the potential of attention-based neural machine translation in simultaneous translation. We introduce a novel decoding algorithm, called simultaneous greedy decoding, that allows an existing neural machine translation model to begin translating before a full source sentence is received. This approach is unique from previous works on simultaneous translation in that segmentation and translation are done jointly to maximize the translation quality and that translating each segment is strongly conditioned on all the previous segments. This paper presents a first step toward building a full simultaneous translation system based on neural machine translation.' (Score: 0.2973)\n",
            "2. Text: 'HUME: Human UCCA-Based Evaluation of Machine Translation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.2839)\n",
            "3. Text: 'Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese--English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.' (Score: 0.2749)\n",
            "\n",
            "Query 2\n",
            "1. Text: 'A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, existing reading comprehension datasets are mostly in English. To add diversity in reading comprehension datasets, in this paper we propose a new Chinese reading comprehension dataset for accelerating related research in the community. The proposed dataset contains two different types: cloze-style reading comprehension and user query reading comprehension, associated with large-scale training data as well as human-annotated validation and hidden test set. Along with this dataset, we also hosted the first Evaluation on Chinese Machine Reading Comprehension (CMRC-2017) and successfully attracted tens of participants, which suggest the potential impact of this dataset.' (Score: 0.2906)\n",
            "2. Text: 'Two-Stage Synthesis Networks for Transfer Learning in Machine Comprehension We develop a technique for transfer learning in machine comprehension (MC) using a novel two-stage synthesis network (SynNet). Given a high-performing MC model in one domain, our technique aims to answer questions about documents in another domain, where we use no labeled data of question-answer pairs. Using the proposed SynNet with a pretrained model from the SQuAD dataset on the challenging NewsQA dataset, we achieve an F1 measure of 44.3% with a single model and 46.6% with an ensemble, approaching performance of in-domain models (F1 measure of 50.0%) and outperforming the out-of-domain baseline of 7.6%, without use of provided annotations.' (Score: 0.2377)\n",
            "3. Text: 'RACE: Large-scale ReAding Comprehension Dataset From Examinations We investigate the task of distractor generation for multiple choice reading comprehension questions from examinations. In contrast to all previous works, we do not aim at preparing words or short phrases distractors, instead, we endeavor to generate longer and semantic-rich distractors which are closer to distractors in real reading comprehension from examinations. Taking a reading comprehension article, a pair of question and its correct option as input, our goal is to generate several distractors which are somehow related to the answer, consistent with the semantic context of the question and have some trace in the article. We propose a hierarchical encoder-decoder framework with static and dynamic attention mechanisms to tackle this task. Specifically, the dynamic attention can combine sentence-level and word-level attention varying at each recurrent time step to generate a more readable sequence. The static attention is to modulate the dynamic attention not to focus on question irrelevant sentences or sentences which contribute to the correct option. Our proposed framework outperforms several strong baselines on the first prepared distractor generation dataset of real reading comprehension questions. For human evaluation, compared with those distractors generated by baselines, our generated distractors are more functional to confuse the annotators.' (Score: 0.2239)\n",
            "\n",
            "Query 3\n",
            "1. Text: 'Privacy-preserving Neural Representations of Text Text summarization and text simplification are two major ways to simplify the text for poor readers, including children, non-native speakers, and the functionally illiterate. Text summarization is to produce a brief summary of the main ideas of the text, while text simplification aims to reduce the linguistic complexity of the text and retain the original meaning. Recently, most approaches for text summarization and text simplification are based on the sequence-to-sequence model, which achieves much success in many text generation tasks. However, although the generated simplified texts are similar to source texts literally, they have low semantic relevance. In this work, our goal is to improve semantic relevance between source texts and simplified texts for text summarization and text simplification. We introduce a Semantic Relevance Based neural model to encourage high semantic similarity between texts and summaries. In our model, the source text is represented by a gated attention encoder, while the summary representation is produced by a decoder. Besides, the similarity score between the representations is maximized during training. Our experiments show that the proposed model outperforms the state-of-the-art systems on two benchmark corpus.' (Score: 0.2437)\n",
            "2. Text: 'Integrating Transformer and Paraphrase Rules for Sentence Simplification Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned simplification mapping rules from normal- simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we pro- pose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state- of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text_simplification.' (Score: 0.2211)\n",
            "3. Text: 'Learning Unsupervised Word Translations Without Adversaries Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.' (Score: 0.1783)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 7 — Similarity Search with BoW/TF-IDF (Part 1 – Step 3 Core)\n",
        "# =========================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Required variables already created earlier in the notebook:\n",
        "# - paper_texts  (Section 3)\n",
        "# - bow_results, tfidf_results (Sections 4 and 5)\n",
        "# - query_texts, query_labels  (Section 6)\n",
        "\n",
        "RUN_ORDER_BOW = [\"bow_lc_on_sw_off\", \"bow_lc_on_sw_on\", \"bow_lc_off_sw_off\", \"bow_lc_off_sw_on\"]\n",
        "RUN_ORDER_TFIDF = [\"tfidf_lc_on_sw_off\", \"tfidf_lc_on_sw_on\", \"tfidf_lc_off_sw_off\", \"tfidf_lc_off_sw_on\"]\n",
        "\n",
        "def run_similarity_for_run(run_name: str, results_dict: dict):\n",
        "    out = results_dict[run_name]\n",
        "    cfg = out[\"config\"]\n",
        "\n",
        "    vectorizer = out[\"vectorizer\"]\n",
        "    paper_vectors = out[\"X\"]                 # <<< paper_vectors defined\n",
        "    query_vectors = vectorizer.transform(query_texts)  # <<< query_vectors defined (transform, NOT fit_transform)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(f\"[{run_name}] lowercase={cfg.lowercase}, stop_words={cfg.stop_words}\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # ---- TEMPLATE SNIPPET (completed) ----\n",
        "    similarity_matrix = cosine_similarity(query_vectors, paper_vectors)\n",
        "\n",
        "    for nquery in [0, 1, 2]:\n",
        "      print(f\"{query_labels[nquery]}\")\n",
        "      similarity_scores = similarity_matrix[nquery]\n",
        "      top_indices = np.argsort(similarity_scores)[::-1][:3]  # Indices of top 3 scores\n",
        "      for i, index in enumerate(top_indices, 1):\n",
        "        print(f\"{i}. Text: '{paper_texts[index]}' (Score: {similarity_scores[index]:.4f})\")\n",
        "      print()\n",
        "    # --------------------------------------\n",
        "\n",
        "print(\"\\n\\n############################\")\n",
        "print(\"### BoW similarity search ###\")\n",
        "print(\"############################\")\n",
        "for run_name in RUN_ORDER_BOW:\n",
        "    run_similarity_for_run(run_name, bow_results)\n",
        "\n",
        "print(\"\\n\\n#############################\")\n",
        "print(\"### TF-IDF similarity search ###\")\n",
        "print(\"#############################\")\n",
        "for run_name in RUN_ORDER_TFIDF:\n",
        "    run_similarity_for_run(run_name, tfidf_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting test_all.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_all.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "from section4_bow import (\n",
        "    BoWConfig,\n",
        "    build_bow_vectors,\n",
        "    sparse_matrix_stats as bow_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as bow_case_sensitive_stopword_survivors,\n",
        ")\n",
        "\n",
        "from section5_tfidf import (\n",
        "    TfidfConfig,\n",
        "    build_tfidf_vectors,\n",
        "    sparse_matrix_stats as tfidf_sparse_matrix_stats,\n",
        "    case_sensitive_stopword_survivors as tfidf_case_sensitive_stopword_survivors,\n",
        "    l2_norm_sample_stats,\n",
        "    top_terms_for_doc,\n",
        ")\n",
        "\n",
        "from section6_queries import (\n",
        "    QueryItem,\n",
        "    build_query_item,\n",
        "    get_default_queries,\n",
        "    get_query_texts,\n",
        "    validate_queries,\n",
        ")\n",
        "\n",
        "from section7_similarity import (\n",
        "    compute_similarity_matrix,\n",
        "    top_k_indices,\n",
        "    top_k_for_all_queries,\n",
        ")\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "\n",
        "class TestSection4BoW(unittest.TestCase):\n",
        "    def test_build_bow_vectors_shape_and_sparse(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = BoWConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_bow_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(hasattr(vect, \"vocabulary_\"))\n",
        "\n",
        "    def test_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_bow_vectors(texts, BoWConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_bow_vectors(texts, BoWConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_bow_vectors(texts, BoWConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_bow_vectors(texts, BoWConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_bow_vectors(texts, BoWConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = bow_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_sparse_stats_sane(self):\n",
        "        texts = [\"cat sat\", \"dog barked\"]\n",
        "        _, X = build_bow_vectors(texts, BoWConfig(name=\"s\", lowercase=True, stop_words=None))\n",
        "        s = bow_sparse_matrix_stats(X)\n",
        "        self.assertEqual(s[\"n_docs\"], 2)\n",
        "        self.assertGreater(s[\"n_vocab\"], 0)\n",
        "        self.assertGreaterEqual(s[\"density\"], 0.0)\n",
        "        self.assertLessEqual(s[\"density\"], 1.0)\n",
        "\n",
        "\n",
        "class TestSection5Tfidf(unittest.TestCase):\n",
        "    def test_build_tfidf_vectors_shape_sparse_and_float(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        cfg = TfidfConfig(name=\"t\", lowercase=True, stop_words=None)\n",
        "        vect, X = build_tfidf_vectors(texts, cfg)\n",
        "        self.assertEqual(X.shape[0], 3)\n",
        "        self.assertGreater(X.shape[1], 0)\n",
        "        self.assertTrue(hasattr(X, \"nnz\"))\n",
        "        self.assertTrue(\"float\" in str(X.dtype))\n",
        "\n",
        "        stats = tfidf_sparse_matrix_stats(X)\n",
        "        self.assertEqual(stats[\"n_docs\"], 3)\n",
        "        self.assertGreater(stats[\"n_vocab\"], 0)\n",
        "\n",
        "    def test_tfidf_lowercase_merges_tokens(self):\n",
        "        texts = [\"BERT model\", \"bert model\"]\n",
        "\n",
        "        vect_off, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"off\", lowercase=False, stop_words=None))\n",
        "        feats_off = set(vect_off.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"BERT\", feats_off)\n",
        "        self.assertIn(\"bert\", feats_off)\n",
        "\n",
        "        vect_on, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"on\", lowercase=True, stop_words=None))\n",
        "        feats_on = set(vect_on.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"BERT\", feats_on)\n",
        "        self.assertIn(\"bert\", feats_on)\n",
        "\n",
        "    def test_tfidf_stopwords_removed_when_lowercase_true(self):\n",
        "        texts = [\"the cat sat\", \"the dog sat\"]\n",
        "\n",
        "        vect_no, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"no\", lowercase=True, stop_words=None))\n",
        "        feats_no = set(vect_no.get_feature_names_out().tolist())\n",
        "        self.assertIn(\"the\", feats_no)\n",
        "\n",
        "        vect_yes, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"yes\", lowercase=True, stop_words=\"english\"))\n",
        "        feats_yes = set(vect_yes.get_feature_names_out().tolist())\n",
        "        self.assertNotIn(\"the\", feats_yes)\n",
        "\n",
        "    def test_tfidf_case_sensitive_stopword_survivors_detected(self):\n",
        "        texts = [\"The cat and the dog\"]\n",
        "        vect, _ = build_tfidf_vectors(texts, TfidfConfig(name=\"cs\", lowercase=False, stop_words=\"english\"))\n",
        "        survivors = tfidf_case_sensitive_stopword_survivors(vect)\n",
        "        self.assertGreaterEqual(survivors, 1)\n",
        "\n",
        "    def test_tfidf_l2_norms_are_about_one(self):\n",
        "        texts = [\"cat sat\", \"dog sat\", \"cat dog\"]\n",
        "        _, X = build_tfidf_vectors(texts, TfidfConfig(name=\"n\", lowercase=True, stop_words=None))\n",
        "        norms = l2_norm_sample_stats(X, sample_n=3, seed=0)\n",
        "        self.assertAlmostEqual(norms[\"mean\"], 1.0, places=6)\n",
        "\n",
        "    def test_tfidf_idf_downweights_common_terms_when_tf_equal(self):\n",
        "        texts = [\"common rare1\", \"common rare2\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"idf\", lowercase=True, stop_words=None))\n",
        "        feats = vect.get_feature_names_out().tolist()\n",
        "\n",
        "        i_common = feats.index(\"common\")\n",
        "        i_rare1 = feats.index(\"rare1\")\n",
        "\n",
        "        row0 = X.getrow(0).toarray().ravel()\n",
        "        self.assertGreater(row0[i_rare1], row0[i_common])\n",
        "\n",
        "    def test_top_terms_for_doc_returns_sorted(self):\n",
        "        texts = [\"cat sat sat\", \"dog sat\"]\n",
        "        vect, X = build_tfidf_vectors(texts, TfidfConfig(name=\"top\", lowercase=True, stop_words=None))\n",
        "        top = top_terms_for_doc(X, vect, doc_index=0, top_n=5)\n",
        "        self.assertTrue(len(top) > 0)\n",
        "        weights = [w for _, w in top]\n",
        "        self.assertTrue(all(weights[i] >= weights[i + 1] for i in range(len(weights) - 1)))\n",
        "\n",
        "\n",
        "class TestSection6Queries(unittest.TestCase):\n",
        "    def test_default_queries_exist_and_valid(self):\n",
        "        queries = get_default_queries(joiner=\" \")\n",
        "        validate_queries(queries)\n",
        "        self.assertEqual(len(queries), 3)\n",
        "        self.assertEqual([q.label for q in queries], [\"Query 1\", \"Query 2\", \"Query 3\"])\n",
        "        self.assertTrue(all(isinstance(q, QueryItem) for q in queries))\n",
        "\n",
        "    def test_query_texts_are_in_order(self):\n",
        "        queries = get_default_queries(joiner=\" \")\n",
        "        texts = get_query_texts(queries)\n",
        "        self.assertEqual(len(texts), 3)\n",
        "        self.assertTrue(all(isinstance(t, str) and len(t) > 0 for t in texts))\n",
        "\n",
        "    def test_build_query_item_strict_joiner(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            build_query_item(\"Query 1\", \"T\", \"A\", joiner=\"  \")\n",
        "        with self.assertRaises(ValueError):\n",
        "            build_query_item(\"Query 1\", \"T\", \"A\", joiner=\"\")\n",
        "\n",
        "    def test_validate_queries_rejects_wrong_order(self):\n",
        "        q1 = build_query_item(\"Query 1\", \"T1\", \"A1\", joiner=\" \")\n",
        "        q2 = build_query_item(\"Query 2\", \"T2\", \"A2\", joiner=\" \")\n",
        "        q3 = build_query_item(\"Query 3\", \"T3\", \"A3\", joiner=\" \")\n",
        "        with self.assertRaises(AssertionError):\n",
        "            validate_queries([q2, q1, q3])\n",
        "\n",
        "    def test_validate_queries_rejects_bad_text_pipeline(self):\n",
        "        bad = QueryItem(label=\"Query 1\", title=\"T\", abstract=\"A\", text=\"T  A\")\n",
        "        q2 = build_query_item(\"Query 2\", \"T2\", \"A2\", joiner=\" \")\n",
        "        q3 = build_query_item(\"Query 3\", \"T3\", \"A3\", joiner=\" \")\n",
        "        with self.assertRaises(AssertionError):\n",
        "            validate_queries([bad, q2, q3])\n",
        "\n",
        "\n",
        "class TestSection7Similarity(unittest.TestCase):\n",
        "    def test_top_k_indices_tie_break_is_deterministic(self):\n",
        "        scores = [0.5, 0.5, 0.2, 0.5]\n",
        "        top = top_k_indices(scores, k=3)\n",
        "        # same score 0.5 -> lowest indices first\n",
        "        self.assertTrue(np.array_equal(top, np.array([0, 1, 3])))\n",
        "\n",
        "    def test_compute_similarity_matrix_shape_and_values_bow(self):\n",
        "        docs = [\"cat sat\", \"dog barked\", \"cat meowed\"]\n",
        "        queries = [\"cat sat\", \"dog\"]\n",
        "\n",
        "        vect, X_docs = build_bow_vectors(docs, BoWConfig(name=\"bow\", lowercase=True, stop_words=None))\n",
        "        X_q = vect.transform(queries)\n",
        "\n",
        "        S = compute_similarity_matrix(X_q, X_docs)\n",
        "        self.assertEqual(S.shape, (2, 3))\n",
        "\n",
        "        # Query 0 should match doc 0 best; query 1 should match doc 1 best\n",
        "        self.assertEqual(int(np.argmax(S[0])), 0)\n",
        "        self.assertEqual(int(np.argmax(S[1])), 1)\n",
        "\n",
        "    def test_top_k_for_all_queries_returns_k_per_query(self):\n",
        "        docs = [\"a b c\", \"a b\", \"x y z\"]\n",
        "        queries = [\"a b\", \"x y\"]\n",
        "\n",
        "        vect, X_docs = build_tfidf_vectors(docs, TfidfConfig(name=\"tf\", lowercase=True, stop_words=None))\n",
        "        X_q = vect.transform(queries)\n",
        "\n",
        "        S = compute_similarity_matrix(X_q, X_docs)\n",
        "        res = top_k_for_all_queries(S, k=2)\n",
        "\n",
        "        self.assertEqual(len(res), 2)\n",
        "        self.assertEqual(len(res[0].top_indices), 2)\n",
        "        self.assertEqual(len(res[1].top_indices), 2)\n",
        "\n",
        "    def test_dimension_mismatch_raises(self):\n",
        "        # 2 features vs 3 features should raise\n",
        "        q = np.zeros((1, 2))\n",
        "        d = np.zeros((3, 3))\n",
        "        with self.assertRaises(ValueError):\n",
        "            compute_similarity_matrix(q, d)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_all.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_rejects_non_dict_records (test_all.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_all.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_all.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "test_build_bow_vectors_shape_and_sparse (test_all.TestSection4BoW.test_build_bow_vectors_shape_and_sparse) ... ok\n",
            "test_case_sensitive_stopword_survivors_detected (test_all.TestSection4BoW.test_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_lowercase_merges_tokens (test_all.TestSection4BoW.test_lowercase_merges_tokens) ... ok\n",
            "test_sparse_stats_sane (test_all.TestSection4BoW.test_sparse_stats_sane) ... ok\n",
            "test_stopwords_removed_when_lowercase_true (test_all.TestSection4BoW.test_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_build_tfidf_vectors_shape_sparse_and_float (test_all.TestSection5Tfidf.test_build_tfidf_vectors_shape_sparse_and_float) ... ok\n",
            "test_tfidf_case_sensitive_stopword_survivors_detected (test_all.TestSection5Tfidf.test_tfidf_case_sensitive_stopword_survivors_detected) ... ok\n",
            "test_tfidf_idf_downweights_common_terms_when_tf_equal (test_all.TestSection5Tfidf.test_tfidf_idf_downweights_common_terms_when_tf_equal) ... ok\n",
            "test_tfidf_l2_norms_are_about_one (test_all.TestSection5Tfidf.test_tfidf_l2_norms_are_about_one) ... ok\n",
            "test_tfidf_lowercase_merges_tokens (test_all.TestSection5Tfidf.test_tfidf_lowercase_merges_tokens) ... ok\n",
            "test_tfidf_stopwords_removed_when_lowercase_true (test_all.TestSection5Tfidf.test_tfidf_stopwords_removed_when_lowercase_true) ... ok\n",
            "test_top_terms_for_doc_returns_sorted (test_all.TestSection5Tfidf.test_top_terms_for_doc_returns_sorted) ... ok\n",
            "test_build_query_item_strict_joiner (test_all.TestSection6Queries.test_build_query_item_strict_joiner) ... ok\n",
            "test_default_queries_exist_and_valid (test_all.TestSection6Queries.test_default_queries_exist_and_valid) ... ok\n",
            "test_query_texts_are_in_order (test_all.TestSection6Queries.test_query_texts_are_in_order) ... ok\n",
            "test_validate_queries_rejects_bad_text_pipeline (test_all.TestSection6Queries.test_validate_queries_rejects_bad_text_pipeline) ... ok\n",
            "test_validate_queries_rejects_wrong_order (test_all.TestSection6Queries.test_validate_queries_rejects_wrong_order) ... ok\n",
            "test_compute_similarity_matrix_shape_and_values_bow (test_all.TestSection7Similarity.test_compute_similarity_matrix_shape_and_values_bow) ... ok\n",
            "test_dimension_mismatch_raises (test_all.TestSection7Similarity.test_dimension_mismatch_raises) ... ok\n",
            "test_top_k_for_all_queries_returns_k_per_query (test_all.TestSection7Similarity.test_top_k_for_all_queries_returns_k_per_query) ... ERROR\n",
            "test_top_k_indices_tie_break_is_deterministic (test_all.TestSection7Similarity.test_top_k_indices_tie_break_is_deterministic) ... ok\n",
            "\n",
            "======================================================================\n",
            "ERROR: test_top_k_for_all_queries_returns_k_per_query (test_all.TestSection7Similarity.test_top_k_for_all_queries_returns_k_per_query)\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\Cris-SX\\Desktop\\UA\\CHAD-MASTER-IA\\TPLN\\tpln-practice1\\code\\lvl1\\lvl2\\tpln-practice-code\\test_all.py\", line 247, in test_top_k_for_all_queries_returns_k_per_query\n",
            "    vect, X_docs = build_tfidf_vectors(docs, TfidfConfig(name=\"tf\", lowercase=True, stop_words=None))\n",
            "                   ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\Cris-SX\\Desktop\\UA\\CHAD-MASTER-IA\\TPLN\\tpln-practice1\\code\\lvl1\\lvl2\\tpln-practice-code\\section5_tfidf.py\", line 65, in build_tfidf_vectors\n",
            "    X = vectorizer.fit_transform(texts)\n",
            "  File \"C:\\Users\\Cris-SX\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2105, in fit_transform\n",
            "    X = super().fit_transform(raw_documents)\n",
            "  File \"C:\\Users\\Cris-SX\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"C:\\Users\\Cris-SX\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1377, in fit_transform\n",
            "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
            "                    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Cris-SX\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1283, in _count_vocab\n",
            "    raise ValueError(\n",
            "        \"empty vocabulary; perhaps the documents only contain stop words\"\n",
            "    )\n",
            "ValueError: empty vocabulary; perhaps the documents only contain stop words\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 25 tests in 0.033s\n",
            "\n",
            "FAILED (errors=1)\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_all.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYDuwvtrlvCy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiX0btXHlvwx"
      },
      "source": [
        "###Step 4: Analysis of the results obtained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4huZZ3yl4uu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9_ZGeoih9AD"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "In this section students should use `setence-embeddings` to obtain sentence-embedding representations of the dataset and to peform searches for best matches regarding the examples proposed in the instructions of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEVig0EkcCm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr74dlaTkzeG"
      },
      "source": [
        "###Step 1: Trying a general purpose small monolingual model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5sToE6jl-e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBydGHYMl-t-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xHKqT5TmDGF"
      },
      "source": [
        "###Step 2: Comparing other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WPRck-OmB-Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6gmrCnmCWM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orh6Qh9OmHhB"
      },
      "source": [
        "###Step 3: Moving to a multilingual environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBXNcaK-mNa_"
      },
      "source": [
        "When trying multilingual models, you will have to build a multilingual collection of papers. To do so, extend the collection of papers from the EMNLP conference used in the first part of this notebook with an extra collection of papers from the SEPLN conference, which are both in English and Spanish. Create a new dataset that concatenates both collections to try a multilingual search. The collection of SEPLN papers can be downloaded from [https://www.dlsi.ua.es/~mespla/sepln.json](https://www.dlsi.ua.es/~mespla/sepln.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzXQs6-FmOkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeJ3So5UmO1T"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4Xcu1Dmeiy"
      },
      "source": [
        "##Concluding remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izHrjfNBmg5z"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
