{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNBki8x4YoCi"
      },
      "source": [
        "# Using vectorized representations of text for semantic text similarity\n",
        "\n",
        "This notebook has been created to allow students of the TNLP 25/26 course to complete their assignment on vectorized representations of text. This notebook is provided with the minimal information to start working on the assignment. Students will have to follow the [instructions](https://mespla.github.io/tpln2526/assignment-searchinvectorialspace/) of the assignment reflecting in this notebook the work done.\n",
        "\n",
        "The starting point will be to install both the `scikit-learn` and the `sentence-embeddings` python libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3_yWE7DX_eD",
        "outputId": "6d65e29e-3902-49bb-fa9c-adb47e567c27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.54.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.6.0+cpu)\n",
            "Requirement already satisfied: scipy in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.34.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.4)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cris-sx\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
            "Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
            "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.5/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 0.8/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.0/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.0/8.7 MB 1.2 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 1.3/8.7 MB 1.1 MB/s eta 0:00:07\n",
            "   ------- -------------------------------- 1.6/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 1.8/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   --------- ------------------------------ 2.1/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   ---------- ----------------------------- 2.4/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   ------------ --------------------------- 2.6/8.7 MB 1.2 MB/s eta 0:00:06\n",
            "   ------------- -------------------------- 2.9/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   -------------- ------------------------- 3.1/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 3.4/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 3.7/8.7 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------ --------------------- 3.9/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   ------------------- -------------------- 4.2/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 4.5/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 4.7/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 5.0/8.7 MB 1.2 MB/s eta 0:00:04\n",
            "   ------------------------ --------------- 5.2/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 5.5/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 5.8/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 6.0/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 6.3/8.7 MB 1.2 MB/s eta 0:00:03\n",
            "   ------------------------------ --------- 6.6/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 6.8/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 6.8/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 7.1/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 7.3/8.7 MB 1.2 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 7.6/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 7.9/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 8.1/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 8.4/8.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.7/8.7 MB 1.2 MB/s eta 0:00:00\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, joblib, scikit-learn, sentence-transformers\n",
            "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 sentence-transformers-5.1.2 threadpoolctl-3.6.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tgcVmtaYmkN"
      },
      "source": [
        "Once this is done, students will have to download and read the file containing the dataset consisting of a list of scientific paper's title and abstract.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWRIDdL4YknO",
        "outputId": "f1e5a49b-e8e3-4a77-d5c3-021740a6963a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Cache] Using existing file: emnlp2016-2018.json\n",
            "\n",
            "[Provenance]\n",
            " - Source URL: https://sbert.net/datasets/emnlp2016-2018.json\n",
            " - Local path: emnlp2016-2018.json\n",
            " - File size : 1.05 MB\n",
            " - SHA-256   : 9e6020503e5f0dd0e91dbb970d47f43c7621f67321249daed5990057e159961c\n",
            "\n",
            "[Sanity checks]\n",
            " - N papers loaded: 974\n",
            " - Key presence: OK (all required keys found in all records)\n",
            " - Empty titles   : 0\n",
            " - Empty abstracts: 0\n",
            " - Non-string titles   : 0\n",
            " - Non-string abstracts: 0\n",
            " - Year distribution (top): [(2018, 549), (2017, 230), (2016, 195)]\n",
            " - Years outside 2016–2018: 0\n",
            " - Venue distribution (top): [('EMNLP', 974)]\n",
            " - Duplicate URL keys: 0 (unique non-empty URLs: 974)\n",
            "\n",
            "[Access check — first record]\n",
            " - title   : Rule Extraction for Tree-to-Tree Transducers by Cost Minimization\n",
            " - abstract: Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set...\n",
            " - url     : http://aclweb.org/anthology/D16-1002\n",
            " - venue   : EMNLP\n",
            " - year    : 2016\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 2 — Dataset Acquisition & Loading (EMNLP 2016–2018 JSON)\n",
        "# =========================\n",
        "\n",
        "import os, json, hashlib\n",
        "import requests\n",
        "from collections import Counter\n",
        "\n",
        "DATA_URL = \"https://sbert.net/datasets/emnlp2016-2018.json\"\n",
        "DATA_PATH = \"emnlp2016-2018.json\"   # Feel free to rename, but keep it consistent across the notebook\n",
        "FORCE_DOWNLOAD = False             # Set True if you want to re-download even if the file exists\n",
        "\n",
        "def sha256_of_file(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def download_json(url: str, out_path: str, timeout: int = 120) -> None:\n",
        "    r = requests.get(url, stream=True, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    with open(out_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "# --- (1) Acquire the dataset file (download if missing) ---\n",
        "if FORCE_DOWNLOAD or (not os.path.exists(DATA_PATH)):\n",
        "    print(f\"[Download] Fetching dataset from: {DATA_URL}\")\n",
        "    download_json(DATA_URL, DATA_PATH)\n",
        "    print(f\"[Download] Saved to: {DATA_PATH}\")\n",
        "else:\n",
        "    print(f\"[Cache] Using existing file: {DATA_PATH}\")\n",
        "\n",
        "# --- (2) Load JSON into memory ---\n",
        "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    papers = json.load(f)\n",
        "\n",
        "# Alias for compatibility with the starter template cells\n",
        "data = papers\n",
        "\n",
        "# --- (3) Validate expected JSON structure: root must be a list of dicts ---\n",
        "if not isinstance(papers, list):\n",
        "    raise TypeError(f\"Expected JSON root to be a list, got: {type(papers)}\")\n",
        "\n",
        "if len(papers) == 0:\n",
        "    raise ValueError(\"Dataset loaded but it is empty (N=0). File may be corrupted or not the expected dataset.\")\n",
        "\n",
        "if not isinstance(papers[0], dict):\n",
        "    raise TypeError(f\"Expected each item to be a dict, got first item type: {type(papers[0])}\")\n",
        "\n",
        "# --- (4) Provenance info (practical traceability) ---\n",
        "file_size_mb = os.path.getsize(DATA_PATH) / (1024 * 1024)\n",
        "print(f\"\\n[Provenance]\")\n",
        "print(f\" - Source URL: {DATA_URL}\")\n",
        "print(f\" - Local path: {DATA_PATH}\")\n",
        "print(f\" - File size : {file_size_mb:.2f} MB\")\n",
        "print(f\" - SHA-256   : {sha256_of_file(DATA_PATH)}\")\n",
        "\n",
        "# --- (5) Mandatory sanity checks ---\n",
        "N = len(papers)\n",
        "print(f\"\\n[Sanity checks]\")\n",
        "print(f\" - N papers loaded: {N}\")\n",
        "\n",
        "required_keys = {\"title\", \"abstract\", \"url\", \"venue\", \"year\"}\n",
        "missing_key_counts = Counter()\n",
        "\n",
        "empty_title = 0\n",
        "empty_abstract = 0\n",
        "non_string_title = 0\n",
        "non_string_abstract = 0\n",
        "\n",
        "years = []\n",
        "venues = []\n",
        "urls = []\n",
        "\n",
        "for p in papers:\n",
        "    # Key presence\n",
        "    for k in required_keys:\n",
        "        if k not in p:\n",
        "            missing_key_counts[k] += 1\n",
        "\n",
        "    # Title / abstract existence + type + emptiness\n",
        "    t = p.get(\"title\", None)\n",
        "    a = p.get(\"abstract\", None)\n",
        "\n",
        "    if not isinstance(t, str):\n",
        "        non_string_title += 1\n",
        "        t = \"\" if t is None else str(t)\n",
        "    if not isinstance(a, str):\n",
        "        non_string_abstract += 1\n",
        "        a = \"\" if a is None else str(a)\n",
        "\n",
        "    if len(t.strip()) == 0:\n",
        "        empty_title += 1\n",
        "    if len(a.strip()) == 0:\n",
        "        empty_abstract += 1\n",
        "\n",
        "    # Metadata distributions (for plausibility checks)\n",
        "    venues.append(str(p.get(\"venue\", \"\")).strip())\n",
        "    urls.append(str(p.get(\"url\", \"\")).strip())\n",
        "\n",
        "    y = p.get(\"year\", None)\n",
        "    try:\n",
        "        years.append(int(y))\n",
        "    except Exception:\n",
        "        years.append(None)\n",
        "\n",
        "# Report key presence\n",
        "if sum(missing_key_counts.values()) == 0:\n",
        "    print(\" - Key presence: OK (all required keys found in all records)\")\n",
        "else:\n",
        "    print(\" - Key presence issues:\")\n",
        "    for k in sorted(required_keys):\n",
        "        if missing_key_counts[k] > 0:\n",
        "            print(f\"   * Missing '{k}': {missing_key_counts[k]} records\")\n",
        "\n",
        "# Report title/abstract health\n",
        "print(f\" - Empty titles   : {empty_title}\")\n",
        "print(f\" - Empty abstracts: {empty_abstract}\")\n",
        "print(f\" - Non-string titles   : {non_string_title}\")\n",
        "print(f\" - Non-string abstracts: {non_string_abstract}\")\n",
        "\n",
        "# Year plausibility\n",
        "valid_years = [y for y in years if isinstance(y, int)]\n",
        "year_counts = Counter(valid_years)\n",
        "print(f\" - Year distribution (top): {year_counts.most_common(5)}\")\n",
        "\n",
        "outside = [y for y in valid_years if y < 2016 or y > 2018]\n",
        "print(f\" - Years outside 2016–2018: {len(outside)}\")\n",
        "\n",
        "# Venue plausibility\n",
        "venue_counts = Counter([v for v in venues if v])\n",
        "print(f\" - Venue distribution (top): {venue_counts.most_common(5)}\")\n",
        "\n",
        "# Duplicate URLs (very practical duplicate detector)\n",
        "url_counts = Counter([u for u in urls if u])\n",
        "dupe_urls = sum(1 for u, c in url_counts.items() if c > 1)\n",
        "print(f\" - Duplicate URL keys: {dupe_urls} (unique non-empty URLs: {len(url_counts)})\")\n",
        "\n",
        "# --- (6) Access check (prove we can read title/abstract) ---\n",
        "print(f\"\\n[Access check — first record]\")\n",
        "print(f\" - title   : {papers[0].get('title','')[:120]}{'...' if len(papers[0].get('title',''))>120 else ''}\")\n",
        "print(f\" - abstract: {papers[0].get('abstract','')[:200]}{'...' if len(papers[0].get('abstract',''))>200 else ''}\")\n",
        "print(f\" - url     : {papers[0].get('url','')}\")\n",
        "print(f\" - venue   : {papers[0].get('venue','')}\")\n",
        "print(f\" - year    : {papers[0].get('year','')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5a7FAOehkh-"
      },
      "source": [
        "## Part 1:\n",
        "From this point, students should be able to obtain BoW and TF-IDF representations of the dataset, and to obtain similar matches for the new scientific paper titles in the instructions of the exercise. Include here the code use to build the representations, as well as the discussion on the results obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-tWKGq3LQd6"
      },
      "outputs": [],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05P6MYR3lLok"
      },
      "source": [
        "###Step 1: Preprocessing the Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing section3_preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile section3_preprocessing.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    _HAS_PANDAS = True\n",
        "except Exception:\n",
        "    pd = None  # type: ignore\n",
        "    _HAS_PANDAS = False\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PrepConfig:\n",
        "    title_key: str = \"title\"\n",
        "    abstract_key: str = \"abstract\"\n",
        "    joiner: str = \" \"\n",
        "    make_dataframe: bool = True\n",
        "\n",
        "\n",
        "def _as_text(x: Any) -> str:\n",
        "    \"\"\"Minimal, non-aggressive coercion: keep text as-is (no casing/punct stripping).\"\"\"\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, str):\n",
        "        return x\n",
        "    return str(x)\n",
        "\n",
        "\n",
        "def build_paper_texts(\n",
        "    records: Sequence[Dict[str, Any]],\n",
        "    config: PrepConfig = PrepConfig(),\n",
        ") -> Tuple[List[str], Optional[\"pd.DataFrame\"]]:\n",
        "    \"\"\"\n",
        "    Build the working 'text' field exactly as required:\n",
        "        text = title + \" \" + abstract\n",
        "\n",
        "    Document unit: one paper = one document.\n",
        "    Returns:\n",
        "      - paper_texts: list[str] with one concatenated document per paper\n",
        "      - df (optional): pandas DataFrame including title/abstract/url/venue/year/text\n",
        "    \"\"\"\n",
        "    if not isinstance(records, (list, tuple)):\n",
        "        raise TypeError(f\"records must be a sequence (list/tuple) of dicts, got {type(records)}\")\n",
        "\n",
        "    paper_texts: List[str] = []\n",
        "    rows_for_df: List[Dict[str, Any]] = []\n",
        "\n",
        "    for i, rec in enumerate(records):\n",
        "        if not isinstance(rec, dict):\n",
        "            raise TypeError(f\"Each record must be a dict. Found {type(rec)} at index {i}.\")\n",
        "\n",
        "        title = _as_text(rec.get(config.title_key, \"\"))\n",
        "        abstract = _as_text(rec.get(config.abstract_key, \"\"))\n",
        "\n",
        "        # Exact construction rule (single space joiner)\n",
        "        text = title + config.joiner + abstract\n",
        "\n",
        "        paper_texts.append(text)\n",
        "\n",
        "        if config.make_dataframe:\n",
        "            rows_for_df.append(\n",
        "                {\n",
        "                    \"title\": title,\n",
        "                    \"abstract\": abstract,\n",
        "                    \"url\": rec.get(\"url\", \"\"),\n",
        "                    \"venue\": rec.get(\"venue\", \"\"),\n",
        "                    \"year\": rec.get(\"year\", \"\"),\n",
        "                    \"text\": text,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    df_out = None\n",
        "    if config.make_dataframe:\n",
        "        if not _HAS_PANDAS:\n",
        "            raise ImportError(\"pandas is required for make_dataframe=True, but it is not available.\")\n",
        "        df_out = pd.DataFrame(rows_for_df)\n",
        "\n",
        "    return paper_texts, df_out\n",
        "\n",
        "\n",
        "def validate_paper_texts(paper_texts: Sequence[str], expected_n: int) -> None:\n",
        "    \"\"\"Sanity checks for Section 3 outputs.\"\"\"\n",
        "    if len(paper_texts) != expected_n:\n",
        "        raise AssertionError(f\"len(paper_texts)={len(paper_texts)} != expected_n={expected_n}\")\n",
        "    if any(not isinstance(t, str) for t in paper_texts):\n",
        "        raise AssertionError(\"paper_texts must contain only strings.\")\n",
        "    if any(len(t) == 0 for t in paper_texts):\n",
        "        raise AssertionError(\"paper_texts contains empty strings (unexpected for this dataset).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Section 3] Preprocessing complete.\n",
            " - Document unit          : 1 paper = 1 document\n",
            " - N documents (paper_texts): 974\n",
            " - DataFrame created      : True\n",
            "\n",
            "[Section 3] Example document (first record):\n",
            "Rule Extraction for Tree-to-Tree Transducers by Cost Minimization Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the ...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>url</th>\n",
              "      <th>venue</th>\n",
              "      <th>year</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rule Extraction for Tree-to-Tree Transducers b...</td>\n",
              "      <td>Finite-state transducers give efficient repres...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1002</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>Rule Extraction for Tree-to-Tree Transducers b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A Neural Network for Coordination Boundary Pre...</td>\n",
              "      <td>We propose a neural-network based model for co...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1003</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>A Neural Network for Coordination Boundary Pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Distinguishing Past, On-going, and Future Even...</td>\n",
              "      <td>The tremendous amount of user generated data t...</td>\n",
              "      <td>http://aclweb.org/anthology/D16-1005</td>\n",
              "      <td>EMNLP</td>\n",
              "      <td>2016</td>\n",
              "      <td>Distinguishing Past, On-going, and Future Even...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  Rule Extraction for Tree-to-Tree Transducers b...   \n",
              "1  A Neural Network for Coordination Boundary Pre...   \n",
              "2  Distinguishing Past, On-going, and Future Even...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  Finite-state transducers give efficient repres...   \n",
              "1  We propose a neural-network based model for co...   \n",
              "2  The tremendous amount of user generated data t...   \n",
              "\n",
              "                                    url  venue  year  \\\n",
              "0  http://aclweb.org/anthology/D16-1002  EMNLP  2016   \n",
              "1  http://aclweb.org/anthology/D16-1003  EMNLP  2016   \n",
              "2  http://aclweb.org/anthology/D16-1005  EMNLP  2016   \n",
              "\n",
              "                                                text  \n",
              "0  Rule Extraction for Tree-to-Tree Transducers b...  \n",
              "1  A Neural Network for Coordination Boundary Pre...  \n",
              "2  Distinguishing Past, On-going, and Future Even...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =========================\n",
        "# Section 3 — Data Preparation for Vectorization (Part 1 – Step 1)\n",
        "# =========================\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "# Use the variable created in Section 2 (you already set: data = papers)\n",
        "records = data  # list of dicts, one paper per record\n",
        "\n",
        "config = PrepConfig(\n",
        "    title_key=\"title\",\n",
        "    abstract_key=\"abstract\",\n",
        "    joiner=\" \",          # MUST be exactly one space\n",
        "    make_dataframe=True  # optional, but very useful in a notebook\n",
        ")\n",
        "\n",
        "paper_texts, papers_df = build_paper_texts(records, config=config)\n",
        "\n",
        "# Robust sanity checks\n",
        "N = len(records)\n",
        "validate_paper_texts(paper_texts, expected_n=N)\n",
        "\n",
        "print(\"[Section 3] Preprocessing complete.\")\n",
        "print(f\" - Document unit          : 1 paper = 1 document\")\n",
        "print(f\" - N documents (paper_texts): {len(paper_texts)}\")\n",
        "print(f\" - DataFrame created      : {papers_df is not None}\")\n",
        "print(\"\\n[Section 3] Example document (first record):\")\n",
        "print(paper_texts[0][:400] + (\"...\" if len(paper_texts[0]) > 400 else \"\"))\n",
        "\n",
        "# Optional: quick inspection (keeps later steps easier)\n",
        "if papers_df is not None:\n",
        "    display(papers_df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing test_section3_preprocessing.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_section3_preprocessing.py\n",
        "import unittest\n",
        "\n",
        "from section3_preprocessing import PrepConfig, build_paper_texts, validate_paper_texts\n",
        "\n",
        "\n",
        "class TestSection3Preprocessing(unittest.TestCase):\n",
        "    def test_basic_concatenation_rule(self):\n",
        "        records = [\n",
        "            {\"title\": \"Hello\", \"abstract\": \"World\", \"url\": \"u\", \"venue\": \"v\", \"year\": 2016},\n",
        "            {\"title\": \"A\", \"abstract\": \"B\", \"url\": \"u2\", \"venue\": \"v2\", \"year\": 2017},\n",
        "        ]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNone(df)\n",
        "        self.assertEqual(paper_texts, [\"Hello World\", \"A B\"])\n",
        "\n",
        "    def test_stable_length_and_validation(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"X\"} for _ in range(5)]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        validate_paper_texts(paper_texts, expected_n=5)  # should not raise\n",
        "\n",
        "    def test_type_coercion_is_minimal_and_safe(self):\n",
        "        records = [{\"title\": 123, \"abstract\": None}]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        paper_texts, _ = build_paper_texts(records, cfg)\n",
        "        # title becomes \"123\", abstract becomes \"\"\n",
        "        self.assertEqual(paper_texts[0], \"123 \")\n",
        "\n",
        "    def test_rejects_non_dict_records(self):\n",
        "        records = [{\"title\": \"OK\", \"abstract\": \"OK\"}, \"not_a_dict\"]\n",
        "        cfg = PrepConfig(make_dataframe=False)\n",
        "        with self.assertRaises(TypeError):\n",
        "            build_paper_texts(records, cfg)\n",
        "\n",
        "    def test_dataframe_creation_if_enabled(self):\n",
        "        records = [{\"title\": \"T\", \"abstract\": \"A\", \"url\": \"u\", \"venue\": \"EMNLP\", \"year\": 2016}]\n",
        "        cfg = PrepConfig(make_dataframe=True)\n",
        "        paper_texts, df = build_paper_texts(records, cfg)\n",
        "        self.assertIsNotNone(df)\n",
        "        self.assertIn(\"text\", df.columns)\n",
        "        self.assertEqual(df.loc[0, \"text\"], paper_texts[0])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_basic_concatenation_rule (test_section3_preprocessing.TestSection3Preprocessing.test_basic_concatenation_rule) ... ok\n",
            "test_dataframe_creation_if_enabled (test_section3_preprocessing.TestSection3Preprocessing.test_dataframe_creation_if_enabled) ... ok\n",
            "test_rejects_non_dict_records (test_section3_preprocessing.TestSection3Preprocessing.test_rejects_non_dict_records) ... ok\n",
            "test_stable_length_and_validation (test_section3_preprocessing.TestSection3Preprocessing.test_stable_length_and_validation) ... ok\n",
            "test_type_coercion_is_minimal_and_safe (test_section3_preprocessing.TestSection3Preprocessing.test_type_coercion_is_minimal_and_safe) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.002s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "!python -m unittest -v test_section3_preprocessing.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLiO06ahlU5O"
      },
      "source": [
        "###Step 2: Building the BoW and TF-IDF Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x31erbZHlp57"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62WFYhu7lqRi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCXK65aHlq9Y"
      },
      "source": [
        "###Step 3: Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0xuwShjlur3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwFOkv1syVnM"
      },
      "source": [
        "Use the following code snipet to compute the pairwise cosine similarity, and to sort the top 3 candidates for each query. In this example, `query_vectors` and `paper_vectors` correspond to the vectorized collections of queries and papers (as stored in the JSON file previously downloaded), respectively. The variable `paper_texts` contain the list of concatenated titles+abstracts of the papers loaded from the JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_OlJS4Ubx7IF"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'query_vectors' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#...\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m similarity_matrix = cosine_similarity(\u001b[43mquery_vectors\u001b[49m, paper_vectors)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nquery \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m]:\n\u001b[32m      8\u001b[39m   similarity_scores = similarity_matrix[nquery]\n",
            "\u001b[31mNameError\u001b[39m: name 'query_vectors' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#...\n",
        "\n",
        "similarity_matrix = cosine_similarity(query_vectors, paper_vectors)\n",
        "\n",
        "for nquery in [0, 1, 2]:\n",
        "  similarity_scores = similarity_matrix[nquery]\n",
        "  top_indices = np.argsort(similarity_scores)[::-1][:3]  # Indices of top 3 scores\n",
        "  for i, index in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. Text: '{paper_texts[index]}' (Score: {similarity_scores[index]:.4f})\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYDuwvtrlvCy"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiX0btXHlvwx"
      },
      "source": [
        "###Step 4: Analysis of the results obtained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4huZZ3yl4uu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9_ZGeoih9AD"
      },
      "source": [
        "## Part 2:\n",
        "\n",
        "In this section students should use `setence-embeddings` to obtain sentence-embedding representations of the dataset and to peform searches for best matches regarding the examples proposed in the instructions of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbEVig0EkcCm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr74dlaTkzeG"
      },
      "source": [
        "###Step 1: Trying a general purpose small monolingual model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5sToE6jl-e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBydGHYMl-t-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xHKqT5TmDGF"
      },
      "source": [
        "###Step 2: Comparing other models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WPRck-OmB-Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii6gmrCnmCWM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orh6Qh9OmHhB"
      },
      "source": [
        "###Step 3: Moving to a multilingual environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBXNcaK-mNa_"
      },
      "source": [
        "When trying multilingual models, you will have to build a multilingual collection of papers. To do so, extend the collection of papers from the EMNLP conference used in the first part of this notebook with an extra collection of papers from the SEPLN conference, which are both in English and Spanish. Create a new dataset that concatenates both collections to try a multilingual search. The collection of SEPLN papers can be downloaded from [https://www.dlsi.ua.es/~mespla/sepln.json](https://www.dlsi.ua.es/~mespla/sepln.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzXQs6-FmOkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeJ3So5UmO1T"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm4Xcu1Dmeiy"
      },
      "source": [
        "##Concluding remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izHrjfNBmg5z"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
